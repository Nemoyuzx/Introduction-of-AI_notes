# Introduction toÂ Artificial Intelligence AIå¯¼è®º

**EBU4203**

- TB1: Introduction to AI, uncertainty in decision making, machine learning basics  
  äººå·¥æ™ºèƒ½å…¥é—¨ï¼Œå†³ç­–ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œæœºå™¨å­¦ä¹ åŸºç¡€

- TB2: Deep learning and reinforcement learning  
  æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ 

- TB3: Practical AI Applications and Computer Vision  
  å®ç”¨äººå·¥æ™ºèƒ½åº”ç”¨ä¸è®¡ç®—æœºè§†è§‰

- TB4: Natural Language Processing (NLP) and future trends in AI  
  è‡ªç„¶è¯­è¨€å¤„ç†ä¸äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿

---

## Introduce ç®€è¦ä»‹ç»

### assessment

- 1 x Class Test  3% è¯¾å ‚å°æµ‹
  
  - After teaching block 2

- 2 x Self-revision Online Quizzes 3% ç½‘ä¸Šè‡ªæµ‹
  
  - Open for a week

- Laboratory 14% å®éªŒ
  
  - Lab reports å®éªŒæŠ¥å‘Š
  
  - ä¸€å…±ä¸‰æ¬¡,ç¬¬ä¸€æ¬¡ä¸ç”¨å†™

- Final exam 80%
  
  - closed-book written exam é—­å·è€ƒè¯•
  
  - Past papers will be put on QMPlus è¿‡å»çš„è¯•å·å°†æ”¾åœ¨ QMPlus ä¸Š
  
  - Note: A minimum total mark of 40% is required to pass this module  
    æ³¨æ„: é€šè¿‡æœ¬æ¨¡å—æœ€ä½æ€»åˆ†ä¸º40%

- Coursework: 
  
  - Note: There is a coursework hurdle of 30% (A minimum total coursework mark of 30% is required to pass this module)  
    æ³¨: æœ‰ä¸€ä¸ª30% çš„è¯¾ç¨‹ä½œä¸šéšœç¢(é€šè¿‡æ­¤æ¨¡å—éœ€è¦æœ€å°‘30% çš„è¯¾ç¨‹ä½œä¸šæ€»åˆ†)

### Information

- Course website: è¯¾ç¨‹ç½‘ç«™
  
  - Login to QMPlus
  
  - Course Area: EBU4203 (Introduction to AI)
  
  - <mark>Check it regularly, as it is possible there could be additional information e.g. messages, extra practice exercises, tutorials, etc.  
    å®šæœŸæ£€æŸ¥ï¼Œå› ä¸ºå¯èƒ½ä¼šæœ‰é¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚ä¿¡æ¯ï¼Œé¢å¤–çš„ç»ƒä¹ ï¼Œæ•™ç¨‹ç­‰ã€‚</mark>

- Email
  
  - <mark>You are expected to check your QM email every week at least!   
    ä½ è‡³å°‘åº”è¯¥æ¯å‘¨æŸ¥çœ‹ä¸€æ¬¡ QM é‚®ä»¶ï¼</mark>

### Recommended Text book and references æ¨èçš„æ•™ç§‘ä¹¦å’Œå‚è€ƒèµ„æ–™

- ["1"] Russell, S., & Norvig, P. (2021). Artificial Intelligence: a modern approach, 4th US ed. University of California, Berkeley.  
  ["1"]æ‹‰å¡å°”ï¼ŒSã€‚ & è¯ºç»´æ ¼ï¼ŒPã€‚(2021)ã€‚äººå·¥æ™ºèƒ½: ä¸€ç§ç°ä»£æ–¹æ³•ï¼Œç¾å›½ç¬¬å››ç‰ˆã€‚åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ã€‚

- There are plenty of books available on this topic.  
  æœ‰å¾ˆå¤šå…³äºè¿™ä¸ªä¸»é¢˜çš„ä¹¦ã€‚

### Few tips

- Attend every lecture, tutorial, lab and assessment sessions.  
  å‚åŠ æ¯ä¸€ä¸ªè®²åº§ï¼Œè¾…å¯¼ï¼Œå®éªŒå®¤å’Œè¯„ä¼°ä¼šè®®ã€‚

- Revise your lecture materials after every class.  
  æ¯èŠ‚è¯¾åéƒ½è¦ä¿®æ”¹è®²ä¹‰ã€‚

- Make use of available materials, and read books and online materials.  
  åˆ©ç”¨ç°æœ‰çš„èµ„æ–™ï¼Œé˜…è¯»ä¹¦ç±å’Œç½‘ä¸Šèµ„æ–™ã€‚

- Be interactive during the class and tutorial sessions.  
  åœ¨è¯¾å ‚å’Œè¾…å¯¼è¯¾ç¨‹ä¸­ä¿æŒäº’åŠ¨ã€‚

- Ask your lecturers/TAs and discuss with your classmates.  
  è¯¢é—®ä½ çš„è®²å¸ˆ/åŠ©æ•™ï¼Œå¹¶ä¸ä½ çš„åŒå­¦è®¨è®ºã€‚

### mentimeter

interaction tools

## week 1

- Part 1: Introduction to AI AIå¼•å…¥

- Part 2: Uncertainty in decision making å†³ç­–çš„ä¸ç¡®å®šæ€§

- Part 3: Machine learning basics æœºå™¨å­¦ä¹ åŸºç¡€

### Part 1: Introduction to AI ç¬¬ä¸€éƒ¨åˆ†: äººå·¥æ™ºèƒ½å¯¼è®º

- Definition and scope of AI äººå·¥æ™ºèƒ½çš„å®šä¹‰å’ŒèŒƒå›´

- Motivation for exploring AI æ¢ç´¢äººå·¥æ™ºèƒ½çš„åŠ¨æœº

- Brief history of AI äººå·¥æ™ºèƒ½ç®€å²

- Branches and applications of AI äººå·¥æ™ºèƒ½çš„åˆ†æ”¯ä¸åº”ç”¨

- Ethical considerations in AI äººå·¥æ™ºèƒ½çš„ä¼¦ç†æ€è€ƒ

#### What is artificial intelligenceï¼Ÿ

<u>Definition and scope of AI äººå·¥æ™ºèƒ½çš„å®šä¹‰å’ŒèŒƒå›´</u>

##### Alan Turing å›¾çµ â€”â€” å›¾çµæµ‹è¯• Turing Test

- The Turing Test aims to  evaluate **whether a machine  can exhibit intelligence  comparable to that of a human.**  
  å›¾çµæµ‹è¯•çš„ç›®çš„æ˜¯è¯„ä¼°ä¸€å°æœºå™¨èƒ½å¦å±•ç°å‡ºä¸äººç±»ç›¸å½“çš„æ™ºåŠ›ã€‚

- A **text conversation** between a judge, a human, and a machine, where the judge tries to determine whether he is conversing with a human or a machine.  
  ä¸€ç§æ³•å®˜ã€äººç±»å’Œæœºå™¨ä¹‹é—´çš„æ–‡æœ¬å¯¹è¯ï¼Œæ³•å®˜è¯•å›¾ç¡®å®šä»–æ˜¯åœ¨ä¸äººç±»è¿˜æ˜¯æœºå™¨äº¤è°ˆã€‚

- é‡è¦æ€§å’Œå±€é™
  
  - Significance:
    
    - The Turing Test serves as a method for assessing the level of artificial intelligence.  
      å›¾çµæµ‹è¯•æ˜¯è¯„ä¼°äººå·¥æ™ºèƒ½æ°´å¹³çš„ä¸€ç§æ–¹æ³•ã€‚
    
    - If a machine can pass the Turing Test, it indicates a certain level of intelligence and raises questions about AI capabilities.  
      å¦‚æœä¸€å°æœºå™¨èƒ½å¤Ÿé€šè¿‡å›¾çµæµ‹è¯•ï¼Œå®ƒè¡¨æ˜äº†ä¸€å®šç¨‹åº¦çš„æ™ºèƒ½ï¼Œå¹¶æå‡ºäº†å…³äºäººå·¥æ™ºèƒ½èƒ½åŠ›çš„é—®é¢˜ã€‚
  
  - Limitationsï¼š
    
    - The Turing Test focuses **solely on external behavior** and does not evaluate internal cognitive processes.  
      å›¾çµæµ‹è¯•åªå…³æ³¨å¤–éƒ¨è¡Œä¸ºï¼Œä¸è¯„ä¼°å†…éƒ¨è®¤çŸ¥è¿‡ç¨‹ã€‚
    
    - It may be influenced by **subjective judgments** from the judge and other factors.  
      å®ƒå¯èƒ½å—åˆ°æ³•å®˜ä¸»è§‚åˆ¤æ–­ç­‰å› ç´ çš„å½±å“ã€‚

- IBM's Jeopardy Challenge: An intriguing step toward AI passing the Turing Test  
  IBM çš„å±é™©æŒ‘æˆ˜: è¿ˆå‘äººå·¥æ™ºèƒ½é€šè¿‡å›¾çµæµ‹è¯•çš„æœ‰è¶£ä¸€æ­¥
  
  - In 2011, IBM's **supercomputer "Watson" won the Jeopardy Challenge**, becoming the first robot champion.  
    2011å¹´ï¼ŒIBM çš„è¶…çº§è®¡ç®—æœºâ€œæ²ƒæ£®â€èµ¢å¾—äº†å±é™©æŒ‘æˆ˜èµ›ï¼Œæˆä¸ºç¬¬ä¸€ä¸ªæœºå™¨äººå† å†›ã€‚
  
  - Watson's performance in the Jeopardy Challenge: **Comprehending questions, analysing information, and selecting the most probable answer**.  
    æ²ƒæ£®åœ¨å±é™©æŒ‘æˆ˜ä¸­çš„è¡¨ç°: ç†è§£é—®é¢˜ï¼Œåˆ†æä¿¡æ¯ï¼Œé€‰æ‹©æœ€å¯èƒ½çš„ç­”æ¡ˆã€‚
  
  - IBM's Jeopardy Challenge provided a demonstration of **a technological breakthrough related to
  
  - the Turing Test**, proving the potential of machines to process natural language and reasoning, and driving further development in the field of artificial intelligence.  
    IBM çš„ Jeopardy Challenge å±•ç¤ºäº†ä¸å›¾çµæµ‹è¯•ç›¸å…³çš„æŠ€æœ¯çªç ´ï¼Œè¯æ˜äº†æœºå™¨å¤„ç†è‡ªç„¶è¯­è¨€å’Œæ¨ç†çš„æ½œåŠ›ï¼Œå¹¶æ¨åŠ¨äº†äººå·¥æ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

##### The Four potential goals or definitions of AI äººå·¥æ™ºèƒ½çš„å››ä¸ªæ½œåœ¨ç›®æ ‡æˆ–å®šä¹‰

- They differentiates computer systems on the basis of rationality and thinking vs. acting  
  ä»–ä»¬åŒºåˆ†è®¡ç®—æœºç³»ç»Ÿçš„åŸºç¡€æ˜¯ç†æ€§å’Œæ€è€ƒä¸è¡ŒåŠ¨:
  
  <img title="" src="./images/f32b4a76-c830-4be3-ae9a-bccf083cceb0.png" alt="loading-ag-1224" data-align="inline">
  
  ![loading-ag-1222](./images/_b192889a-6532-449a-9e48-f11a2b3a9100_.png)

- At its simplest form, artificial intelligence is a field, which **combines computer science and robust datasets**, to enable **problem-solving**.  
  ç®€å•æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå°†è®¡ç®—æœºç§‘å­¦å’Œå¼ºå¤§çš„æ•°æ®é›†ç»“åˆèµ·æ¥çš„é¢†åŸŸï¼Œå®ƒèƒ½å¤Ÿè§£å†³é—®é¢˜ã€‚

- It also encompasses sub-fields of **machine learning and deep learning**, which are frequently mentioned in conjunction with artificial intelligence.   
  å®ƒè¿˜åŒ…æ‹¬æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å­é¢†åŸŸï¼Œè¿™äº›é¢†åŸŸç»å¸¸ä¸äººå·¥æ™ºèƒ½ä¸€èµ·è¢«æåŠã€‚

- These disciplines are comprised of AI algorithms which seek to **create expert systems which make predictions or classifications based on input data**.  
  è¿™äº›å­¦ç§‘ç”±äººå·¥æ™ºèƒ½ç®—æ³•ç»„æˆï¼Œè¯¥ç®—æ³•å¯»æ±‚åˆ›å»ºä¸“å®¶ç³»ç»Ÿï¼Œæ ¹æ®è¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ†ç±»ã€‚

##### The scope of AI äººå·¥æ™ºèƒ½çš„èŒƒå›´

- As we begin the new millennium   
  åƒç¦§å¹´åå¼€å§‹
  
  - science and technology are changing rapidly   
    ç§‘å­¦æŠ€æœ¯æ­£åœ¨è¿…é€Ÿå˜åŒ–
  
  - â€œoldâ€ sciences such as physics are relatively well-understood   
    åƒç‰©ç†è¿™æ ·çš„â€œå¤è€â€ç§‘å­¦ç›¸å¯¹æ¥è¯´å·²ç»è¢«å¹¿ä¸ºäººçŸ¥äº†
  
  - computers are ubiquitous  
    ç”µè„‘æ— å¤„ä¸åœ¨

- Grand Challenges in Science and Technology  
  ç§‘å­¦æŠ€æœ¯é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜
  
  - understanding the brain   
    å¯¹è„‘ç§‘å­¦çš„ç†è§£ä¸ç ”ç©¶
  
  - reasoning, cognition, creativity   
    æ¨ç†ã€è®¤çŸ¥ã€åˆ›é€ åŠ›
  
  - creating intelligent machines  
    åˆ›é€ æ™ºèƒ½æœºå™¨

##### The Foundations of AIÂ äººå·¥æ™ºèƒ½çš„åŸºç¡€

```mermaid
flowchart LR
    AI{Artificial Intellegence}-->A(Mathematics æ•°å­¦)
    A-->AA["What are the formal rules to draw valid conclusions? å¾—å‡ºæœ‰æ•ˆç»“è®ºçš„å½¢å¼è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ"]
    A-->A2["What can be computed? ä»€ä¹ˆèƒ½è¢«è®¡ç®—"]
    A-->A3["How do we reason with uncertain information? æˆ‘ä»¬å¦‚ä½•å¯¹ä¸ç¡®å®šçš„ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Ÿ"]
    AI-->B(Neuroscience ç¥ç»ç§‘å­¦)
    B-->B1["How do brains process information? å¤§è„‘æ˜¯å¦‚ä½•å¤„ç†ä¿¡æ¯çš„ï¼Ÿ"]
    AI-->C(Linguistics è¯­è¨€å­¦)
    C-->C1["How does language relate to thought? è¯­è¨€å’Œæ€æƒ³æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"]
    AI-->D(Economics ç»æµå‘å±•)
    D-->D1["How should we make decisions so as to maximize payoff? æˆ‘ä»¬åº”è¯¥å¦‚ä½•åšå‡ºå†³ç­–ï¼Œä»¥ä½¿æ”¶ç›Šæœ€å¤§åŒ–ï¼Ÿ"]
    D-->D2["How should we do this when others may not go along? å½“å…¶ä»–äººå¯èƒ½ä¸åŒæ„çš„æ—¶å€™ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ"]
    D-->D3["How should we do this when the payoff may be far in the future? æˆ‘ä»¬åº”è¯¥å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹æ—¶ï¼Œå›æŠ¥å¯èƒ½åœ¨é¥è¿œçš„æœªæ¥ï¼Ÿ"]
    AI-->E(Psychology å¿ƒç†å­¦)
    E-->E1["How do humans and animals think and act? äººç±»å’ŒåŠ¨ç‰©æ˜¯å¦‚ä½•æ€è€ƒå’Œè¡ŒåŠ¨çš„ï¼Ÿ"]
    AI-->F(Control Theory æ§åˆ¶è®º)
    F-->F1["How can artifacts operate under their own control? äººå·¥äº§ç‰©å¦‚ä½•åœ¨è‡ªå·±çš„æ§åˆ¶ä¸‹è¿ä½œï¼Ÿ"]
    AI-->G(Philosophy å“²å­¦)
    G-->G1["Can formal rules be used to draw valid conclusions? å½¢å¼è§„åˆ™å¯ä»¥ç”¨æ¥å¾—å‡ºæœ‰æ•ˆçš„ç»“è®ºå—ï¼Ÿ"]
    G-->G2["How does the mind arise from a physical brain? æ€æƒ³æ˜¯å¦‚ä½•ä»å¤§è„‘ä¸­äº§ç”Ÿçš„ï¼Ÿ"]
    G-->G3["Where does knowledge come from? çŸ¥è¯†ä»ä½•è€Œæ¥ï¼Ÿ"]
    G-->G4["How does knowledge lead to action? çŸ¥è¯†å¦‚ä½•å¯¼è‡´è¡ŒåŠ¨ï¼Ÿ"]

```

#### How does AI work?  äººå·¥æ™ºèƒ½æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

<u>Motivation for exploring AI</u> 

##### Why AI Matters? ä¸ºä»€ä¹ˆäººå·¥æ™ºèƒ½å¾ˆé‡è¦

1. Potential to Transform
   AI has the potential **to revolutionize various aspects** of our lives, work, and leisure activities.  
   äººå·¥æ™ºèƒ½æœ‰å¯èƒ½å½»åº•æ”¹å˜æˆ‘ä»¬ç”Ÿæ´»ã€å·¥ä½œå’Œä¼‘é—²æ´»åŠ¨çš„å„ä¸ªæ–¹é¢ã€‚

2. Business Automation
   AI has been effectively utilized in businesses **to automate tasks** that were previously performed by humans, such as customer service, lead generation, fraud detection, and quality control.  
   äººå·¥æ™ºèƒ½å·²ç»è¢«æœ‰æ•ˆåœ°åº”ç”¨äºä¼ä¸šä¸­ï¼Œä½¿ä»¥å‰ç”±äººç±»æ‰§è¡Œçš„ä»»åŠ¡è‡ªåŠ¨åŒ–ï¼Œä¾‹å¦‚å®¢æˆ·æœåŠ¡ã€å¼•å¯¼ç”Ÿæˆã€æ¬ºè¯ˆæ£€æµ‹å’Œè´¨é‡æ§åˆ¶ã€‚

3. Superior Performance
   In many areas, **AI outperforms humans** in tasks, especially those that are **repetitive and detail-oriented**. AI tools can **quickly analyse large volumes of legal documents**, ensuring accurate and complete information.  
   åœ¨è®¸å¤šé¢†åŸŸï¼Œäººå·¥æ™ºèƒ½åœ¨ä»»åŠ¡æ–¹é¢èƒœè¿‡äººç±»ï¼Œå°¤å…¶æ˜¯é‚£äº›é‡å¤æ€§å’Œæ³¨é‡ç»†èŠ‚çš„ä»»åŠ¡ã€‚äººå·¥æ™ºèƒ½å·¥å…·å¯ä»¥å¿«é€Ÿåˆ†æå¤§é‡çš„æ³•å¾‹æ–‡ä»¶ï¼Œç¡®ä¿å‡†ç¡®å’Œå®Œæ•´çš„ä¿¡æ¯

4. Efficiency and Accuracy
   AI tools can **complete tasks quickly and with relatively few errors**, particularly in areas that require analysing extensive data sets. This enables businesses to gain insights into their operations that may have otherwise gone unnoticed.  
   äººå·¥æ™ºèƒ½å·¥å…·å¯ä»¥å¿«é€Ÿå®Œæˆä»»åŠ¡ï¼Œé”™è¯¯ç›¸å¯¹è¾ƒå°‘ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åˆ†æå¤§é‡æ•°æ®é›†çš„é¢†åŸŸã€‚è¿™ä½¿å¾—ä¼ä¸šèƒ½å¤Ÿæ·±å…¥äº†è§£ä»–ä»¬çš„ä¸šåŠ¡ï¼Œå¦åˆ™å¯èƒ½ä¼šè¢«å¿½è§†ã€‚

5. Generative AI Tools
   The growing population of generative AI tools holds great importance in fields like education, marketing, and product design. These tools offer **innovative solutions and creative outputs**.  
   è¶Šæ¥è¶Šå¤šçš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·¥å…·åœ¨æ•™è‚²ã€å¸‚åœºè¥é”€å’Œäº§å“è®¾è®¡ç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿™äº›å·¥å…·æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆå’Œåˆ›é€ æ€§çš„äº§å‡ºã€‚

##### AI opens the door to new opportunities äººå·¥æ™ºèƒ½ä¸ºæ–°çš„æœºä¼šæ‰“å¼€äº†å¤§é—¨

- UBER

- Meta

- Microsoft

- Alphabet

- Apple

##### The advantages of AI äººå·¥æ™ºèƒ½çš„ä¼˜åŠ¿

1. Good at detail-oriented jobs  
   æ“…é•¿ç»†èŠ‚å¯¼å‘çš„å·¥ä½œ

2. Saves labour and increases productivity  
   èŠ‚çœåŠ³åŠ¨åŠ›ï¼Œæé«˜ç”Ÿäº§åŠ›

3. Delivers consistent results  
   äº§ç”Ÿä¸€è‡´çš„ç»“æœ

4. AI-powered virtual agents are always available  
   äººå·¥æ™ºèƒ½é©±åŠ¨çš„è™šæ‹Ÿä»£ç†æ€»æ˜¯å¯ç”¨çš„

5. Reduced time for data-heavy tasks  
   å‡å°‘æ•°æ®é‡å¤§çš„ä»»åŠ¡çš„æ—¶é—´

6. Can improve customer satisfaction through personalization  
   å¯ä»¥é€šè¿‡ä¸ªæ€§åŒ–æé«˜å®¢æˆ·æ»¡æ„åº¦

##### AI is NOT everything (limitations) äººå·¥æ™ºèƒ½ä¸æ˜¯ä¸€åˆ‡(å±€é™æ€§)

1. Expensive  
   æ˜‚è´µçš„

2. Requires deep technical expertise  
   éœ€è¦æ·±åšçš„ä¸“ä¸šæŠ€æœ¯

3. Limited supply of qualified workers to build AI tools  
   äººå·¥æ™ºèƒ½å·¥å…·çš„åˆæ ¼å·¥äººä¾›åº”æœ‰é™

4. Reflects the biases of its  training data, at scale.  
   åœ¨è§„æ¨¡ä¸Šåæ˜ äº†å…¶è®­ç»ƒæ•°æ®çš„åå·®ã€‚

5. Lack of ability to generalize from one task to another  
   ç¼ºä¹ä»ä¸€é¡¹ä»»åŠ¡å½’çº³åˆ°å¦ä¸€é¡¹ä»»åŠ¡çš„èƒ½åŠ›

6. Eliminates human jobs, increasing unemployment rates  
   å‡å°‘äººç±»å·¥ä½œï¼Œå¢åŠ å¤±ä¸šç‡

#### Brief history of AI äººå·¥æ™ºèƒ½ç®€å²

<u>Ancient Roots of Intelligent Artifacts
æ™ºèƒ½ç‰©å“çš„å¤è€æ ¹æº</u>

##### From Mythical Servants æ¥è‡ªç¥è¯ä»†äºº

- The concept of inanimate objects endowed with intelligence has been around since ancient times.  
  è¢«èµ‹äºˆæ™ºæ…§çš„æ— ç”Ÿå‘½ç‰©ä½“çš„æ¦‚å¿µè‡ªå¤ä»¥æ¥å°±å­˜åœ¨ã€‚
  
  - Greek god Hephaestus and robot-like servants out of gold   
    å¸Œè…Šç¥èµ«è²æ–¯æ‰˜æ–¯å’Œæœºå™¨äººèˆ¬çš„ä»†äººç”¨é‡‘å­åšçš„
  
  - Engineers in ancient Egypt and statues  of gods animated by priests  
    å¤åŸƒåŠçš„å·¥ç¨‹å¸ˆå’Œç¥­å¸åˆ¶ä½œçš„ç¥åƒ

##### To Symbolic Thinkers å¯¹è±¡å¾æ€æƒ³å®¶çš„æ€è€ƒ

- They used the tools and logic of their times to describe human thought processes as symbols, laying the foundation for AI concepts such as general knowledge representation.  
  ä»–ä»¬åˆ©ç”¨å½“æ—¶çš„å·¥å…·å’Œé€»è¾‘å°†äººç±»çš„æ€ç»´è¿‡ç¨‹æè¿°ä¸ºç¬¦å·ï¼Œä¸ºä¸€èˆ¬çŸ¥è¯†è¡¨ç¤ºç­‰äººå·¥æ™ºèƒ½æ¦‚å¿µå¥ å®šäº†åŸºç¡€ã€‚

- Aristotle äºšé‡Œå£«å¤šå¾·
  Ramon Llull æ‹‰è’™Â·æŸ³åˆ©
  RenÃ© Descartes å‹’å†… Â· ç¬›å¡å°”
  Thomas Bayes æ‰˜é©¬æ–¯Â·è´å¶æ–¯

##### Pioneers of Programmable Machines å¯ç¼–ç¨‹æœºå™¨çš„å…ˆé©±

The foundational work that would give rise to the modern computer  
äº§ç”Ÿç°ä»£è®¡ç®—æœºçš„åŸºç¡€å·¥ä½œ

- the mill with a printing mechanism of the Analytical Engine å¸¦æœ‰åˆ†ææœºæ‰“å°æœºæ„çš„ç£¨åŠ

- Babbage's difference engine å·´è´å¥‡çš„å·®åˆ†å¼•æ“

##### Milestones in the Journey of AI

```mermaid
flowchart TB
    subgraph A["1940s â€”â€” Emergence of AI"]
      direction LR
      AA("John Von Neumann
      Von Neumann Architecture")
      AB("Warren McCulloch and Walter Pitts
      McCulloch-Pitts Neuron")
      AA-->AB
    end
    subgraph B["1950s â€”â€” Emergence of AI"]
    direction LR
    BA("Alan Turing
    Turing Test")
    end
    subgraph C["1956 â€”â€” Birth of modern AI"]
    direction LR
    CA("The summer Dartmouth conference")
    CB("Allen Newell and Herbert A. Simon 
    â€”â€” Logic Theorist,
    proving certain mathematical theorems 
    and referred to as the first AI program.")
    CA-->CB
    end
    subgraph D["1950s-1960s â€”â€” Birth of modern AI"]
    direction LR
    DA("Lisp, a language for AI programming")
    DB("ELIZA, an early NLP program")
    DC("The General Problem Solver (GPS) algorithm")
    DA-->DB-->DC
    end
    subgraph E["1970s-1980s â€”â€” AI winters"]
    direction TB
    EAA("The achievement of AI proved elusive, not imminent")
    EAB("hampered by limitations in computer processing and memory")
    EAC("the complexity of the problem.")
    EB("Government and corporations backed away from their support of AI research")
    EC("1974 to 1980 known as the first â€œAI Winterâ€.")
    ED("the second â€œAI winterâ€ lasted until the mid-1990s")
    EE("a short-lived new wave of AI enthusiasm in 1980s")
    EAA-->EB
    EAB-->EB
    EAC-->EB
    EB-->EC-->EE-->ED
    end
    subgraph F["1990s â€”â€” AI Renaissance"]
    direction LR
    FA("The Increases in computational power and an explosion of data sparked an AI renaissance in the late 1990s.")
    FB("In 1997, IBM's Deep Blue defeated Russian chess grandmaster Garry Kasparov, 
becoming the first computer program to beat a world chess champion.")
    FA-->FB
    end
    subgraph G["2000s â€”â€” AI in our lives"]
    direction LR
    GA("Further advances in machine learning, deep learning, NLP, speech recognition and computer vision gave rise to products and services.")
    GBA("Google search")
    GBB("NETFLIX recommendation system")
    GBC("Meta Face recognition")
    GBD("Microsoft AI")
    GBE("Driverless cars")
    GA-->GBA
    GA-->GBB
    GA-->GBC
    GA-->GBD
    GA-->GBE
    end
    subgraph H["2010s â€”â€” AI breakthroughs"]
    direction LR
    HA("Siri")
    HB("Amazon Alexa")
    HC("Google DeepMind")
    HD("Deep faking")
    HF("GPT-3")
    HG("TensorFlow")
    HI("Computer Vision")
    HA<-->HB<-->HC<-->HD<-->HE<-->HF<-->HG<-->HI
    end
    subgraph I["2020s â€”â€” Generative AI"]
    direction LR
    IAA("Generative AI starts with a prompt that could be in the multiple forms. ")
    IAB("Various AI algorithms then return new content in response to the prompt.")
    IB("The technology is still in early stages, as evidenced by its tendency to hallucinate or skew answers.")
    IAA-->IB
    IAB-->IB
    end
    A-->B-->C-->D-->E-->F-->G-->H-->I


```

```mermaid
flowchart TB
    subgraph A["1940s â€”â€” äººå·¥æ™ºèƒ½çš„å‡ºç°"]
      direction LR
      AA("çº¦ç¿°Â·å†¯Â·è¯ºä¾æ›¼
      å†¯Â·è¯ºä¾æ›¼ä½“ç³»ç»“æ„")
      AB("éº¦å¡æ´›å…‹ å’Œ æ²ƒå°”ç‰¹Â·çš®èŒ¨
      McCulloch-Pitts ç¥ç»å…ƒ")
      AA-->AB
    end
    subgraph B["1950s â€”â€” äººå·¥æ™ºèƒ½çš„å‡ºç°"]
    direction LR
    BA("é˜¿å…°Â·å›¾çµ
    å›¾çµæµ‹è¯•")
    end
    subgraph C["1956 â€”â€” ç°ä»£äººå·¥æ™ºèƒ½çš„è¯ç”Ÿ"]
    direction LR
    CA("è¾¾ç‰¹èŒ…æ–¯å¤å­£ä¼šè®®")
    CB("è‰¾ä¼¦Â·çº½å„å°” and èµ«ä¼¯ç‰¹Â·è¥¿è’™ 
    â€”â€” é€»è¾‘ç†è®ºå®¶,
    è¯æ˜æŸäº›æ•°å­¦å®šç† 
    è¢«ç§°ä¸ºç¬¬ä¸€ä¸ªäººå·¥æ™ºèƒ½ç¨‹åºã€‚")
    CA-->CB
    end
    subgraph D["1950s-1960s â€”â€” ç°ä»£äººå·¥æ™ºèƒ½çš„è¯ç”Ÿ"]
    direction LR
    DA("Lispï¼Œäººå·¥æ™ºèƒ½ç¼–ç¨‹è¯­è¨€")
    DB("ELIZAï¼Œä¸€ä¸ªæ—©æœŸçš„ NLP ç¨‹åº")
    DC("ä¸€èˆ¬é—®é¢˜è§£å†³å™¨(GPS)ç®—æ³•")
    DA-->DB-->DC
    end
    subgraph E["1970s-1980s â€”â€” AI å¯’å†¬"]
    direction TB
    EAA("äººå·¥æ™ºèƒ½çš„æˆå°±è¢«è¯æ˜æ˜¯éš¾ä»¥æ‰æ‘¸çš„ï¼Œè€Œä¸æ˜¯è¿«åœ¨çœ‰ç«çš„")
    EAB("å—åˆ°è®¡ç®—æœºå¤„ç†å’Œå†…å­˜é™åˆ¶çš„é˜»ç¢")
    EAC("é—®é¢˜çš„å¤æ‚æ€§ã€‚")
    EB("æ”¿åºœå’Œä¼ä¸šæ”¾å¼ƒäº†å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ”¯æŒ")
    EC("1974å¹´è‡³1980å¹´è¢«ç§°ä¸ºç¬¬ä¸€ä¸ªâ€œäººå·¥æ™ºèƒ½å†¬å¤©â€ã€‚")
    ED("ç¬¬äºŒä¸ªâ€œäººå·¥æ™ºèƒ½å†¬å¤©â€ä¸€ç›´æŒç»­åˆ°ä¸Šä¸–çºª90å¹´ä»£ä¸­æœŸ")
    EE("20ä¸–çºª80å¹´ä»£çŸ­æš‚çš„äººå·¥æ™ºèƒ½çƒ­æ½®")
    EAA-->EB
    EAB-->EB
    EAC-->EB
    EB-->EC-->EE-->ED
    end
    subgraph F["1990s â€”â€” AI çš„æ–‡è‰ºå¤å…´"]
    direction LR
    FA("è®¡ç®—èƒ½åŠ›çš„æé«˜å’Œæ•°æ®çš„çˆ†ç‚¸å¼å¢é•¿åœ¨20ä¸–çºª90å¹´ä»£æœ«å¼•å‘äº†äººå·¥æ™ºèƒ½çš„å¤å…´ã€‚")
    FB("1997å¹´ï¼ŒIBM çš„â€œæ·±è“â€å‡»è´¥äº†ä¿„ç½—æ–¯å›½é™…è±¡æ£‹å¤§å¸ˆåŠ é‡Œ Â· å¡æ–¯å¸•ç½—å¤«,
æˆä¸ºç¬¬ä¸€ä¸ªå‡»è´¥ä¸–ç•Œè±¡æ£‹å† å†›çš„è®¡ç®—æœºç¨‹åºã€‚")
    FA-->FB
    end
    subgraph G["2000s â€”â€” æˆ‘ä»¬ç”Ÿæ´»ä¸­çš„äººå·¥æ™ºèƒ½"]
    direction LR
    GA("æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰çš„è¿›ä¸€æ­¥å‘å±•å¸¦æ¥äº†äº§å“å’ŒæœåŠ¡ã€‚")
    GBA("Google search")
    GBB("NETFLIX recommendation system")
    GBC("Meta Face recognition")
    GBD("Microsoft AI")
    GBE("Driverless cars")
    GA-->GBA
    GA-->GBB
    GA-->GBC
    GA-->GBD
    GA-->GBE
    end
    subgraph H["2010s â€”â€” äººå·¥æ™ºèƒ½çš„çªç ´"]
    direction LR
    HA("Siri")
    HB("Amazon Alexa")
    HC("Google DeepMind")
    HD("Deep faking")
    HF("GPT-3")
    HG("TensorFlow")
    HI("Computer Vision")
    HA<-->HB<-->HC<-->HD<-->HE<-->HF<-->HG<-->HI
    end
    subgraph I["2020s â€”â€” ç”Ÿæˆå¼AI"]
    direction LR
    IAA("ç”Ÿæˆå¼ AI ä»ä¸€ä¸ªå¯ä»¥æ˜¯å¤šç§å½¢å¼çš„æç¤ºå¼€å§‹ã€‚")
    IAB("ç„¶åå„ç§ AI ç®—æ³•å“åº”æç¤ºè¿”å›æ–°å†…å®¹ã€‚")
    IB("è¿™é¡¹æŠ€æœ¯ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œå…¶äº§ç”Ÿå¹»è§‰æˆ–æ‰­æ›²ç­”æ¡ˆçš„å€¾å‘å°±æ˜¯æ˜è¯ã€‚")
    IAA-->IB
    IAB-->IB
    end
    A-->B-->C-->D-->E-->F-->G-->H-->I


```

#### Branches and applications of AI äººå·¥æ™ºèƒ½çš„åˆ†æ”¯ä¸åº”ç”¨

##### Weak AI vs. Strong AI

- Weak AI
  
  - also called Narrow AI or Artificial Narrow Intelligence (ANI)
  
  - is **AI trained and focused to perform specific tasks.** 
  
  - Weak AI drives most of the AI that surrounds us today. â€˜Narrowâ€™ might be a more accurate descriptor for this type of AI as it is anything but weak; it enables some very robust applications.  
    ä»Šå¤©æˆ‘ä»¬å‘¨å›´çš„å¤§éƒ¨åˆ†äººå·¥æ™ºèƒ½éƒ½æ˜¯ç”±å¼±äººå·¥æ™ºèƒ½é©±åŠ¨çš„ã€‚â€œçª„â€å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å‡†ç¡®çš„æè¿°è¿™ç§ç±»å‹çš„äººå·¥æ™ºèƒ½ï¼Œå› ä¸ºå®ƒæ˜¯ä»»ä½•ä¸œè¥¿ï¼Œä½†å¼±ï¼Œå®ƒä½¿ä¸€äº›éå¸¸å¥å£®çš„åº”ç”¨ç¨‹åºã€‚

- Strong AI
  
  - made up of **Artificial General Intelligence (AGI)** and **Artificial Super Intelligence (ASI)**.  
    ç”±äººå·¥é€šç”¨æ™ºèƒ½(AGI)å’Œäººå·¥è¶…çº§æ™ºèƒ½(ASI)ç»„æˆã€‚
  
  - AGI, or general AI, is a theoretical form of AI where a machine would have **an intelligence equal to humans**; it would have a **self-aware consciousness** that has the ability to solve problems, learn, and plan for the future.   
    äººå·¥æ™ºèƒ½(AGI)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ç§ç†è®ºå½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œæœºå™¨æ‹¥æœ‰ä¸äººç±»ç›¸å½“çš„æ™ºèƒ½; å®ƒå…·æœ‰è‡ªæˆ‘æ„è¯†ï¼Œèƒ½å¤Ÿè§£å†³é—®é¢˜ã€å­¦ä¹ å’Œè§„åˆ’æœªæ¥ã€‚
  
  - ASIâ€”also known as superintelligenceâ€”would **surpass the intelligence and ability of the human brain**.   
    äººå·¥æ™ºèƒ½ãƒ¼ãƒ¼ä¹Ÿè¢«ç§°ä¸ºè¶…çº§æ™ºèƒ½ãƒ¼ãƒ¼å°†è¶…è¶Šäººç±»å¤§è„‘çš„æ™ºåŠ›å’Œèƒ½åŠ›ã€‚
  
  - While strong AI is still entirely theoretical with no practical examples in use today, that doesn't mean AI researchers aren't also exploring its development.  
    è™½ç„¶å¼ºå¤§çš„äººå·¥æ™ºèƒ½ä»ç„¶å®Œå…¨æ˜¯ç†è®ºä¸Šçš„ï¼Œæ²¡æœ‰å®é™…åº”ç”¨çš„ä¾‹å­ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€äººå·¥æ™ºèƒ½ç ”ç©¶äººå‘˜æ²¡æœ‰æ¢ç´¢å®ƒçš„å‘å±•ã€‚

##### Four Types of AI

- <u>Type 1: Reactive machines ç±»å‹1: æ´»æ€§æœºå™¨</u>
  
  - have no memory æ²¡æœ‰è®°å¿†
  
  - task-specific åªèƒ½æ‰§è¡Œç‰¹å®šä»»åŠ¡
  
  - EXP.
    
    - An example is **Deep Blue**, the IBM chess program that beat Garry Kasparov in the 1990s.  
      ä¸€ä¸ªä¾‹å­æ˜¯æ·±è“(Deep Blue) ï¼ŒIBM çš„å›½é™…è±¡æ£‹ç¨‹åºåœ¨ä¸Šä¸–çºª90å¹´ä»£å‡»è´¥äº†åŠ é‡Œâ€¢å¡æ–¯å¸•ç½—å¤«(Garry Kasparov)ã€‚
    
    - Deep Blue can identify pieces on a chessboard and make predictions, but because it has no memory, it **cannot use past experiences to inform future ones.**  
      æ·±è“å¯ä»¥è¯†åˆ«æ£‹ç›˜ä¸Šçš„æ£‹å­å¹¶åšå‡ºé¢„æµ‹ï¼Œä½†æ˜¯å› ä¸ºå®ƒæ²¡æœ‰è®°å¿†ï¼Œæ‰€ä»¥å®ƒä¸èƒ½ç”¨è¿‡å»çš„ç»å†æ¥å‘Šè¯‰æœªæ¥çš„ç»å†ã€‚

- <u>Type 2: Limited memor Â ç¬¬2ç±»: è®°å¿†åŠ›æœ‰é™</u>
  
  - have memory æ‹¥æœ‰è®°å¿†  
  
  - use past experiences to inform future decisions.   
    åˆ©ç”¨è¿‡å»çš„ç»éªŒä¸ºå°†æ¥çš„å†³ç­–æä¾›ä¾æ®ã€‚  
    Some of the decision-making functions in self-driving cars are designed this way.  
    è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„ä¸€äº›å†³ç­–åŠŸèƒ½å°±æ˜¯è¿™æ ·è®¾è®¡çš„ã€‚

- <u>Type 3: Theory of mind ç±»å‹3: å¿ƒç†ç†è®º</u>
  
  - have the social intelligence to **understand emotions**  
    å…·æœ‰ç†è§£æƒ…æ„Ÿçš„ç¤¾ä¼šæ™ºæ…§
  
  - This type of AI will be able to infer human intentions and predict behavior, a necessary skill for AI systems to become integral members of human teams.  
    è¿™ç§ç±»å‹çš„äººå·¥æ™ºèƒ½å°†èƒ½å¤Ÿæ¨æ–­äººç±»çš„æ„å›¾å’Œé¢„æµ‹è¡Œä¸ºï¼Œè¿™æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿæˆä¸ºäººç±»å›¢é˜Ÿä¸å¯æˆ–ç¼ºçš„æˆå‘˜æ‰€å¿…éœ€çš„æŠ€èƒ½ã€‚

- <u>Type 4: Self-awareness ç±»å‹4: è‡ªæˆ‘æ„è¯†</u>
  
  - have **a sense of self**, which gives them consciousness.   
    æœ‰è‡ªæˆ‘æ„è¯†ï¼Œè¿™ç»™äº†ä»–ä»¬æ„è¯†ã€‚
  
  - understand their own current stateThis type of AI does not yet exist.  
    äº†è§£è‡ªå·±çš„ç°çŠ¶è¿™ç§ç±»å‹çš„äººå·¥æ™ºèƒ½å°šä¸å­˜åœ¨ã€‚

##### Relationship between artificial intelligence, machine learning, and deep learning äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ çš„å…³ç³»

<img title="" src="./images/____.png" alt="loading-ag-1220" data-align="inline">

##### How machine learning works?

- â¢ Models æ¨¡å‹
  
  - Assumptions to be mapped to the learning problem  
    æ˜ å°„åˆ°å­¦ä¹ é—®é¢˜çš„å‡è®¾
  
  - **(problem modelling, defining the assumption space)**   
    (é—®é¢˜å»ºæ¨¡ï¼Œå®šä¹‰å‡è®¾ç©ºé—´)

- â¢ Strategies ç­–ç•¥
  
  - Criteria for learning/selecting the optimal model from the hypothesis space  
    ä»å‡è®¾ç©ºé—´å­¦ä¹ /é€‰æ‹©æœ€ä¼˜æ¨¡å‹çš„å‡†åˆ™
  
  - **(Determine objective function)**  
    (ç¡®å®šç›®æ ‡å‡½æ•°)

- â¢ Algorithm ç®—æ³•
  
  - Specific calculations for solving the optimal model based on the objective function  
    åŸºäºç›®æ ‡å‡½æ•°æ±‚è§£æœ€ä¼˜æ¨¡å‹çš„å…·ä½“è®¡ç®—
  
  - **(solving for model parameters)**   
    (æ¨¡å‹å‚æ•°æ±‚è§£)

##### Classification of models by data label

- Data Label
  
  - Supervised learning ç›‘ç£å­¦ä¹ 
    Supervised learning samples have labels (output targets); learns labelled interfaces from data (input-output mapping function), suitable for predictive data labelling  
    ç›‘ç£å¼å­¦ä¹ æ ·æœ¬æœ‰æ ‡ç­¾(è¾“å‡ºç›®æ ‡) ï¼Œä»æ•°æ®ä¸­å­¦ä¹ æœ‰æ ‡ç­¾çš„ç•Œé¢(è¾“å…¥è¾“å‡ºæ˜ å°„åŠŸèƒ½) ï¼Œé€‚ç”¨äºé¢„æµ‹æ€§æ•°æ®æ ‡ç­¾
    
    - åˆ†ç±» classfication
  
  - unsupervised learning æ— ç›‘ç£å­¦ä¹ 
    Unsupervised learning samples have no labelling; learns patterns from data, suitable for describing data  
    éç›‘ç£å¼å­¦ä¹ æ ·æœ¬æ²¡æœ‰æ ‡ç­¾ï¼Œä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œé€‚åˆæè¿°æ•°æ®
    
    - èšç±» clustering
  
  - Semi-supervised learning åŠç›‘ç£å­¦ä¹  ï¼ˆä¸ç®—åˆ°ä¸‰ç§é‡Œé¢ï¼Œè€Œæ˜¯ä½œä¸ºå‰ä¸¤ç§çš„èåˆï¼‰
    
    - Starting point: labelled samples difficult to obtain, unlabelled samples relatively inexpensive.  
      èµ·å§‹ç‚¹: æ ‡è®°æ ·å“éš¾ä»¥è·å¾—ï¼Œæœªæ ‡è®°æ ·å“ç›¸å¯¹ä¾¿å®œã€‚
    
    - Idea: Assume that unlabelled samples are independently and identically distributed with labelled samples, i.e., contain important information about the distribution of the data  
      æƒ³æ³•: å‡è®¾æœªæ ‡è®°çš„æ ·å“ä¸æ ‡è®°çš„æ ·å“åˆ†å¸ƒç‹¬ç«‹ä¸”ç›¸åŒï¼Œå³å«æœ‰å…³äºæ•°æ®åˆ†å¸ƒçš„é‡è¦ä¿¡æ¯

```mermaid
flowchart LR
    A[["large amount of unlabeled data"]]
    B[["small amount of labeled data"]]
    C("machine learning model")
    D[["Pseudo-labeled Dataset"]]
    E(ML model trained on pseudo labeled data and labeled data)
    F>"it will be either an apple or a pineapple"]
    A-->|"learning process"|C
    B-->C
    C-->D-->E
    E-->|"predicted"|F
```

```mermaid
flowchart LR

    G[["data to be predicted"]]
    H("Trained Machine learning model")
    I>"two groups are identified, Now a human could label group 1 as apples and group 2 as pineapples"]
    G-->H-->I
```

- Reinforcement Learning å¼ºåŒ–å­¦ä¹ 
  uses unlabelled data but can know whether it is getting closer or further away from the goal (rewarding feedback)  
  ä½¿ç”¨æœªæ ‡è®°çš„æ•°æ®ï¼Œä½†å¯ä»¥çŸ¥é“å®ƒæ˜¯å¦ç¦»ç›®æ ‡è¶Šæ¥è¶Šè¿‘æˆ–è¶Šæ¥è¶Šè¿œ(å¥–åŠ±åé¦ˆ)

- ##### Use cases of AI technology  äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç”¨ä¾‹
1. Automation: AI technologies paired with automation tools like robotic process automation (RPA) **automate repetitive, rules-based tasks, expanding task volume and types**.  
     è‡ªåŠ¨åŒ–: äººå·¥æ™ºèƒ½æŠ€æœ¯é…åˆè‡ªåŠ¨åŒ–å·¥å…·ï¼Œå¦‚æœºå™¨äººè¿‡ç¨‹è‡ªåŠ¨åŒ–(RPA)è‡ªåŠ¨åŒ–é‡å¤ï¼ŒåŸºäºè§„åˆ™çš„ä»»åŠ¡ï¼Œæ‰©å¤§ä»»åŠ¡é‡å’Œç±»å‹ã€‚

2. Machine Learning: Enables computers to **act without explicit programming**. Deep learning automates **predictive analytics**.  
   æœºå™¨å­¦ä¹ : ä½¿è®¡ç®—æœºä¸éœ€è¦ç¼–ç¨‹å°±èƒ½è¿è¡Œã€‚æ·±åº¦å­¦ä¹ ä½¿é¢„æµ‹åˆ†æè‡ªåŠ¨åŒ–ã€‚  

3. Computer Vision (CV): Gives machines the ability to **see and analyse visual information** using cameras and digital signal processing.  
   è®¡ç®—æœºè§†è§‰(CV) : ä½¿æœºå™¨èƒ½å¤Ÿçœ‹åˆ°å’Œåˆ†æè§†è§‰ä¿¡æ¯ä½¿ç”¨ç›¸æœºå’Œæ•°å­—ä¿¡å·å¤„ç†ã€‚

4. Natural Language Processing (NLP): **Processes human language by computer programs**, including tasks like translation, sentiment analysis, and speech recognition.   
   è‡ªç„¶è¯­è¨€å¤„ç†(NLP) : é€šè¿‡è®¡ç®—æœºç¨‹åºå¤„ç†äººç±»è¯­è¨€ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æå’Œè¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ã€‚

5. Robotics: Engineering field focused on designing and manufacturing robots for tasks challenging for humans or requiring consistent performance.  
   æœºå™¨äººå­¦: å·¥ç¨‹é¢†åŸŸä¸“æ³¨äºè®¾è®¡å’Œåˆ¶é€ æœºå™¨äººæ¥å®Œæˆå¯¹äººç±»å…·æœ‰æŒ‘æˆ˜æ€§æˆ–éœ€è¦ä¸€è‡´æ€§èƒ½çš„ä»»åŠ¡ã€‚  

6. Self-Driving Cars: Utilize computer vision, image recognition, and deep learning to navigate roads and avoid obstacles.   
   è‡ªåŠ¨é©¾é©¶æ±½è½¦: åˆ©ç”¨è®¡ç®—æœºè§†è§‰ã€å›¾åƒè¯†åˆ«å’Œæ·±åº¦å­¦ä¹ æ¥é©¾é©¶é“è·¯å’Œé¿å¼€éšœç¢ç‰©ã€‚

7. Text, Image, and Audio Generation: Generative AI techniques create various media types based on text prompts, applied extensively across businesses for content creation.  
   æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆ: ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯åŸºäºæ–‡æœ¬æç¤ºåˆ›å»ºå„ç§åª’ä½“ç±»å‹ï¼Œå¹¿æ³›åº”ç”¨äºä¼ä¸šå†…å®¹åˆ›å»ºã€‚

##### AI applications

1. Healthcare: AI is used to improve diagnoses, mine patient data, and assist with administrative tasks like scheduling appointments.  
   åŒ»ç–—ä¿å¥: äººå·¥æ™ºèƒ½ç”¨äºæ”¹å–„è¯Šæ–­ï¼ŒæŒ–æ˜æ‚£è€…æ•°æ®ï¼Œå¹¶ååŠ©è¡Œæ”¿ä»»åŠ¡ï¼Œå¦‚å®‰æ’é¢„çº¦ã€‚

2. Business: Machine learning and chatbots enhance customer service, while generative AI has the potential to revolutionize product design and disrupt business models.  
   ä¸šåŠ¡: æœºå™¨å­¦ä¹ å’ŒèŠå¤©æœºå™¨äººæé«˜äº†å®¢æˆ·æœåŠ¡ï¼Œè€Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æœ‰å¯èƒ½å½»åº•æ”¹é©äº§å“è®¾è®¡å’Œé¢ è¦†å•†ä¸šæ¨¡å¼ã€‚

3. Education: AI automates grading, adapts to student needs, and provides additional support. It also aids in crafting course materials and changing the learning process.  
   æ•™è‚²: äººå·¥æ™ºèƒ½è‡ªåŠ¨è¯„åˆ†ï¼Œé€‚åº”å­¦ç”Ÿçš„éœ€è¦ï¼Œå¹¶æä¾›é¢å¤–çš„æ”¯æŒã€‚å®ƒè¿˜æœ‰åŠ©äºç²¾å¿ƒåˆ¶ä½œè¯¾ç¨‹ææ–™å’Œæ”¹å˜å­¦ä¹ è¿‡ç¨‹ã€‚

4. Finance: AI disrupts the financial industry through personal finance applications, automated trading, and the buying process for homes.  
   é‡‘è: äººå·¥æ™ºèƒ½é€šè¿‡ä¸ªäººç†è´¢åº”ç”¨ã€è‡ªåŠ¨äº¤æ˜“å’Œè´­æˆ¿è¿‡ç¨‹æ‰°ä¹±äº†é‡‘èä¸šã€‚

5. Law: AI assists with legal processes such as document classification, data description, and outcome prediction.  
   æ³•å¾‹: äººå·¥æ™ºèƒ½ååŠ©æ³•å¾‹ç¨‹åºï¼Œå¦‚æ–‡æ¡£åˆ†ç±»ï¼Œæ•°æ®æè¿°å’Œç»“æœé¢„æµ‹ã€‚

6. Entertainment and Media: AI is used for targeted advertising, content recommendation, script creation, automated journalism, and movie production.  
   å¨±ä¹å’Œåª’ä½“: äººå·¥æ™ºèƒ½ç”¨äºå®šå‘å¹¿å‘Šã€å†…å®¹æ¨èã€å‰§æœ¬åˆ›ä½œã€è‡ªåŠ¨åŒ–æ–°é—»å’Œç”µå½±åˆ¶ä½œã€‚

7. Software Coding and IT Processes: Generative AI tools aid in code generation, while AI automates IT processes like data entry and security measures.  
   è½¯ä»¶ç¼–ç å’Œ IT è¿‡ç¨‹: ç”Ÿæˆ AI å·¥å…·å¸®åŠ©ä»£ç ç”Ÿæˆï¼Œè€Œ AI è‡ªåŠ¨åŒ– IT è¿‡ç¨‹ï¼Œå¦‚æ•°æ®è¾“å…¥å’Œå®‰å…¨æªæ–½ã€‚

8. Security: AI is applied to cybersecurity for threat detection, anomaly detection, and behavior analytics.  
   å®‰å…¨æ€§: äººå·¥æ™ºèƒ½åº”ç”¨äºç½‘ç»œå®‰å…¨ï¼Œç”¨äºå¨èƒæ£€æµ‹ã€å¼‚å¸¸æ£€æµ‹å’Œè¡Œä¸ºåˆ†æã€‚

9. Manufacturing: Robots collaborate with human workers in tasks previously done separately, increasing efficiency and multitasking capabilities.  
   åˆ¶é€ ä¸š: æœºå™¨äººä¸äººç±»å·¥äººåä½œå®Œæˆä»¥å‰å•ç‹¬å®Œæˆçš„ä»»åŠ¡ï¼Œæé«˜æ•ˆç‡å’Œå¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚

10. Banking: Chatbots and virtual assistants improve customer service and compliance with regulations, while AI aids in decision-making for loans and investments.  
    é“¶è¡Œä¸šåŠ¡: èŠå¤©æœºå™¨äººå’Œè™šæ‹ŸåŠ©ç†æ”¹å–„å®¢æˆ·æœåŠ¡å’Œéµå®ˆè§„å®šï¼Œè€Œäººå·¥æ™ºèƒ½ååŠ©è´·æ¬¾å’ŒæŠ•èµ„å†³ç­–ã€‚

11. Transportation: AI manages traffic, predicts flight delays, enhances supply chain management, and promotes safer and more efficient transportation methods.  
    è¿è¾“: äººå·¥æ™ºèƒ½ç®¡ç†äº¤é€šï¼Œé¢„æµ‹èˆªç­å»¶è¯¯ï¼ŒåŠ å¼ºä¾›åº”é“¾ç®¡ç†ï¼Œä¿ƒè¿›æ›´å®‰å…¨å’Œæ›´æœ‰æ•ˆçš„è¿è¾“æ–¹æ³•ã€‚

#### Ethical considerations in AIÂ  äººå·¥æ™ºèƒ½çš„ä¼¦ç†æ€è€ƒ ï¼ˆlimitationsï¼‰

##### Training Bias å«æœ‰æ­§è§†çš„è®­ç»ƒ

- AI systems can **perpetuate biases present in the training data**, which can lead to **unfair or discriminatory outcomes**.  
  äººå·¥æ™ºèƒ½ç³»ç»Ÿå¯èƒ½ä½¿åŸ¹è®­æ•°æ®ä¸­å­˜åœ¨çš„åè§é•¿æœŸå­˜åœ¨ï¼Œä»è€Œå¯¼è‡´ä¸å…¬å¹³æˆ–æ­§è§†æ€§çš„ç»“æœã€‚  

- Monitoring and addressing bias in machine learning algorithms is crucial to ensure fairness and avoid reinforcing existing inequalities.  
  ç›‘æµ‹å’Œå¤„ç†æœºå™¨å­¦ä¹ ç®—æ³•ä¸­çš„åå·®å¯¹äºç¡®ä¿å…¬å¹³æ€§å’Œé¿å…åŠ å‰§ç°æœ‰çš„ä¸å¹³ç­‰æ˜¯è‡³å…³é‡è¦çš„ã€‚

##### Misuse è¯¯ç”¨ï¼Œæ»¥ç”¨

- AI technology can be misused for malicious purposes  
  äººå·¥æ™ºèƒ½æŠ€æœ¯å¯èƒ½è¢«æ»¥ç”¨äºæ¶æ„ç›®çš„
  
  - creating deepfakes 
  
  - engaging in phishing attacks. è¿›è¡Œç½‘ç»œé’“é±¼æ”»å‡»ã€‚

- Safeguarding against misuse requires careful regulation and security measures.  
  é˜²æ­¢æ»¥ç”¨éœ€è¦è®¤çœŸçš„ç›‘ç®¡å’Œå®‰å…¨æªæ–½ã€‚

##### Interpretability å¯è§£é‡Šæ€§

- AI algorithms can be **difficult to interpret**. AIç®—æ³•éš¾ä»¥è¢«æ•°å­¦è§£é‡Š  
  
  - deep learning æ·±åº¦å­¦ä¹ 
  
  - generative adversarial network (GAN)  ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ

- This poses challenges in industries with regulatory compliance requirements, where **interpretability is necessary to meet legal obligations**.  
  è¿™å¯¹æœ‰å®ˆè§„è¦æ±‚çš„è¡Œä¸šæå‡ºäº†æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨è¿™äº›è¡Œä¸šï¼Œè§£é‡Šæ€§å¯¹äºå±¥è¡Œæ³•å¾‹ä¹‰åŠ¡æ˜¯å¿…è¦çš„ã€‚

##### Job Displacement å·¥ä½œè¢«æ›¿ä»£

- The automation enabled by AI can lead **to job losses and significant disruptions in the workforce.**   
  äººå·¥æ™ºèƒ½å¸¦æ¥çš„è‡ªåŠ¨åŒ–å¯èƒ½å¯¼è‡´å¤±ä¸šå’ŒåŠ³åŠ¨åŠ›å¤§é‡ä¸­æ–­ã€‚ 

- Preparing for the impact on employment and addressing the need for upskilling and reskilling becomes crucial.  
  ä¸ºå¯¹å°±ä¸šçš„å½±å“åšå¥½å‡†å¤‡ä»¥åŠè§£å†³æé«˜æŠ€èƒ½å’Œé‡æ–°æé«˜æŠ€èƒ½çš„éœ€è¦å˜å¾—è‡³å…³é‡è¦ã€‚

##### Legal Concerns æ³•å¾‹é—®é¢˜

- AI raises legal issues, including potential cases of **AI-generated libel and copyright infringement**.   
  AI æå‡ºäº†æ³•å¾‹é—®é¢˜ï¼ŒåŒ…æ‹¬å¯èƒ½å‡ºç°çš„ç”±AIå¼•å‘çš„è¯½è°¤å’Œç›—ç‰ˆæ¡ˆä»¶ ã€‚

- Developing appropriate legal frameworks and regulations to address these concerns is essential.  
  å¿…é¡»åˆ¶å®šé€‚å½“çš„æ³•å¾‹æ¡†æ¶å’Œæ¡ä¾‹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

##### Data Privacy æ•°æ®éšç§

- AI applications often **rely on vast amounts of sensitive data**, particularly in fields like banking, healthcare, and law.  
  äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºé€šå¸¸ä¾èµ–äºå¤§é‡çš„æ•æ„Ÿæ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨é“¶è¡Œã€åŒ»ç–—ä¿å¥å’Œæ³•å¾‹ç­‰é¢†åŸŸã€‚

- Ensuring proper data privacy protections and adhering to relevant regulations is crucial to safeguard individualsâ€™ privacy.  
  ç¡®ä¿é€‚å½“çš„æ•°æ®éšç§ä¿æŠ¤å’Œéµå®ˆç›¸å…³æ³•è§„å¯¹äºä¿æŠ¤ä¸ªäººéšç§è‡³å…³é‡è¦ã€‚

##### Address è§£å†³æ–¹æ³•

- responsible AI development è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å¼€å‘

- robust regulations å¼ºæœ‰åŠ›çš„ç›‘ç®¡

- transparency é€æ˜ï¼Œé€æ˜æ€§

- ongoing monitoring æŒç»­ç›‘æµ‹

- stakeholder engagement åˆ©ç›Šç›¸å…³è€…å‚ä¸

- <u>Ethical considerations must be an integral part of the AI development process.</u>
  <u>é“å¾·è€ƒè™‘å¿…é¡»æ˜¯äººå·¥æ™ºèƒ½å¼€å‘è¿‡ç¨‹çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚</u>

---

### Part 2: Uncertainty in decision making å†³ç­–çš„ä¸ç¡®å®šæ€§

- Logic and uncertainty é€»è¾‘å’Œä¸ç¡®å®šæ€§

- Probability theory æ¦‚ç‡è®º

- Random Variables éšæœºå˜é‡

- Bayes rule and conditional independence è´å¶æ–¯è§„åˆ™å’Œæ¡ä»¶ç‹¬ç«‹

- Bayes (belief) network è´å¶æ–¯(ä¿¡å¿µ)ç½‘ç»œ

#### Logic and uncertainty é€»è¾‘å’Œä¸ç¡®å®šæ€§

```
- Aim 
  - To familiarise with uncertainty quantifications
  - To understand probabilistic reasoning and Bayes rule
- Outcome
  - Appreciate uncertainties
  - Quantification and reasoning using Probability
  - Probabilistic reasoning
  - Brief uncertain reasoning using 
  - Bayes Network
```

##### Major problem with logical-agent approaches ç”¨é€»è¾‘å»åº”ç”¨æ™ºèƒ½çš„ä¸»è¦é—®é¢˜

1. Agents almost never have access to the whole truth about theirenvironments  
   æ™ºèƒ½å‡ ä¹æ°¸è¿œæ— æ³•äº†è§£ä»–ä»¬æ‰€å¤„ç¯å¢ƒçš„å…¨éƒ¨çœŸç›¸

2. There are important questions for which there is no yes/no answer (even in simple terms)  
   æœ‰äº›é‡è¦çš„é—®é¢˜æ²¡æœ‰æ˜¯éå›ç­”(å³ä½¿æ˜¯ç®€å•çš„å›ç­”)

3. Therefore, an agent must reason under uncertainty.  
   å› æ­¤ï¼Œæ™ºèƒ½å¿…é¡»åœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹è¿›è¡Œæ¨ç†ã€‚

4. Uncertainty also arises because of an agentâ€™s incomplete or incorrect understanding of itsenvironment.  
   ä¸ç¡®å®šæ€§çš„äº§ç”Ÿä¹Ÿæ˜¯ç”±äºæ™ºèƒ½å¯¹å…¶ç¯å¢ƒçš„ä¸å®Œå…¨æˆ–ä¸æ­£ç¡®çš„ç†è§£ã€‚

##### Why application fails (when uncertainties are not considered appropriately) ? å®é™…åº”ç”¨ä¸ºä½•å¤±è´¥

1. <u>**LAZINESS**:</u> **too much work** to list the complete set ofantecedents or consequents needed to ensure an **exceptionless rule** and **too hard** to use such rules.  
   æ‡’æƒ°: ä¸ºäº†ç¡®ä¿ä¸€ä¸ªæ— ä¾‹å¤–çš„è§„åˆ™å’Œå¤ªéš¾ä½¿ç”¨è¿™æ ·çš„è§„åˆ™ï¼Œéœ€è¦åˆ—å‡ºä¸€æ•´å¥—å®Œæ•´çš„å‰å› åæœï¼Œå·¥ä½œé‡å¤ªå¤§ã€‚

2. <u>**THEORETICAL** ignorance:</u> Medical science has no completetheory for the domain.  
   ç†è®ºä¸Šçš„æ— çŸ¥: åŒ»å­¦åœ¨è¿™ä¸ªé¢†åŸŸæ²¡æœ‰å®Œæ•´çš„ç†è®ºã€‚

3. <u>**PRACTICAL** ignorance:</u> Even if we **know all the rules**,we might be **uncertain** about a particular patient because not **all the necessary tests** have **been or can be run**.  
   å®é™…æ— çŸ¥: å³ä½¿æˆ‘ä»¬çŸ¥é“æ‰€æœ‰çš„è§„åˆ™ï¼Œæˆ‘ä»¬ä¹Ÿå¯èƒ½å¯¹æŸä¸ªç‰¹å®šçš„ç—…äººä¸ç¡®å®šï¼Œå› ä¸ºå¹¶éæ‰€æœ‰å¿…è¦çš„æ£€æŸ¥éƒ½å·²ç»æˆ–å¯ä»¥è¿è¡Œã€‚

##### Reasoning under uncertainty ä¸ç¡®å®šæ€§æ¨ç†

- A rational agent is one that makes rational decisions â€” to maximize its performance measure  
  ç†æ€§ä»£ç†äººæ˜¯åšå‡ºç†æ€§å†³ç­–çš„äººãƒ¼ãƒ¼ä¸ºäº†æœ€å¤§é™åº¦åœ°æé«˜å…¶ç»©æ•ˆæŒ‡æ ‡

- A rational decision depends on  
  ç†æ€§çš„å†³å®šå–å†³äº
  
  - the **relative** importance of various goals  
    ä¸åŒç›®æ ‡çš„ç›¸å¯¹é‡è¦æ€§
  
  - the **likelihood** they will be achieved  
    å®ç°è¿™äº›ç›®æ ‡çš„å¯èƒ½æ€§  
  
  - the **degree** to which they will be achieved  
    è¾¾åˆ°çš„ç¨‹åº¦

##### Types of uncertainty ä¸ç¡®å®šæ€§ç±»å‹

- Uncertainty in **prior knowledge**  
  å…ˆéªŒçŸ¥è¯†çš„ä¸ç¡®å®šæ€§

- Uncertainty in **actions**  
  è¡ŒåŠ¨çš„ä¸ç¡®å®šæ€§

- Uncertainty in **perception**  
  æ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§

Uncertainty is a summary of all that is not explicitly considered in the agentâ€™s knowledge base.  
ä¸ç¡®å®šæ€§æ˜¯ä»£ç†çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ˜ç¡®è€ƒè™‘çš„æ‰€æœ‰ä¸ç¡®å®šæ€§çš„æ€»ç»“ã€‚

##### Handling uncertainty ä¸ç¡®å®šæ€§å¤„ç†

- **Default reasoning [Optimistic]**   
  **æ­£å‘æ¨ç†ã€”ä¹è§‚ã€•**  
  an agent assumes normality, until there is evidence of thecontrary.  
  é™¤éæœ‰ç›¸åçš„è¯æ®ï¼Œå¦åˆ™æ™ºèƒ½å°±ä¼šå‡è£…ä¸€åˆ‡æ­£å¸¸ã€‚

- **Worst-case reasoning [Pessimistic]**  
  **æœ€åæƒ…å†µæ¨ç†ã€”æ‚²è§‚è€…ã€•**  
  The agent assumes the **worst case**, and chooses the actions that maximizes a **utility function** in this case.  
  æ™ºèƒ½å‡è®¾æœ€åçš„æƒ…å†µï¼Œå¹¶åœ¨è¿™ç§æƒ…å†µä¸‹é€‰æ‹©ä½¿æ•ˆç”¨å‡½æ•°æœ€å¤§åŒ–çš„æ“ä½œã€‚  
  <u>Disadvantages: </u>  
  â‘ not **worth the effort** to develop or explore such a scenario;Â ä¸å€¼å¾—åŠªåŠ›å‘å±•æˆ–æ¢ç´¢è¿™ç§æƒ…å†µ  
  â‘ may **waste resources** preparing for highly unlikelycontingencies; å¯èƒ½ä¼šæµªè´¹èµ„æºï¼Œä¸ºæä¸å¯èƒ½å‘ç”Ÿçš„çªå‘äº‹ä»¶åšå‡†å¤‡  
  â‘ **restricted** way of handling an emergency. å¤„ç†ç´§æ€¥äº‹ä»¶çš„æœ‰é™æ–¹å¼  

- **Probabilistic reasoning [Realist]**  
  **æ¦‚ç‡æ€§æ¨ç† [ç°å®]**

#### Probability theory æ¦‚ç‡è®º

##### Probabilistic reasoning æ¦‚ç‡æ€§æ¨ç†

- The agent has **probabilistic beliefs**  
  
  - pieces of knowledge with associated probabilities (**strengths**)  
    å…·æœ‰ç›¸å…³æ¦‚ç‡(ä¼˜åŠ¿)çš„çŸ¥è¯†ç‰‡æ®µ
  
  - chooses its actions to maximize the expected value of some **utility function**  
    é€‰æ‹©è‡ªå·±çš„è¡Œä¸ºæ¥ä½¿æŸç§æ•ˆç”¨å‡½æ•°çš„æœŸæœ›å€¼æœ€å¤§åŒ–

- Rationale: The world is not divided between â€œnormalâ€ andâ€œabnormalâ€, nor is it adversarial. Possible situations have various **likelihoods/chance** (probabilities)  
  ç†ç”±: è¿™ä¸ªä¸–ç•Œæ²¡æœ‰â€œæ­£å¸¸â€å’Œâ€œä¸æ­£å¸¸â€ä¹‹åˆ†ï¼Œä¹Ÿæ²¡æœ‰å¯¹æŠ—ã€‚å¯èƒ½çš„æƒ…å†µæœ‰å„ç§å„æ ·çš„å¯èƒ½æ€§/æœºä¼š(æ¦‚ç‡)

##### Probabilistic reasoning and degrees of belief æ¦‚ç‡æ¨ç†å’Œä¿¡ä»»åº¦

- The agentâ€™s knowledge can only provide a **degree of belief** in the relevant sentences  
  ä»£ç†äººçš„çŸ¥è¯†åªèƒ½æä¾›å¯¹ç›¸å…³å¥å­çš„ä¸€å®šç¨‹åº¦çš„ä¿¡ä»»

- The agent cannot say whether a sentence is true, but only that is **true ğ’™%** of the times  
  ä»£ç†ä¸èƒ½è¯´å‡ºä¸€ä¸ªå¥å­æ˜¯å¦ä¸ºçœŸï¼Œä½†åªèƒ½è¯´å‡ºçœŸçš„ x% çš„æ¬¡æ•°

- The main tool for handling degrees of belief is **Probability Theory**   
  å¤„ç†ä¿¡ä»»åº¦çš„ä¸»è¦å·¥å…·æ˜¯æ¦‚ç‡è®º

- The use of probability **summarizes** the **uncertainty** that stems from humanâ€™s **laziness or ignorance** about the domain  
  æ¦‚ç‡çš„ä½¿ç”¨æ¦‚æ‹¬äº†ç”±äºäººç±»çš„æ‡’æƒ°æˆ–å¯¹é¢†åŸŸçš„æ— çŸ¥è€Œäº§ç”Ÿçš„ä¸ç¡®å®šæ€§

##### Probability theory & facts æ¦‚ç‡è®ºä¸äº‹å®

- Probability Theory makes the same ontological commitments as First-order Logic:  
  æ¦‚ç‡è®ºä½œå‡ºä¸ä¸€é˜¶é€»è¾‘ç›¸åŒçš„æœ¬ä½“è®ºæ‰¿è¯º:  
  Every sentence ğœ‘ is either true or false  
  æ¯ä¸ªå¥å­ Ï† ä¸æ˜¯çœŸå°±æ˜¯å‡

- The **degree** of belief that ğœ‘ is true is a number P between 0 and 1
  
  - P(ğœ‘) = 1 â†’ ğœ‘ is certainly true
  
  - P(ğœ‘) = 0 â†’ ğœ‘ is certainly not true
  
  - P(ğœ‘) = 0.65 â†’ ğœ‘ is true with a 65% chance

##### Probability facts æ¦‚ç‡äº‹å®

- Let ğ‘¨ be a propositional variable, a symbol denoting aproposition that is either true or false.  
  è®¾ a æ˜¯ä¸€ä¸ªå‘½é¢˜å˜é‡ï¼Œä¸€ä¸ªè¡¨ç¤ºå‘½é¢˜æ˜¯çœŸæˆ–å‡çš„ç¬¦å·ã€‚

- ğ‘·(ğ‘¨) denotes the probability that ğ‘¨ is true in the absence of any other information.  
  P (A)è¡¨ç¤ºåœ¨æ²¡æœ‰ä»»ä½•å…¶ä»–ä¿¡æ¯çš„æƒ…å†µä¸‹ A ä¸ºçœŸçš„æ¦‚ç‡ã€‚

- Similarly:
  
  - ğ‘ƒ(Â¬ğ´) = probability that ğ´ is false (~ or NOT)
    ğ‘ƒ(Â¬ğ´) = A ä¸ºå‡(æˆ–éå‡)çš„æ¦‚ç‡
  
  - ğ‘·(ğ‘¨ âˆ© ğ‘©) = probability that both ğ‘¨ and ğ‘© are true
    P (A âˆ© B) = A å’Œ B éƒ½ä¸ºçœŸçš„æ¦‚ç‡
  
  - ğ‘·(ğ‘¨ âˆª ğ‘©) = probability that either ğ‘¨ or ğ‘© (or both) are true
    P (A âˆª B) = A æˆ– B (æˆ–ä¸¤è€…)ä¸ºçœŸçš„æ¦‚ç‡

- Interpretation
  
  - If P is the probability of an event:ğŸ â‰¤ ğ‘· â‰¤ ğŸ
  
  - P = 0 means the event **cannot** occur
  
  - P = 1 means the event is **certain** to occur
  
  - The closer to 1, the **more likely** the event
    ![loading-ag-1218](./images/488251d3-def3-4e14-ade8-ba2e466591b5.png)
    
    - A priori å…ˆå‰ä¿¡æ¯
    
    - Relative frequency ç›¸å¯¹é¢‘ç‡
    
    - Subjective ä¸»è§‚

##### Recap: axioms of probability æ¦‚è¿°: æ¦‚ç‡å…¬ç†

- Complementary events äº’è¡¥äº‹ä»¶
  
  - ğ‘ƒ(ğ´)+ ğ‘ƒ(Â¬ğ´) = 1
    Hence, ğ‘ƒ(ğ´) = 1 âˆ’ ğ‘ƒ(Â¬ğ´)

- Combining events å¤åˆäº‹ä»¶
  
  - ğ´ or ğµ; ğ‘ƒ(ğ´âˆªğµ) = ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)âˆ’ğ‘ƒ(ğ´âˆ©ğµ) [Union (dark blue and yellow)]  
  
  - ğ´ and ğµ; ğ‘ƒ(ğ´âˆ©ğµ) = ğ‘ƒ(ğ´) Ã— ğ‘ƒ(ğµ) [Intersection]

##### Subjective/Bayesian Probability ä¸»è§‚/è´å¶æ–¯æ¦‚ç‡

1. Probabilities relate propositions to oneâ€™s own state of knowledge  
   æ¦‚ç‡å°†å‘½é¢˜ä¸è‡ªå·±çš„çŸ¥è¯†çŠ¶æ€è”ç³»èµ·æ¥

2. Probabilities of propositions change with new evidence  
   å‘½é¢˜çš„æ¦‚ç‡éšç€æ–°è¯æ®çš„å‡ºç°è€Œæ”¹å˜

3. This is analogous to logical entailment status KB |= ğœ‘(which changes with more knowledge), NOT truth!  
   è¿™ç±»ä¼¼äºé€»è¾‘è•´å«çŠ¶æ€ KB | = Ï† (éšç€çŸ¥è¯†çš„å¢åŠ è€Œæ”¹å˜) ï¼Œä¸æ˜¯çœŸç†ï¼
- Therefore, Probability is an important reasoning for decisionmaking analysis!  
   å› æ­¤ï¼Œæ¦‚ç‡è®ºæ˜¯å†³ç­–åˆ†æçš„ä¸€ä¸ªé‡è¦æ¨ç†æ–¹æ³•ï¼

##### Unconditional & conditional probability æ— æ¡ä»¶åŠæ— æ¡ä»¶æ¦‚ç‡

1. ğ‘·(ğ‘¨) is the **unconditional (or prior) probability** of fact ğ‘¨  
   P (A)æ˜¯äº‹å® A çš„æ— æ¡ä»¶(æˆ–ä¼˜å…ˆ)æ¦‚ç‡

2. An agent can use the unconditional probability of ğ‘¨ to reason about ğ‘¨ in theabsence of further information  
   åœ¨æ²¡æœ‰è¿›ä¸€æ­¥ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½å¯ä»¥ä½¿ç”¨ A çš„æ— æ¡ä»¶æ¦‚ç‡æ¥æ¨ç† A

3. If further evidence ğ‘© becomes available, the agent must use the **conditional (or posterior) probability**: $P(ğ‘¨ | ğ‘©)$      
   å¦‚æœè¿›ä¸€æ­¥çš„è¯æ® B å˜å¾—å¯ç”¨ï¼Œæ™ºèƒ½å¿…é¡»ä½¿ç”¨æ¡ä»¶(æˆ–åéªŒ)æ¦‚ç‡: $P (A | B)$

4. the probability of ğ´ **given that (all) the agent knows (is)** ğ‘©  
   ç»™å®šæ™ºèƒ½çŸ¥é“ B çš„å¯èƒ½æ€§

Note: ğ‘·(ğ‘¨) can be thought as the conditional probability of ğ‘¨ with respect to the empty evidence: $P(A) = P(A |B)$  
æ³¨: å¯¹äºç©ºè¯æ®ï¼ŒP (A)å¯ä»¥è¢«è®¤ä¸ºæ˜¯ A çš„æ¡ä»¶æ¦‚ç‡: $P (A) = P (A | B)$

##### Conditional probability æ¡ä»¶æ¦‚ç‡

- Definitionï¼š
  
  $$
  ğ‘·(ğ‘¨ âˆ© ğ‘©) = ğ‘·(ğ‘¨|ğ‘©) ğ‘·(ğ‘©)
  $$
  
  - **Read $P(A|B)$: Probability of ğ‘¨ given that we know ğ‘©**
    $P(A)$ is called the **prior probability** of ğ‘¨  
    $P(A)$è¢«ç§°ä¸ºAçš„å…ˆéªŒæ¦‚ç‡
    $P(A|B)$ is called the **posterior or conditional probability** of ğ‘¨ given ğ‘© 
    $P(A|B)$è¢«ç§°ä¸º a ç»™å®šçš„ b çš„åé¢æˆ–æ¡ä»¶æ¦‚ç‡

- Definition:
  
  $$
  ğ‘·(ğ‘© | ğ‘¨)= ğ‘·(ğ‘¨ âˆ© ğ‘©) / ğ‘·(ğ‘¨)
  $$
  
  - â€œğµ | ğ´â€ means â€œğµ given ğ´â€
    $P (B | A)$ is the probability that ğµ will happen if ğ´ has already happened.
    $P (B | A)$æ˜¯å½“ A å·²ç»å‘ç”Ÿæ—¶ B å°†å‘ç”Ÿçš„æ¦‚ç‡ã€‚

- Conditional probabilities are defined in terms of unconditional ones
  æ¡ä»¶æ¦‚ç‡æ˜¯ç”¨æ— æ¡ä»¶æ¦‚ç‡æ¥å®šä¹‰çš„

- Whenever $ğ‘·(ğ‘©) > ğŸ,$
  
  $$
  ğ‘·(ğ‘¨ | ğ‘©) = ğ‘·(ğ‘¨ âˆ© ğ‘©) / ğ‘·(ğ‘©)
  $$
  
  $$
  ğ‘·(ğ‘¨ âˆ© ğ‘©)= ğ‘·(ğ‘¨ | ğ‘©) ğ‘·(ğ‘©) = ğ‘·(ğ‘© | ğ‘¨) ğ‘·(ğ‘¨)
  $$

- ğ´ and ğµ are independent,then A å’Œ B æ˜¯ç‹¬ç«‹çš„ï¼Œåˆ™
  
  $$
  ğ‘ƒ(ğ´ | ğµ) = ğ‘ƒ(ğ´)
  $$
  
  $$
  ğ‘ƒ(ğµ | ğ´) = ğ‘ƒ(ğµ)
  $$
  
  $$
  ğ‘ƒ(ğ´ âˆ© ğµ) = ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)
  $$

- Another generalisationï¼šå¦ä¸€ä¸ªæ¦‚æ‹¬æ˜¯:
  
  $$
  P(Aâˆ© B âˆ© C) = P(A|B,C) P(B|C) P(C)
  $$

##### Frequency Interpretation é¢‘ç‡è§£é‡Š

- Draw a ball from a bag containing n balls of the same size, red and s yellow.
  ä»è£…æœ‰çº¢è‰²å’Œé»„è‰²ç­‰å¤§å°çš„ n ä¸ªçƒçš„è¢‹å­ä¸­æŠ½å‡ºä¸€ä¸ªçƒã€‚

- The probability that the proposition A = â€œthe ball is redâ€ is true corresponds to the relative frequency with which we expect to draw a red ball
  å‘½é¢˜ A = â€œçƒæ˜¯çº¢è‰²çš„â€æ˜¯çœŸçš„çš„æ¦‚ç‡å¯¹åº”äºæˆ‘ä»¬æœŸæœ›ç”»ä¸€ä¸ªçº¢è‰²çƒçš„ç›¸å¯¹é¢‘ç‡

$$
P(ğ´) = \frac{r}{n}
$$

#### Random Variables éšæœºå˜é‡

##### Random Variables Definition

---

- A random variable is a variable rangingover a certain domain of ğ‘½ğ’‚ğ’ğ’–ğ’†ğ’”
  ä¸€ä¸ªéšæœºå˜é‡æ˜¯ä¸€ä¸ªå˜é‡èŒƒå›´è¶…è¿‡ä¸€å®šçš„å€¼åŸŸ

- It is discrete if it ranges over a discrete (that is,countable) domain
  å¦‚æœå®ƒçš„èŒƒå›´è¶…è¿‡ä¸€ä¸ªç¦»æ•£(å³å¯æ•°)åŸŸï¼Œåˆ™å®ƒæ˜¯ç¦»æ•£çš„

- continuous if it ranges over the real numbers
  åœ¨å®æ•°èŒƒå›´å†…æ˜¯è¿ç»­çš„

- We will only consider discrete randomvariables with finite domains
  æˆ‘ä»¬åªè€ƒè™‘æœ‰é™åŸŸçš„ç¦»æ•£éšæœºå˜é‡

> Note: Propositional variables can be seen as randomvariables over the Boolean domain
> Â Â Â Â Â Â Â Â Â Â Â å‘½é¢˜å˜é‡å¯ä»¥çœ‹ä½œæ˜¯å¸ƒå°”åŸŸä¸Šçš„éšæœºå˜é‡

---

- A proposition that takes the value True with probability ğ‘ and False with probability $1 âˆ’ p$ is a random variable with distribution $(p, 1 âˆ’ p)$

- If a bag contains balls having 3 possible colors â€“ red, yellow, and blue â€“ the color of a ball picked at random from the bag is a random variable with 3 possible values
  ä¸€ä¸ªå–æ¦‚ç‡ä¸º p çš„ True å’Œæ¦‚ç‡ä¸º1-p çš„ False å€¼çš„å‘½é¢˜æ˜¯ä¸€ä¸ªå…·æœ‰åˆ†å¸ƒçš„éšæœºå˜é‡

- The (probability) distribution of a random variable $X$ with n values $x_1,x_2,...,x_n$ is:
  éšæœºå˜é‡ $X $ä¸ n å€¼ $x _ 1,x _ 2,... ,x _ n $çš„(æ¦‚ç‡)åˆ†å¸ƒæ˜¯:
  
  $$
  (p_1,p_2,...,p_n)
  $$
  
  $$
  P(X = x_i)=p_i
  $$
  
  $$
  \sum_{i=i,...,n}p_n=1
  $$

---

| Variable | Domain                           |
| -------- | -------------------------------- |
| Age      | {1, 2, . . . , 120}              |
| Weather  | {sunnt, dry, cloudy, rain, snow} |
| Size     | {amsll, medium, large}           |
| Blonde   | {true, false}                    |

- The probability that a random variable ğ‘‹ has value ğ‘£ğ‘ğ‘™ is written as
  éšæœºå˜é‡ X å…·æœ‰å€¼ val çš„æ¦‚ç‡å†™ä¸º
  
  $$
  P(X=val)
  $$

> Note 1: ğ‘ƒ(ğ´ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’) is written shortly as ğ‘ƒ(ğ‘) while ğ‘ƒ(ğ´ = ğ‘“ğ‘ğ‘™ğ‘ ğ‘’) iswritten as ğ‘ƒ(Â¬ğ‘)
> Â Â Â Â Â Â Â Â Â Â Â Â Â Â P (A = true)ç®€å†™ä¸º P (a) ï¼Œè€Œ P (A = false)ç®€å†™ä¸º P (a)
> Note 2: Traditionally, in ProbabilityTheory variables are capitalized andconstant values are NOT.
> Â Â Â Â Â Â Â Â Â Â Â Â Â Â ä¼ ç»Ÿä¸Šï¼Œåœ¨æ¦‚ç‡è®ºä¸­ï¼Œå˜é‡æ˜¯å¤§å†™çš„ï¼Œè€Œå¸¸æ•°å€¼ä¸æ˜¯ã€‚

---

##### Probability distribution æ¦‚ç‡åˆ†å¸ƒ

- If ğ‘¿ is a random variable, we use the bold case ğ‘·(ğ‘¿) to denote a vector ofvalues for the probabilities of each individual element that ğ‘¿ can take.
  å¦‚æœ X æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç²—ä½“å¤§å°å†™ P (X)æ¥è¡¨ç¤º X å¯ä»¥æ¥å—çš„æ¯ä¸ªå•ç‹¬å…ƒç´ çš„æ¦‚ç‡çš„å€¼å‘é‡ã€‚

- Example
  
  $$
  ğ‘ƒ(ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ = ğ‘ ğ‘¢ğ‘›ğ‘›ğ‘¦) = 0.6\\
ğ‘ƒ(ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ = ğ‘Ÿğ‘ğ‘–ğ‘›) = 0.2\\
ğ‘ƒ(ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ = ğ‘ğ‘™ğ‘œğ‘¢ğ‘‘ğ‘¦) = 0.18\\
ğ‘ƒ(ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ = ğ‘ ğ‘›ğ‘œğ‘¤) = 0.02\\
Then \ ğ‘ƒ(ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ) = \{ 0.6,0.2,0.18,0.02\}
  $$

- ğ‘·(ğ‘¾ğ’†ğ’‚ğ’•ğ’‰ğ’†ğ’“) is called a **probability distribution** for the random variable
  ğ‘·(ğ‘¾ğ’†ğ’‚ğ’•ğ’‰ğ’†ğ’“) è¢«ç§°ä¸ºéšæœºå˜é‡çš„**æ¦‚ç‡åˆ†å¸ƒ**

##### Expected value æœŸæœ›å€¼

- Random variable $X$ with $n$ values $x_1,x_2,...,x_n$ and distribution $(p _1,p_2,...,p_n)$      
  éšæœºå˜é‡ $X $with $n $å€¼ $x _ 1,x _ 2,... ,x _ n $å’Œåˆ†å¸ƒ $(p _ 1,p _ 2,... ,p _ n) $     

- Function $U$ of  $X$       
  å‡½æ•° $U $å¯¹äº $X $   

- The **expected value** of $ğ‘ˆ$ after doing $ğ´$ is   
  åœ¨å®Œæˆ $A $ä¹‹åï¼Œ$U $çš„**é¢„æœŸå€¼**æ˜¯
  
  $$
  E[U]=\sum_{i=1,...,n}p_iU(x_i)
  $$

##### Joint Probability Distribution (JPD) è”åˆæ¦‚ç‡åˆ†å¸ƒ

- If $X_1,...,X_n$ are random variables,
  
  $$
  P(X_1,...,X_n)
  $$
  
  denotes their **joint probability distribution (JPD)**, an ğ‘›-dimensional matrix specifying the probability of every possible combination of values for $X_1,...,X_n$   
  
  > å³å¤šä¸ªéšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒç»„åˆæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ

- All relevant probabilities about a vector $\{X_1,...,X_n\}$ of random variables can becomputed from $P(X_1,...,X_n)$   
  å…³äºéšæœºå˜é‡å‘é‡ ${ X _ 1ï¼Œ... ï¼ŒX _ n } $çš„æ‰€æœ‰ç›¸å…³æ¦‚ç‡éƒ½å¯ä»¥ä» $P (X _ 1ï¼Œ... ï¼ŒX _ n) $ä¸­è®¡ç®—å‡ºæ¥
  
  > å•ä¸ªéšæœºå˜é‡ä¸­å„ä¸ªäº‹ä»¶æ¦‚ç‡å’Œä»ä¸º1ï¼ŒçŸ©é˜µä¸­çš„æ¦‚ç‡è¦åŒæ—¶æ»¡è¶³ä¸¤ä¸ªè½´

- A JPD $P(X_1,...,X_n)$ **provides complete information** about the probabilities of its random variables.   
  è”åˆæ¦‚ç‡åˆ†å¸ƒ $P (X _ 1,... ,X _ n) $æä¾›å…³äºå…¶éšæœºå˜é‡æ¦‚ç‡çš„å®Œæ•´ä¿¡æ¯ã€‚

---

- EXP
  
  |        | Sky=sunny | Sky=cloudy | Sky=rain | Sky=snow | P(Wind) |
  | ------ | --------- | ---------- | -------- | -------- | ------- |
  | W      | 0.3       | 0.15       | 0.17     | 0.01     | 0.63    |
  | -W     | 0.3       | 0.05       | 0.01     | 0.01     | 0.37    |
  | P(Sky) | 0.6       | 0.20       | 0.18     | 0.02     | 1.00    |
  
  |                 | $Toothache$ | $\neg \ Toothache$ |
  | --------------- | ----------- | ------------------ |
  | $Cavity$        | 0.04        | 0.06               |
  | $\neg \ Cavity$ | 0.01        | 0.89               |

---

- Limitation of Joint Probability Distribution è”åˆæ¦‚ç‡åˆ†å¸ƒçš„å±€é™
  
  - However, JPDâ€™s are often **hard to create** (incomplete knowledge of the domain).   
    ç„¶è€Œï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒé€šå¸¸å¾ˆéš¾åˆ›å»º(ä¸å®Œæ•´çš„é¢†åŸŸçŸ¥è¯†)ã€‚
  
  - Even when available,  JPD **tables are very expensive**, or **impossible**, to store because of **their size**.    
    å³ä½¿åœ¨å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œç”±äºè”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨çš„å¤§å°ï¼Œå­˜å‚¨å®ƒä»¬ä¹Ÿæ˜¯éå¸¸æ˜‚è´µçš„ï¼Œç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚  
  
  - A JPD table for $n$ random variables, each ranging over $k $ distinct values, has $k^n$ entries!     
    ç”¨äº $n $éšæœºå˜é‡çš„ è”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨(æ¯ä¸ªå˜é‡çš„èŒƒå›´éƒ½è¶…è¿‡ $k $ä¸åŒçš„å€¼)å…·æœ‰ $k ^ n $æ¡ç›®ï¼   
  
  - A better approach is to come up with conditional probabilities as needed and compute the others from them.     
    ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•æ˜¯æ ¹æ®éœ€è¦æå‡ºæ¡ä»¶æ¦‚ç‡ï¼Œç„¶åä»ä¸­è®¡ç®—å…¶ä»–æ¦‚ç‡ã€‚

#### Bayes rule and conditional independence è´å¶æ–¯è§„åˆ™å’Œæ¡ä»¶ç‹¬ç«‹

##### Bayes Rule è´å¶æ–¯è§„åˆ™

$$
ğ‘ƒ(ğ´ âˆ© ğµ) = ğ‘ƒ(ğ´ | ğµ)ğ‘ƒ(ğµ) = ğ‘ƒ(ğµ | ğ´)ğ‘ƒ(ğ´)\\ 
\ \\
ğ‘ƒ(ğµ | ğ´) = \frac{ğ‘ƒ(ğ´ | ğµ) ğ‘ƒ(ğµ)}{ğ‘ƒ(ğ´)}

$$

- é€šå¸¸æ˜¯çŸ¥é“åœ¨Bå‘ç”Ÿçš„æƒ…å†µä¸‹Aå‘ç”Ÿçš„æ¦‚ç‡æœ‰å¤šå°‘ï¼Œåè¿‡æ¥éš¾æ±‚  

- ä¸¤ä¸ªäº‹ä»¶å‘ç”Ÿçš„å„è‡ªæ¦‚ç‡ä¹Ÿèƒ½è¢«å¾ˆå¥½çš„ç»Ÿè®¡  

##### Bayes rule â€“ another version è´å¶æ–¯è§„åˆ™-å¦ä¸€ä¸ªç‰ˆæœ¬

- $$
  P(A|B)=\frac{P(B|A)P(A)}{P(B)}=\frac{P(B|A)P(A)}{P(A)P(B|A)+P(\neg A)P(B|\neg A)}
  $$

- $$
  P(B)=P(A)P(B|A)+P(\neg A)P(B|\neg A)
  $$

- ä¸éœ€è¦çŸ¥é“$P(B)$

---

- EXP  
  12% of the men and 4% of the women are taller than6 feet. Furthermore, 20% of the students in the classare women.  
  Suppose that a randomly selected student is taller than 6 feet.Find the probability ğ‘ that the student is a woman.
  
  $$
  ğ‘ƒ(ğ‘¤ğ‘œğ‘šğ‘ğ‘›|ğ‘¡ğ‘ğ‘™ğ‘™) = \frac{ğ‘ƒ(ğ‘¡ğ‘ğ‘™ğ‘™|ğ‘¤ğ‘œğ‘šğ‘ğ‘›)ğ‘ƒ(ğ‘¤ğ‘œğ‘šğ‘n)}{ğ‘ƒ(ğ‘¡ğ‘ğ‘™ğ‘™)}=\frac{ğ‘ƒ(ğ‘¡ğ‘ğ‘™ğ‘™|ğ‘¤ğ‘œğ‘šğ‘ğ‘›)ğ‘ƒ(ğ‘¤ğ‘œğ‘šğ‘n)}{ğ‘ƒ(ğ‘¡ğ‘ğ‘™ğ‘™|ğ‘¤ğ‘œğ‘šğ‘ğ‘›)ğ‘ƒ(ğ‘šğ‘n)+ğ‘ƒ(ğ‘¡ğ‘ğ‘™ğ‘™|ğ‘¤ğ‘œğ‘šğ‘ğ‘›)ğ‘ƒ(ğ‘šğ‘n)}\\ \ \\ =\frac{0.04 \times 0.2}{0.104} = 0.0769
  $$

---

##### Bayes theorem application è´å¶æ–¯å®šç†çš„åº”ç”¨

- Bayes Theorem has found numerous applications in manyfields, including Computer Science  
  è´å¶æ–¯å®šç†åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬è®¡ç®—æœºç§‘å­¦
  
  - Bayesian Networks è´å¶æ–¯ç½‘ç»œ
  
  - Bayesian Classifiers è´å¶æ–¯åˆ†ç±»æœº
  
  - spam filtering, web page classification (e.g. Yahoostyle hierarchies), object classification, etc.  
    åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€ç½‘é¡µåˆ†ç±»(å¦‚ Yahoostyle å±‚æ¬¡ç»“æ„)ã€å¯¹è±¡åˆ†ç±»ç­‰ã€‚

- **Bayesian Machine Learning**: Bayesian Inference / Bayesian Decision Theory  
  è´å¶æ–¯æœºå™¨å­¦ä¹ : è´å¶æ–¯æ¨æ–­/è´å¶æ–¯å†³ç­–ç†è®º

##### Conditional independence æ¡ä»¶ç‹¬ç«‹æ€§

- Two random variables ğ´ and ğµ are (absolutely) independent if  
  ä¸¤ä¸ªéšæœºå˜é‡ A å’Œ B æ˜¯(ç»å¯¹)ç‹¬ç«‹çš„ï¼Œå¦‚æœ
  
  $$
  P(A,B)=P(A)P(B)
  $$

- Using product rule for ğ´ & ğµ independent, we can show  
  ä½¿ç”¨ç‹¬ç«‹äº A & B çš„ä¹˜ç§¯è§„åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥çŸ¥é“
  
  $$
  ğ‘ƒ(ğ´,ğµ) = ğ‘ƒ(ğ´ | ğµ)ğ‘ƒ(ğµ) = ğ‘ƒ(ğ´)ğ‘ƒ(ğµ) \\ \ \\ Therefore \ ğ‘ƒ(ğ´ | ğµ) = ğ‘ƒ(ğ´)

  $$

- If ğ‘› Boolean variables are independent, the full JPD is:  
  å¦‚æœ n ä¸ªå¸ƒå°”å˜é‡æ˜¯ç‹¬ç«‹çš„ï¼Œåˆ™å®Œæ•´çš„ JPD æ˜¯:    
  
  $$
  P(X_1,...,X_2) = {\textstyle \prod_{i}^{}} P(X_i)
  $$
  
  Full joint is generally specified by $2^n-1$ numbers, but when independent only $n$ numbers are needed.  
  å®Œå…¨è¿æ¥é€šå¸¸ç”± $2 ^ n-1 $æ•°å­—æŒ‡å®šï¼Œä½†æ˜¯å½“ç‹¬ç«‹æ—¶åªéœ€è¦ $n $æ•°å­—ã€‚

- Absolute independence is a very strong requirement, seldom met
  ç»å¯¹ç‹¬ç«‹æ˜¯ä¸€ä¸ªéå¸¸å¼ºçƒˆçš„è¦æ±‚ï¼Œå¾ˆå°‘å¾—åˆ°æ»¡è¶³

- Conditional Independence - expressed as:
  
  $$
  P(A|B,C)=P(A|C)
  $$

##### The chain rule for JPD JPDçš„é“¾å¼æ³•åˆ™

$$
P(X_1,...,X_n)\\=P(X_1,...,X_{n-1})P(N_n|X_1,...,X_{n-1})\\=P(X_1,...,X_{n-2})P(X_{n-1}|X_1,...,X_{n-2})P(Xn|X_1,...,X_{n-1})\\ . \\ . \\ . \\= {\textstyle \prod_{i}^{n}} P(X_i|X_1,...,X_{i-1})
$$

#### Bayes (Belief) Network è´å¶æ–¯(ä¿¡å¿µ)ç½‘ç»œ

- Bayesian Networks are a successful example of probabilistic systems that exploit conditional independence to reason efficiently under uncertainty.   
  è´å¶æ–¯ç½‘ç»œæ˜¯ä¸€ä¸ªæˆåŠŸçš„ä¾‹å­ï¼Œå®ƒåˆ©ç”¨æ¡ä»¶ç‹¬ç«‹ç³»ç»Ÿåœ¨ä¸ç¡®å®šæƒ…å†µä¸‹æœ‰æ•ˆåœ°è¿›è¡Œæ¨ç†ã€‚

- A simple, graphical notation for conditional independence assertions and hence for compact specification of full joint distributions.  
  ä¸€ä¸ªç®€å•çš„ï¼Œå›¾å½¢åŒ–çš„ç¬¦å·ç”¨äºæ¡ä»¶ç‹¬ç«‹æ–­è¨€ï¼Œå› æ­¤ä¹Ÿç”¨äºå®Œæ•´è”åˆåˆ†å¸ƒçš„ç´§å‡‘è§„èŒƒã€‚

- Syntax:  å¥æ³•:
  
  - a set of nodes, one per random variable   
    ä¸€ç»„èŠ‚ç‚¹ï¼Œæ¯ä¸ªéšæœºå˜é‡ä¸€ä¸ª
  
  - links mean parent â€œdirectly influencesâ€ child   
    é“¾æ¥æ„å‘³ç€çˆ¶æ¯â€œç›´æ¥å½±å“â€å­©å­
  
  - a directed acyclic graph   
    æœ‰å‘æ— ç¯å›¾
  
  - a conditional distribution (a table) for each node given its parents  $P(X_i|parents(X_i))$  
    ç»™å®šå…¶çˆ¶èŠ‚ç‚¹ $P (X _ i | çˆ¶èŠ‚ç‚¹(X _ i)) $çš„æ¯ä¸ªèŠ‚ç‚¹çš„æ¡ä»¶åˆ†å¸ƒ(è¡¨) 

- In the simplest case, conditional distribution represented as a conditional probability table (CPT)  
  åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œæ¡ä»¶åˆ†å¸ƒè¡¨ç¤ºä¸ºä¸€ä¸ªæ¡ä»¶æ¦‚ç‡è¡¨(CPT)

##### A two node network & conditional probability åŒèŠ‚ç‚¹ç½‘ç»œåŠæ¡ä»¶æ¦‚ç‡

- Nodeğ´is independent of Node ğµ, so it is described by an unconditional probability $P(A)$

- $P(\neg A)$ is given by $1-P(A)$ 

- Node ğµ is conditionally dependent on ğ´. It is described by four numbers, $P(B|\neg A)$,$P(B|\neg A)$, $P(\neg B|A)$and $P(\neg B|\neg A)$. 

- This can be expressed as 2 by 2 **conditional probability table (CPT)**. 

- But$P(\neg B|A)= 1=P(B|A)$ and $P(\neg B|\neg A) = 1-P(B|\neg A)$. 

- Therefore, only **two** independent numbers in CPT.

### Part 3: Machine learning basics ç¬¬3éƒ¨åˆ†: æœºå™¨å­¦ä¹ åŸºç¡€

- Definition of Learning 

- Three Types of Machine Learning 

- Supervised Learning: Decision Trees 

- Linear and non-linear classification methods

#### Machine Learning Basics æœºå™¨å­¦ä¹ åŸºç¡€

- Artificial Intelligence is a scientific field concerned with the development of algorithms that allow computers to learn without being explicitly programmed   
  äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨ç ”ç©¶ç®—æ³•çš„ç§‘å­¦ï¼Œè¿™ç§ç®—æ³•ä½¿å¾—è®¡ç®—æœºä¸éœ€è¦æ˜ç¡®çš„ç¼–ç¨‹å°±èƒ½å­¦ä¹ 

- Machine Learning is a branch of Artificial Intelligence, which focuses on methods that learn from data and make predictions on unseen data  
  æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸»è¦ç ”ç©¶ä»æ•°æ®ä¸­å­¦ä¹ å’Œå¯¹æœªçŸ¥æ•°æ®è¿›è¡Œé¢„æµ‹çš„æ–¹æ³•

![0782867a-57b1-4db6-acdd-10bba74438c6](./images/0782867a-57b1-4db6-acdd-10bba74438c6.png)

##### Learning å­¦ä¹ 

- Definition: â€œlearning is a goal-directed process of a system that **improves the knowledge** or the **knowledge representation** of the system by exploring **experience** and prior **knowledge**â€   
  å®šä¹‰: â€œå­¦ä¹ æ˜¯ä¸€ä¸ªä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„ç³»ç»Ÿè¿‡ç¨‹ï¼Œå®ƒé€šè¿‡æ¢ç´¢ç»éªŒå’Œå…ˆéªŒçŸ¥è¯†æ¥æé«˜ç³»ç»Ÿçš„çŸ¥è¯†æˆ–çŸ¥è¯†è¡¨ç¤ºã€‚â€

- Acquisition of **new declarative knowledge**  
  è·å–æ–°çš„é™ˆè¿°æ€§çŸ¥è¯†

- Development of motor and cognitive skills through **instruction** and **practice**  
  é€šè¿‡æŒ‡å¯¼å’Œç»ƒä¹ å‘å±•è¿åŠ¨å’Œè®¤çŸ¥æŠ€èƒ½

- Organization of new knowledge into general effective representation   
  å°†æ–°çŸ¥è¯†ç»„ç»‡æˆä¸€èˆ¬æœ‰æ•ˆçš„è¡¨ç¤ºå½¢å¼

- Discovery of new facts and theories through **observation** and **experimentation** 
  é€šè¿‡è§‚å¯Ÿå’Œå®éªŒå‘ç°æ–°çš„äº‹å®å’Œç†è®º

##### Forms of Learning å­¦ä¹ å½¢å¼

Any component of an agent can be improved by learning from data. The improvements, and the techniques used to make them, depend on four major factors:  
ä»£ç†çš„ä»»ä½•ç»„ä»¶éƒ½å¯ä»¥é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ æ¥æ”¹è¿›ã€‚è¿™äº›æ”¹è¿›ä»¥åŠåˆ¶é€ å®ƒä»¬çš„æŠ€æœ¯ï¼Œå–å†³äºå››ä¸ªä¸»è¦å› ç´ :

- **component** 

- **prior knowledge**

- **representation**

- **feedback**

##### Components ç»„æˆ

##### Representation and prior knowledge è¡¨å¾å’Œå…ˆéªŒçŸ¥è¯†

- We have seen several examples of representations for agent components: Propositional and first-order logical sentences for the components in a logical agent;   
  æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä»£ç†ç»„ä»¶è¡¨ç¤ºçš„å‡ ä¸ªä¾‹å­: é€»è¾‘ä»£ç†ä¸­ç»„ä»¶çš„å‘½é¢˜å’Œä¸€é˜¶é€»è¾‘å¥;

- Bayesian networks for the inferential components of a decision-theoretic agent, and so on.   
  è´å¶æ–¯ç½‘ç»œç”¨äºæ¨æ–­åˆ†é‡çš„å†³ç­–ç†è®ºä»£ç†ï¼Œç­‰ç­‰ã€‚

- Markov Chain and Hidden Markov Models   
  é©¬å°”å¯å¤«é“¾ä¸éšé©¬å°”å¯å¤«æ¨¡å‹

- We say that learning a (possibly incorrect) general function or rule from specific inputâ€“output pairs is called inductive learning (more about this later).   
  æˆ‘ä»¬è¯´ä»ç‰¹å®šçš„è¾“å…¥è¾“å‡ºå¯¹ä¸­å­¦ä¹ ä¸€ä¸ª(å¯èƒ½ä¸æ­£ç¡®çš„)ä¸€èˆ¬å‡½æ•°æˆ–è§„åˆ™å«åšå½’çº³å­¦ä¹ (ç¨åè¯¦è¿°)ã€‚

![a4cf86d9-ed7b-4002-bd2d-e42a4d466f5d](./images/a4cf86d9-ed7b-4002-bd2d-e42a4d466f5d.png)

#### Three Types of Machine Learning æœºå™¨å­¦ä¹ çš„ä¸‰ç§ç±»å‹

##### Feedback to learn from éœ€è¦å­¦ä¹ çš„åé¦ˆ

- Three types of feedback that determine the three main types of learning: 
  
  - Unsupervised learning: the agent learns patterns in the input even though no explicit feedback is supplied.   
    éç›‘ç£å¼å­¦ä¹ : å³ä½¿æ²¡æœ‰æä¾›æ˜ç¡®çš„åé¦ˆï¼Œä»£ç†ä¹Ÿä¼šåœ¨è¾“å…¥ä¸­å­¦ä¹ æ¨¡å¼ã€‚ï¼ˆèšç±» clusteringï¼‰
    
    > Unsupervised learning categories and techniques 
    > 
    > - Clustering 
    >   
    >   - ok-means clustering 
    >   
    >   - Mean-shift clustering Spectral clustering 
    > 
    > - Density estimation 
    >   
    >   - Gaussian mixture model (GMM) 
    >   
    >   - Graphical models 
    > 
    > - Dimensionality reduction 
    >   
    >   - Principal component analysis (PCA) 
    >   
    >   - Factor analysis
  
  - Supervised learning: the agent **observes** some example inputoutput pairs and learns a function that maps from input to output.  
    ç›‘ç£å¼å­¦ä¹ : ä»£ç†è§‚å¯Ÿä¸€äº›ç¤ºä¾‹è¾“å…¥è¾“å‡ºå¯¹ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªä»è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å‡½æ•°ã€‚
    
    > Supervised learning categories and techniques 
    > 
    > - Numerical classifier functions 
    >   
    >   - Linear classifier, perceptron, logistic regression, support vector machines (SVM), neural networks 
    > 
    > - Parametric (probabilistic) functions 
    >   
    >   - NaÃ¯ve Bayes, Gaussian discriminant analysis (GDA), hidden Markov models (HMM), probabilistic graphical models 
    > 
    > - Non-parametric (instance-based) functions 
    >   
    >   - k-nearest neighbors, kernel regression, kernel density estimation, local regression 
    > 
    > - Symbolic functions 
    >   
    >   - Decision trees, classification and regression trees (CART)
  
  - Reinforcement learning: the agent learns from a series of reinforcementsâ€”rewards or punishments.  
    å¼ºåŒ–å­¦ä¹ : ä»£ç†äººä»ä¸€ç³»åˆ—çš„å¢æ´ä¸­å­¦ä¹ -å¥–åŠ±æˆ–æƒ©ç½šã€‚

- Summary of Machine Learning Types æœºå™¨å­¦ä¹ ç±»å‹ç»¼è¿°
  
  - Supervised: learning with labeled data ç›‘ç£: ä½¿ç”¨æ ‡è®°æ•°æ®å­¦ä¹ 
  
  - Unsupervised: discover patterns in unlabeled data  æ— ç›‘ç£: åœ¨æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°æ¨¡å¼
  
  - Reinforcement learning: learn to act based on feedback/reward  å¼ºåŒ–å­¦ä¹ : å­¦ä¼šæ ¹æ®åé¦ˆ/å›æŠ¥è¡Œäº‹

![ba57d48e-4088-47fd-9496-9d95ab16b8fd](./images/ba57d48e-4088-47fd-9496-9d95ab16b8fd.png)

#### Supervised Learning: Decision Trees ç›‘ç£å¼å­¦ä¹ : å†³ç­–æ ‘

- A simple yet effective form of learning from examples   
  ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä»å®ä¾‹ä¸­å­¦ä¹ çš„å½¢å¼

- is a function that: 
  
  - maps objects with a certain set of discrete attributes to discrete values based on the values of those attributes  
    å°†å…·æœ‰æŸç»„ç¦»æ•£å±æ€§çš„å¯¹è±¡æ˜ å°„ä¸ºåŸºäºè¿™äº›å±æ€§çš„å€¼çš„ç¦»æ•£å€¼

- It is representable as a tree in which   
  å®ƒå¯ä»¥è¡¨ç¤ºä¸ºä¸€æ£µæ ‘ï¼Œå…¶ä¸­
  
  - every non-leaf node corresponds to a test on the value of one of the attributes   
    æ¯ä¸ªéå¶èŠ‚ç‚¹å¯¹åº”äºä¸€ä¸ªå±æ€§å€¼çš„æµ‹è¯•
  
  - every leaf node specifies the value to be returned if that leaf is reached   
    æ¯ä¸ªå¶å­èŠ‚ç‚¹æŒ‡å®šåˆ°è¾¾è¯¥å¶å­æ—¶è¦è¿”å›çš„å€¼

- A decision tree based on attributes $A_1, ...,A_n$ acts as classifier for objects that have those attributes  
  åŸºäºå±æ€§ $A _ 1ï¼Œ... ï¼ŒA _ n $çš„å†³ç­–æ ‘å……å½“å…·æœ‰è¿™äº›å±æ€§çš„å¯¹è±¡çš„åˆ†ç±»å™¨

##### Decision Trees å†³ç­–æ ‘

- Decision trees make predictions by recursively splitting on different attributes according to a tree structure.  
  å†³ç­–æ ‘é€šè¿‡æ ¹æ®æ ‘ç»“æ„å¯¹ä¸åŒçš„å±æ€§è¿›è¡Œé€’å½’åˆ†è£‚æ¥è¿›è¡Œé¢„æµ‹ã€‚

- A decision tree with Boolean output defines a logical predicate
  å…·æœ‰å¸ƒå°”è¾“å‡ºçš„å†³ç­–æ ‘å®šä¹‰äº†é€»è¾‘è°“è¯

---

![2339e9bd-2a1e-49da-9cf2-ce907d9ad74d](./images/2339e9bd-2a1e-49da-9cf2-ce907d9ad74d.png)

![4efdbb52-2a2f-4c8b-afda-a0e22e0fa1b9](./images/4efdbb52-2a2f-4c8b-afda-a0e22e0fa1b9.png)

![089c19c9-f41d-4066-9a91-dddbd50d6b3d](./images/089c19c9-f41d-4066-9a91-dddbd50d6b3d.png)

---

##### Some terminology  ä¸€äº›æœ¯è¯­

- The **goal predicate** is the predicate to be implemented by a decision tree.   
  ç›®æ ‡è°“è¯æ˜¯ç”±å†³ç­–æ ‘å®ç°çš„è°“è¯ã€‚

- The **training set** is the set of examples used to build the tree.   
  è®­ç»ƒé›†æ˜¯ç”¨äºæ„å»ºæ ‘çš„ç¤ºä¾‹é›†ã€‚

- A member of the training set is a **positive example** if it satisfies the goal predicate, it is a **negative example** if it does not.  
  å¦‚æœè®­ç»ƒé›†çš„æˆå‘˜æ»¡è¶³ç›®æ ‡è°“è¯ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ªæ­£é¢ä¾‹å­; å¦‚æœä¸æ»¡è¶³ç›®æ ‡è°“è¯ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ªè´Ÿé¢ä¾‹å­ã€‚

##### A Good Decision Tree  ä¸€ä¸ªå¥½çš„å†³ç­–æ ‘

- extrapolates a **common pattern** from the examples   
  ä»ä¾‹å­ä¸­æ¨æ–­å‡ºä¸€ä¸ªå…¬å…±æ¨¡å¼

- **correctly classifies all possible examples**, not just those in the training set  
  æ­£ç¡®åˆ†ç±»æ‰€æœ‰å¯èƒ½çš„ä¾‹å­ï¼Œè€Œä¸ä»…ä»…æ˜¯é‚£äº›åœ¨è®­ç»ƒé›†

---

##### Choosing an attribute  é€‰æ‹©å±æ€§

patrons is a better choice: it gives more information about the classification  
patronsæ˜¯ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©: å®ƒæä¾›äº†æ›´å¤šå…³äºåˆ†ç±»çš„ä¿¡æ¯ ï¼ˆè®­ç»ƒæ•°æ®å³patronsï¼‰

Prefer more informative attributes leads to smaller trees
æ›´å–œæ¬¢ä¿¡æ¯æ›´ä¸°å¯Œçš„å±æ€§ä¼šå¯¼è‡´æ›´å°çš„æ ‘

Main Idea: start building the tree by testing at its root an attribute that better splits the training set into homogeneous classes  
ä¸»è¦æ€æƒ³: é€šè¿‡æµ‹è¯•ä¸€ä¸ªèƒ½å¤Ÿæ›´å¥½åœ°å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºåŒæ„ç±»çš„å±æ€§ï¼Œå¼€å§‹æ„å»ºæ ‘

![21e821c6-fa46-4442-90ec-28e9be0c217d](./images/21e821c6-fa46-4442-90ec-28e9be0c217d.png)

![5c6cbeff-cbf9-4b94-97f7-cbbecdf79179](./images/5c6cbeff-cbf9-4b94-97f7-cbbecdf79179.png)

##### Choosing the best attribute é€‰æ‹©æœ€å¥½çš„å±æ€§

- éœ€è¦è§£å†³ä»€ä¹ˆ
  
  - What do we exactly mean by â€œbest partitions the training set into homogeneous classes?â€  
    æˆ‘ä»¬ç©¶ç«Ÿæ˜¯ä»€ä¹ˆæ„æ€â€œæœ€ä½³åˆ†åŒºçš„è®­ç»ƒé›†åˆ°åŒè´¨ç±»?â€
  
  - What if every attribute splits the training set into non-homogeneous classes?  
    å¦‚æœæ¯ä¸ªå±æ€§éƒ½å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºéåŒæ„ç±»ä¼šæ€æ ·ï¼Ÿ
  
  - Which one is better?  
    å“ªä¸ªæ›´å¥½ï¼Ÿ

- è§£å†³æ–¹æ³•
  
  - **Information Theory** can help us **choosing**  
    ä¿¡æ¯è®ºå¯ä»¥å¸®åŠ©æˆ‘ä»¬é€‰æ‹©

##### Information theory ä¿¡æ¯è®º

* Studies the mathematical laws governing systems designed to **communicate** or **manipulate** information.  
  ç ”ç©¶ç”¨äºäº¤æµæˆ–æ“çºµä¿¡æ¯çš„ç³»ç»Ÿçš„æ•°å­¦è§„å¾‹ã€‚

* It defines **quantitative** measures of information and the capacity of various systems to **transmit**, **store**, and **process** information.  
  å®ƒå®šä¹‰äº†ä¿¡æ¯çš„å®šé‡åº¦é‡ä»¥åŠå„ç§ç³»ç»Ÿä¼ è¾“ã€å­˜å‚¨å’Œå¤„ç†ä¿¡æ¯çš„èƒ½åŠ›ã€‚

* it measures **the information content**, or e**ntropy**, of **messages/events**.  
  å®ƒåº¦é‡æ¶ˆæ¯/äº‹ä»¶çš„ä¿¡æ¯å†…å®¹æˆ–ç†µã€‚

* Information is measured in **bits**.  
  ä¿¡æ¯æ˜¯ä»¥ä½æ¥è¡¡é‡çš„ã€‚

* One bit represents the information we need to answer a yes/no question when we have no idea about the answer.  
  ä¸€ä¸ªä½è¡¨ç¤ºå½“æˆ‘ä»¬ä¸çŸ¥é“ç­”æ¡ˆæ—¶å›ç­”æ˜¯æˆ–å¦é—®é¢˜æ‰€éœ€è¦çš„ä¿¡æ¯ã€‚

---

##### Information Content / entropy ä¿¡æ¯å†…å®¹/ç†µ

If an event has ğ‘› possible outcomes $(X=i)$, each with prior probability $P(X=i)$, the **information content or entropy** ğ» of the eventâ€™s actual outcome is  
å¦‚æœä¸€ä¸ªäº‹ä»¶æœ‰ n ä¸ªå¯èƒ½çš„ç»“æœ $(X=i)$ ï¼Œæ¯ä¸ªéƒ½æœ‰å…ˆéªŒæ¦‚ç‡ $P(X=i)$ ï¼Œé‚£ä¹ˆè¯¥äº‹ä»¶å®é™…ç»“æœçš„ä¿¡æ¯å«é‡æˆ–ç†µ H æ˜¯  

$$
\mathrm{H}(X)=-\sum_{i=1}^{n} P(X=i) \log _{2} P(X=i)
$$

i.e., the average information content $-\log_2 P(X = i)$  of each possible outcome $X=i$ weighted by the outcomeâ€™s probability  
å³ï¼Œæ¯ä¸ªå¯èƒ½ç»“æœçš„å¹³å‡ä¿¡æ¯å†…å®¹ $- log _ 2P (X = i) $ç”±ç»“æœçš„æ¦‚ç‡åŠ æƒ

**<u>ï¼ï¼ï¼ç†µè¶Šé«˜æ•°æ®åˆ†å¸ƒå’Œæ™®é€‚æ€§è¶Šå¥½ï¼Œè¶Šæœ‰åˆ©äºè®­ç»ƒï¼ï¼ï¼</u>**

- Entropy is a measure of **disorder or uncertainty** 
  ç†µæ˜¯å¯¹æ— åºæˆ–ä¸ç¡®å®šæ€§çš„åº¦é‡

- a measure of â€Expected surpriseâ€   
  â€œæ„æ–™ä¹‹ä¸­çš„æƒŠå–œâ€çš„è¡¡é‡æ ‡å‡†

- The goal of machine learning model in general is to reduce uncertainty.   
  æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€»ä½“ç›®æ ‡æ˜¯é™ä½ä¸ç¡®å®šæ€§ã€‚

- Measured in **bits**.
  ç”¨ä½æ¥è¡¡é‡

**â€œLow Entropyâ€** ä½ç†µ

- Distribution of variable has many peaks and valleys   
  å˜é‡çš„åˆ†å¸ƒæœ‰å¤šä¸ªå³°è°·

- Histograms has many low and highs   
  ç›´æ–¹å›¾æœ‰è®¸å¤šä½ç‚¹å’Œé«˜ç‚¹

- Value sampled are more predictable (low disorder/high level of purity)   
  å–æ ·çš„å€¼æ›´å¯é¢„æµ‹(ä½æ— åº/é«˜çº¯åº¦)

**â€œHigh Entropyâ€**  é«˜ç†µ

- Variable has uniform like distribution  
  å˜é‡å…·æœ‰å‡åŒ€ä¼¼åˆ†å¸ƒ

- Flat histogram  
  å¹³ç›´ç›´æ–¹å›¾

- Value sampled are less predictable (high disorder/low level of purity)   
  å–æ ·çš„æ•°å€¼ä¸æ˜“é¢„æµ‹(é«˜æ— åºåº¦/ä½çº¯åº¦)

##### Entropy Formula ç†µå…¬å¼

- entropy
  
  $$
  \mathrm{H}(X)=-\sum_{i=1}^{n} P(X=i) \log _{2} P(X=i)
  $$

- Conditional Entropy æ¡ä»¶ç†µ
  
  $$
  H(Y|X) = - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log_2 p(y | x)
  $$

---

EXAMPLESï¼š

- Entropy of fair coin toss  å…¬å¹³æ·ç¡¬å¸çš„ç†µ
  
  $$
  H(P(h), P(t)) = H\left( \frac{1}{2}, \frac{1}{2} \right) = -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} = \frac{1}{2} + \frac{1}{2} = 1 \ \text{bit}

  $$

- Entropy of a loaded coin toss where $P(head) = 0.99$  åŠ è½½åæŠ›ç¡¬å¸çš„ç†µï¼Œå…¶ä¸­ $P (head) = 0.99 $
  
  $$
  H(P(h), P(t)) = H\left( \frac{99}{100}, \frac{1}{100} \right) = -0.99 \log_2 0.99 - 0.01 \log_2 0.01 \approx 0.08 \ \text{bits}
  $$

- Entropy of a loaded coin toss with heads on both side  ä¸¤è¾¹éƒ½æœ‰äººå¤´çš„åŠ è½½ç¡¬å¸æŠ•æ·çš„ç†µ
  
  $$
  H(P(h), P(t)) = H(1, 0) = -1 \log_2 1 - 0 \log_2 0 = 0 - 0 = 0 \ \text{bits}

  $$

- What is the entropy of a group in which all examples belong to the same ä¸€ä¸ªç¾¤çš„ç†µæ˜¯å¤šå°‘ï¼Œå…¶ä¸­æ‰€æœ‰çš„ä¾‹å­éƒ½å±äºåŒä¸€ä¸ªç¾¤
  
  $$
  = H(1) = -1 \log_2 1 = 0
  $$
  
  <u>not a good training set for learning ä¸æ˜¯å¾ˆå¥½çš„è®­ç»ƒæ•°æ®</u> 

- What is the entropy of a group in which all examples belong to the same class?  ä¸€ä¸ªç¾¤ä¸­æ‰€æœ‰çš„ä¾‹å­éƒ½å±äºåŒä¸€ä¸ªç±»çš„ç†µæ˜¯å¤šå°‘ï¼Ÿ
  
  $$
  = H\left( \frac{1}{2}, \frac{1}{2} \right) = -0.5 \log_2 0.5 - 0.5 \log_2 0.5 = 1
  $$
  
  <u>good training set for learning  è‰¯å¥½çš„å­¦ä¹ è®­ç»ƒæ•°æ®</u>

---

##### Entropy of a decision tree å†³ç­–æ ‘çš„ç†µ

- For decision trees, the event is question is whether the tree will return â€œyesâ€ or â€œnoâ€ for a given input example ğ‘’   
  å¯¹äºå†³ç­–æ ‘ï¼Œé—®é¢˜æ˜¯å¯¹äºç»™å®šçš„è¾“å…¥ç¤ºä¾‹ eï¼Œæ ‘æ˜¯å¦å°†è¿”å›â€œ yesâ€æˆ–â€œ noâ€

- Assume the training set ğ¸ is a **representative sample** of the domain   
  å‡è®¾è®­ç»ƒé›† E æ˜¯åŸŸçš„ä¸€ä¸ªä»£è¡¨æ€§æ ·æœ¬

- Then, the relative frequency of positive examples in ğ¸ closely approximates the prior probability of a positive example   
  ç„¶åï¼ŒE ä¸­æ­£é¢ä¾‹å­çš„ç›¸å¯¹é¢‘ç‡ä¸æ­£é¢ä¾‹å­çš„å…ˆéªŒæ¦‚ç‡éå¸¸æ¥è¿‘

- If ğ¸ contains ğ‘ positive examples and ğ‘› negative examples, the probability distribution of answers by a correct decision tree is:  
  å¦‚æœ E åŒ…å« p æ­£ä¾‹å­å’Œ n è´Ÿä¾‹å­ï¼Œæ­£ç¡®çš„å†³ç­–æ ‘çš„ç­”æ¡ˆæ¦‚ç‡åˆ†å¸ƒæ˜¯:
  
  $$
  P(\text{yes}) = \frac{p}{p + n} \quad \quad P(\text{no}) = \frac{n}{p + n}

  $$

- Entropy of a correct decision tree: æ­£ç¡®å†³ç­–æ ‘çš„ç†µ:
  
  $$
  H\left( \frac{p}{p + n}, \frac{n}{p + n} \right) = -\frac{p}{p + n} \log_2 \frac{p}{p + n} - \frac{n}{p + n} \log_2 \frac{n}{p + n}
  $$

##### Information gain ä¿¡æ¯å¢ç›Š

Measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable.  
æµ‹é‡æŒ‰ç…§ä¸€ä¸ªéšæœºå˜é‡çš„ç»™å®šå€¼å°†æ•°æ®é›†åˆ†å‰²åæ‰€å¼•èµ·çš„ç†µæˆ–æ„å¤–å‡å°‘ç¨‹åº¦ã€‚

$$
I(X_n, Y) = H(Y) - H(Y | X_n)

$$

ğ‘› = number of splits  N = åˆ†å‰²çš„æ¬¡æ•°

---

EXAMPLES

![bdb012de-c5ca-4c52-9ef9-dec8d2cf24cd](./images/bdb012de-c5ca-4c52-9ef9-dec8d2cf24cd.png)

Find: 

1. Entropy ğ›¨(ğ‘ƒğ‘’ğ‘œğ‘ğ‘™ğ‘’); 
   $H(\text{People}) = - \left( \frac{14}{30} \log_2 \frac{14}{30} \right) - \left( \frac{16}{30} \log_2 \frac{16}{30} \right) = 0.996$ 
2) Entropy ğ›¨(ğ¶â„ğ‘–ğ‘™ğ‘‘1); 
   $H(\text{Child}_1) = - \left( \frac{13}{17} \log_2 \frac{13}{17} \right) - \left( \frac{4}{17} \log_2 \frac{4}{17} \right) = 0.787$ 

3) Entropy ğ›¨(ğ¶â„ğ‘–ğ‘™ğ‘‘2); 
   $H(\text{Child}_2) = - \left( \frac{1}{13} \log_2 \frac{1}{13} \right) - \left( \frac{12}{13} \log_2 \frac{12}{13} \right) = 0.391$ 

4) Information Gain I for 1) - 3).
   Weighted average entropy of children å„¿ç«¥çš„åŠ æƒå¹³å‡æ•°ç†µ $= -\left( \frac{17}{30} \cdot 0.787 \right) - \left( \frac{13}{30} \cdot 0.391 \right) = 0.615$   
   Information Gain $I = 0.996 - 0.615 = 0.38$  for the split.

---

##### Decision Tree - Summary å†³ç­–æ ‘-æ€»ç»“

- At each level, one must choose:  åœ¨æ¯ä¸ªå±‚æ¬¡ï¼Œå¿…é¡»åšå‡ºä¸€ä¸ªé€‰æ‹©
  
  - Which variable to split. è¦æ‹†åˆ†å“ªä¸ªå˜é‡ã€‚
  
  - Possibly where to split it.  å¯èƒ½åœ¨å“ªé‡Œåˆ†ã€‚

- Choose them based on how much information we would gain from the decision!   
  æ ¹æ®æˆ‘ä»¬èƒ½ä»å†³ç­–ä¸­è·å¾—å¤šå°‘ä¿¡æ¯æ¥é€‰æ‹©å®ƒä»¬ï¼  
  (choose attribute that gives the highest gain)!   
  (é€‰æ‹©è·ç›Šæœ€é«˜çš„å±æ€§) ï¼

##### Decision tree - limitations å†³ç­–æ ‘-å±€é™æ€§

- **Noise.** Two training examples may have identical values for all the attributes but be classified differently.  
  å™ªå£°ã€‚ä¸¤ä¸ªè®­ç»ƒä¾‹å­å¯èƒ½å¯¹æ‰€æœ‰å±æ€§å…·æœ‰ç›¸åŒçš„å€¼ï¼Œä½†æ˜¯åˆ†ç±»ä¸åŒã€‚

- **Overfitting.** Irrelevant attributes may make spurious distinctions among training examples.  
  è¿‡æ‹Ÿåˆã€‚ä¸ç›¸å…³çš„å±æ€§å¯èƒ½ä¼šåœ¨è®­ç»ƒä¾‹å­ä¸­é€ æˆè™šå‡çš„åŒºåˆ«ã€‚

- **Missing data.** The value of some attributes of some training examples may be missing.   
  éƒ¨åˆ†æ•°æ®ç¼ºå¤±ã€‚æŸäº›è®­ç»ƒç¤ºä¾‹çš„æŸäº›å±æ€§çš„å€¼å¯èƒ½ç¼ºå°‘ã€‚

- **Multi-valued attributes.** The information gain of an attribute with many different values tends to be non-zero even when the attribute is irrelevant.  
  å¤šå€¼å±æ€§ã€‚å…·æœ‰è®¸å¤šä¸åŒå€¼çš„å±æ€§çš„ä¿¡æ¯å¢ç›Šå¾€å¾€æ˜¯éé›¶çš„ï¼Œå³ä½¿è¯¥å±æ€§æ˜¯ä¸ç›¸å…³çš„ã€‚

- **Continuous-valued attributes.** They must be discretized to be used.  
  è¿ç»­å€¼å±æ€§ã€‚å®ƒä»¬å¿…é¡»ç¦»æ•£åŒ–æ‰èƒ½ä½¿ç”¨ã€‚

#### Linear and non-linear classification methods  çº¿æ€§å’Œéçº¿æ€§åˆ†ç±»æ–¹æ³•

##### linear techniques çº¿æ€§æ–¹æ³•

###### Nearest Neighbor Classifier æœ€è¿‘é‚»åˆ†ç±»å™¨

Nearest Neighbor â€“ for each test data point, assign the class label of the nearest training data point  
æœ€è¿‘é‚»-å¯¹äºæ¯ä¸ªæµ‹è¯•æ•°æ®ç‚¹ï¼Œåˆ†é…æœ€è¿‘è®­ç»ƒæ•°æ®ç‚¹çš„ç±»æ ‡ç­¾

- Adopt a distance function to find the nearest neighbor   
  é‡‡ç”¨è·ç¦»å‡½æ•°æ±‚æœ€è¿‘é‚»
  
  - Calculate the distance to each data point in the training set, and assign the class of the nearest data point (minimum distance)   
    è®¡ç®—åˆ°è®­ç»ƒé›†ä¸­æ¯ä¸ªæ•°æ®ç‚¹çš„è·ç¦»ï¼Œå¹¶åˆ†é…æœ€è¿‘æ•°æ®ç‚¹çš„ç±»(æœ€å°è·ç¦»)

- It does not require learning a set of weights  
  å®ƒä¸éœ€è¦å­¦ä¹ ä¸€ç»„æƒé‡

<img src="./images/13d27133-8783-4746-a821-afb2c8574cda.png" title="" alt="13d27133-8783-4746-a821-afb2c8574cda" style="zoom:33%;">

- For image classification, the distance between all pixels is calculated (e.g., using $l_1$ norm, or $l_2$ norm) 
  å¯¹äºå›¾åƒåˆ†ç±»ï¼Œè®¡ç®—æ‰€æœ‰åƒç´ ä¹‹é—´çš„è·ç¦»(ä¾‹å¦‚ï¼Œä½¿ç”¨ $l _ 1 $æ ‡å‡†æˆ– $l _ 2 $æ ‡å‡†)

- Disadvantages: ç¼ºç‚¹:
  
  - The classifier **must remember** all training data and store it for future comparisons with the test data   
    åˆ†ç±»å™¨å¿…é¡»è®°ä½æ‰€æœ‰çš„è®­ç»ƒæ•°æ®å¹¶å­˜å‚¨å®ƒï¼Œä»¥ä¾¿å°†æ¥ä¸æµ‹è¯•æ•°æ®è¿›è¡Œæ¯”è¾ƒ
  
  - Classifying a test image is **expensive** since it requires a comparison to all training images  
    åˆ†ç±»æµ‹è¯•å›¾åƒæ˜¯æ˜‚è´µçš„ï¼Œå› ä¸ºå®ƒéœ€è¦æ¯”è¾ƒæ‰€æœ‰çš„è®­ç»ƒå›¾åƒ

![a87972e5-657d-4a08-b7ee-3e7dab6899d5](./images/a87972e5-657d-4a08-b7ee-3e7dab6899d5.png)

###### k-Nearest Neighbors Classifier k-è¿‘é‚»åˆ†ç±»å™¨

k-Nearest Neighbors approach considers multiple neighboring data points to classify a test data point  
kè¿‘é‚»æ–¹æ³•è€ƒè™‘å¤šä¸ªç›¸é‚»æ•°æ®ç‚¹å¯¹æµ‹è¯•æ•°æ®ç‚¹è¿›è¡Œåˆ†ç±»

<img src="./images/869fec1d-b42c-4ab6-b1e4-2591334baf68.png" title="" alt="869fec1d-b42c-4ab6-b1e4-2591334baf68" style="zoom:50%;">

###### Linear Classifier çº¿æ€§åˆ†ç±»å™¨

- Find a linear function f of the inputs $x_i$ that separates the classes   
  æ‰¾åˆ°åˆ†éš”ç±»çš„è¾“å…¥çš„çº¿æ€§å‡½æ•°

- $f(x_i,W,b) = Wx_i+b$  **$W,b$ä¸ºå‚æ•°**

- Use pairs of inputs and labels to find the **weights matrix W** and the **bias vector b** The weights and biases are the **parameters** of the function f  
  ä½¿ç”¨è¾“å…¥å’Œæ ‡ç­¾å¯¹æ¥å¯»æ‰¾æƒé‡çŸ©é˜µ W å’Œåå·®å‘é‡ b æƒé‡å’Œåå·®æ˜¯å‡½æ•° f çš„å‚æ•°

- Several methods have been used to find the optimal set of parameters of a linear classifier.   
  A common method of choice is the **Perceptron algorithm**, where the parameters are updated until a minimal error is reached (single layer, does not use backpropagation)   
  æœ‰å‡ ç§æ–¹æ³•å·²ç»è¢«ç”¨æ¥å¯»æ‰¾çº¿æ€§åˆ†ç±»å™¨çš„æœ€ä½³å‚æ•°é›†ã€‚  
  ä¸€ä¸ªå¸¸è§çš„é€‰æ‹©æ–¹æ³•æ˜¯æ„ŸçŸ¥å™¨ç®—æ³•ï¼Œå…¶ä¸­çš„å‚æ•°è¢«æ›´æ–°ï¼Œç›´åˆ°è¾¾åˆ°æœ€å°çš„é”™è¯¯(å•å±‚ï¼Œä¸ä½¿ç”¨åå‘ä¼ æ’­)

- Linear classifier is a simple approach, but it is a building block of advanced classification algorithms, such as SVM and neural networks Earlier multi-layer neural networks were referred to as multi-layer perceptrons (MLPs)  
  çº¿æ€§åˆ†ç±»å™¨æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä½†å®ƒæ˜¯å…ˆè¿›çš„åˆ†ç±»ç®—æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºå’Œç¥ç»ç½‘ç»œçš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚æ—©æœŸçš„å¤šå±‚ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºå¤šå±‚æ„ŸçŸ¥å™¨(MLPs)
  
  

- The decision boundary is linear  å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„
  
  - A straight line in 2D, a flat plane in 3D, a hyperplane in 3D and higher dimensional space  
    äºŒç»´çš„ç›´çº¿ï¼Œä¸‰ç»´çš„å¹³é¢ï¼Œä¸‰ç»´çš„è¶…å¹³é¢å’Œé«˜ç»´ç©ºé—´

###### Support Vector Machines  æ”¯æŒå‘é‡æœº

- How to find the best decision boundary?   
  å¦‚ä½•æ‰¾åˆ°æœ€å¥½çš„å†³ç­–è¾¹ç•Œï¼Ÿ
  
  - All lines in the figure correctly separate the 2 classes   
    å›¾ä¸­çš„æ‰€æœ‰çº¿æ¡æ­£ç¡®åœ°å°†ä¸¤ä¸ªç±»åˆ†å¼€
  
  - The line that is farthest from all training examples will have better generalization capabilities   
    è·ç¦»æ‰€æœ‰åŸ¹è®­å®ä¾‹æœ€è¿œçš„çº¿å°†å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

- SVM solves an optimization problem:   
  SVM è§£å†³äº†ä¸€ä¸ªæœ€ä½³åŒ–é—®é¢˜:
  
  - First, identify a decision boundary that correctly classifies the examples   
    é¦–å…ˆï¼Œç¡®å®šä¸€ä¸ªæ­£ç¡®åˆ†ç±»ç¤ºä¾‹çš„å†³ç­–è¾¹ç•Œ
  
  - Next, increase the geometric margin between the boundary and all examples   
    æ¥ä¸‹æ¥ï¼Œå¢åŠ è¾¹ç•Œå’Œæ‰€æœ‰ç¤ºä¾‹ä¹‹é—´çš„å‡ ä½•è¾¹ç•Œ

- The data points that define the **maximum margin width** are called **support vectors**   
  å®šä¹‰æœ€å¤§è¾¹è·å®½åº¦çš„æ•°æ®ç‚¹ç§°ä¸ºæ”¯æŒå‘é‡

- Find W and b by solving:
  
  $$
  \min \frac{1}{2} \| w \|^2 \\
\text{s.t. } y_i (w \cdot x_i + b) \geq 1, \quad \forall x_i
  $$

<img src="./images/eb47cdcf-6214-4f0f-8683-332551cdbf1d.png" title="" alt="eb47cdcf-6214-4f0f-8683-332551cdbf1d" style="zoom:50%;">

##### Linear vs Non-linear Techniques

- techniques
  
  - Linear classification techniques çº¿æ€§åˆ†ç±»æ–¹æ³•
    
    - Linear classifier çº¿æ€§åˆ†ç±»å™¨
    
    - Perceptron æ„ŸçŸ¥æœº
    
    - Logistic regression é€»è¾‘å›å½’
    
    - Linear SVM çº¿æ€§æ”¯æŒå‘é‡æœº
    
    - NaÃ¯ve Bayes è´å¶æ–¯æœº
  
  - Non-linear classification techniques éçº¿æ€§åˆ†ç±»æ–¹æ³•
    
    - k-nearest neighbors  K-è¿‘é‚»
    
    - Non-linear SVM éçº¿æ€§æ”¯æŒå‘é‡æœº
    
    - Neural networks  ç¥ç»ç½‘ç»œ
    
    - Decision trees  å†³ç­–æ ‘
    
    - Random forest  éšæœºæ£®æ—

- compare
  
  - For some tasks, input data can be linearly separable, and linear classifiers can be suitably applied   
    å¯¹äºæŸäº›ä»»åŠ¡ï¼Œè¾“å…¥æ•°æ®å¯ä»¥çº¿æ€§åˆ†ç¦»ï¼Œé€‚å½“åº”ç”¨çº¿æ€§åˆ†ç±»å™¨
  
  - For other tasks, linear classifiers may have difficulties to produce adequate decision boundaries  
    å¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œçº¿æ€§åˆ†ç±»å™¨å¯èƒ½éš¾ä»¥äº§ç”Ÿè¶³å¤Ÿçš„å†³ç­–è¾¹ç•Œ

<img src="./images/20312627-1b61-4b46-b625-46c429f303c9.png" title="" alt="20312627-1b61-4b46-b625-46c429f303c9" style="zoom:33%;">

##### Non-linear Techniques éçº¿æ€§æ–¹æ³•

###### Non-linear classification éçº¿æ€§åˆ†ç±»

- Features $z_i$ are obtained as **non-linear functions** of the inputs $x_i$   
  ç‰¹å¾ $z _ i $ä½œä¸ºè¾“å…¥ $x _ i $çš„ **éçº¿æ€§å‡½æ•°** è·å¾—

- It results in non-linear decision boundaries   
  å®ƒå¯¼è‡´éçº¿æ€§å†³ç­–è¾¹ç•Œ

- Can deal with non-linearly separable data   
  å¯ä»¥å¤„ç†éçº¿æ€§å¯åˆ†æ•°æ®

<img src="./images/3cfa53a6-975f-4e54-87de-01977fc85609.png" title="" alt="3cfa53a6-975f-4e54-87de-01977fc85609" style="zoom:33%;">

###### Non-linear Support Vector Machines  éçº¿æ€§æ”¯æŒå‘é‡æœº

- The original input space is mapped to a higher-dimensional feature space where the training set is linearly separable  
  å°†åŸå§‹è¾“å…¥ç©ºé—´æ˜ å°„åˆ°è®­ç»ƒé›†çº¿æ€§å¯åˆ†çš„é«˜ç»´ç‰¹å¾ç©ºé—´

- Define a non-linear kernel function to calculate a non-linear decision boundary in the original feature space  
  å®šä¹‰ä¸€ä¸ªéçº¿æ€§æ ¸å‡½æ•°æ¥è®¡ç®—åŸå§‹ç‰¹å¾ç©ºé—´ä¸­çš„éçº¿æ€§å†³ç­–è¾¹ç•Œ

<img src="./images/820a9c9c-a20b-4c6b-9fb0-9a558421db82.png" title="" alt="820a9c9c-a20b-4c6b-9fb0-9a558421db82" style="zoom:33%;">

###### Binary vs Multi-class Classification  äºŒåˆ†ç±»ä¸å¤šåˆ†ç±»

- A classification problem with only 2 classes is referred to as binary classification. The output labels are 0 or 1.   
  åªæœ‰ä¸¤ä¸ªç±»çš„åˆ†ç±»é—®é¢˜ç§°ä¸ºäºŒè¿›åˆ¶åˆ†ç±»ï¼Œè¾“å‡ºæ ‡ç­¾ä¸º0æˆ–1ã€‚

- A problem with 3 or more classes is referred to as multi-class classification  
  æœ‰3ä¸ªæˆ–æ›´å¤šç±»çš„é—®é¢˜ç§°ä¸ºå¤šç±»åˆ†ç±»

- Both the binary and multi-class classification problems can be linearly or nonlinearly separated   
  äºŒè¿›åˆ¶å’Œå¤šç±»åˆ†ç±»é—®é¢˜éƒ½å¯ä»¥çº¿æ€§æˆ–éçº¿æ€§åˆ†ç¦»   
  
  <img src="./images/4f5829f8-91a0-415a-a059-13ff08b7ccc6.png" title="" alt="4f5829f8-91a0-415a-a059-13ff08b7ccc6" style="zoom:33%;">

##### No-Free-Lunch Theorem ä¸å…è´¹åˆé¤å®šç†

- The derived classification models for supervised learning are simplifications of the reality   
  è¡ç”Ÿå‡ºæ¥çš„ç›‘ç£å¼å­¦ä¹ åˆ†ç±»æ¨¡å‹æ˜¯å¯¹ç°å®çš„ç®€åŒ–
  
  - The simplifications are based on certain assumptions.   
    è¿™äº›ç®€åŒ–æ˜¯åŸºäºæŸäº›å‡è®¾ã€‚
  
  - The assumptions fail in some situations.   
    è¿™äº›å‡è®¾åœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¤±è´¥ã€‚

- <u>In summary, No-Free-Lunch Theorem states:   
  æ€»è€Œè¨€ä¹‹ï¼Œâ€œæ²¡æœ‰å…è´¹åˆé¤â€å®šç†æŒ‡å‡º:</u>
  
  - <u>No single classifier works the best for all possible problems   
    å¯¹äºæ‰€æœ‰å¯èƒ½çš„é—®é¢˜ï¼Œæ²¡æœ‰ä¸€ä¸ªåˆ†ç±»å™¨æ˜¯æœ€å¥½çš„</u>
  
  - <u>Since we need to make assumptions to generalize  
    å› ä¸ºæˆ‘ä»¬éœ€è¦åšä¸€äº›å‡è®¾æ¥æ¨å¹¿</u>

## Week 2: Deep Learning & Reinforcement Learning ç¬¬äºŒå‘¨: æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ 

- Part 1: Deep Learning 
  
  - Introduction to deep learning 
  
  - Elements of neural networks and activation functions 
  
  - Training NNs 
  
  - Gradient descent 
  
  - Regularization methods 
  
  - NN architectures 

- Part 2: Reinforcement Learning 
  
  - Introduction to Reinforcement Learning 
  
  - Markov Decision Processes (MDPs) 
  
  - RL Techniques: From Q-learning to Actor-Critic 
  
  - Applications of RL

### Part 1: Deep Learning ç¬¬ä¸€éƒ¨åˆ†: æ·±åº¦å­¦ä¹ 

- Introduction to deep learning 

- Elements of neural networks and activation functions 

- Training NNs 

- Gradient descent 

- Regularization methods 

- NN architectures

#### Introduction to deep learning  æ·±åº¦å­¦ä¹ å…¥é—¨

##### ML vs. Deep Learning æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ 

- Conventional machine learning methods **rely on human-designed feature representations**   
  ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•ä¾èµ–äºäººå·¥è®¾è®¡çš„ç‰¹å¾è¡¨ç¤º
  
  - ML becomes just optimizing weights to best make a final prediction ï°  
    æœºå™¨å­¦ä¹ åªæ˜¯ä¼˜åŒ–æƒé‡ï¼Œä»¥æœ€å¥½åœ°åšå‡ºæœ€ç»ˆé¢„æµ‹ 

- Deep learning (DL) is a machine learning subfield that uses multiple layers for learning data representations   
  æ·±åº¦å­¦ä¹ (DL)æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ å­é¢†åŸŸï¼Œå®ƒä½¿ç”¨å¤šä¸ªå±‚æ¬¡æ¥å­¦ä¹ æ•°æ®è¡¨ç¤º
  
  - DL is exceptionally effective at **learning patterns**  
    DL åœ¨å­¦ä¹ æ¨¡å¼æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆ

<img src="./images/e5288aae-dfc3-405d-b06d-fa2277099766.png" title="" alt="e5288aae-dfc3-405d-b06d-fa2277099766" style="zoom:50%;">

- DL applies a multi-layer process for learning rich hierarchical features (i.e., data representations)   
  DL åº”ç”¨å¤šå±‚è¿‡ç¨‹æ¥å­¦ä¹ ä¸°å¯Œçš„å±‚æ¬¡ç‰¹æ€§(å³æ•°æ®è¡¨ç¤º)
  
  - Input image pixels â†’ Edges â†’ Textures â†’ Parts â†’ Objects

##### Why is DL Useful?

- DL provides a flexible, learnable framework for representing visual, text, linguistic information   
  DL ä¸ºè¡¨ç¤ºè§†è§‰ã€æ–‡æœ¬å’Œè¯­è¨€ä¿¡æ¯æä¾›äº†ä¸€ä¸ªçµæ´»çš„ã€å¯å­¦ä¹ çš„æ¡†æ¶

- Can learn in supervised and unsupervised manner   
  å¯ä»¥åœ¨æœ‰ç›‘ç£å’Œæ— ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ 

- an effective end-to-end learning system   
  æœ‰æ•ˆçš„ç«¯åˆ°ç«¯å­¦ä¹ ç³»ç»Ÿ

- Requires large amounts of training data   
  éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®
  
  

- Since about 2010, DL has outperformed other ML techniques   
  è‡ª2010å¹´ä»¥æ¥ï¼ŒDL å·²ç»è¶…è¶Šäº†å…¶ä»–æœºå™¨å­¦ä¹ æŠ€æœ¯  
  First in vision and speech, then NLP, and other applications  
  é¦–å…ˆæ˜¯è§†è§‰å’Œè¯­è¨€ï¼Œç„¶åæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶ä»–åº”ç”¨

##### A biological neuron vs. artificial neuron ç”Ÿç‰©ç¥ç»å…ƒå¯¹æ¯”äººå·¥ç¥ç»å…ƒ

Brains advantages with respect to digital computers:  
ä¸æ•°å­—è®¡ç®—æœºç›¸æ¯”ï¼Œå¤§è„‘çš„ä¼˜åŠ¿

- Massively parallel  å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†

- Fault-tolerant  å®¹é”™

- Reliable  å¯é çš„

- Graceful degradation ä¼˜é›…é™çº§

##### Representational Power è¡¨è±¡æ€§

NNs with at least one hidden layer are **universal approximators**  
å…·æœ‰è‡³å°‘ä¸€ä¸ªéšå±‚çš„ç¥ç»ç½‘ç»œæ˜¯é€šç”¨é€¼è¿‘å™¨

(å…·æœ‰è‡³å°‘ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œæ˜¯é€šç”¨é€¼è¿‘å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºä»»ä½•è¿ç»­å‡½æ•° $h(x)$ å’Œä»»æ„çš„å°è¯¯å·® $\epsilon >0$ï¼Œæ€»å­˜åœ¨ä¸€ä¸ªåªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ $f(x)$ï¼Œä½¿å¾—å¯¹äºæ‰€æœ‰ $x$ï¼Œéƒ½æ»¡è¶³ $âˆ£h(x)âˆ’f(x)âˆ£< \epsilon $ã€‚)

NN can approximate any arbitrary complex continuous function  
ç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„å¤æ‚çš„è¿ç»­å‡½æ•°

NNs use nonlinear mapping of the inputs x to the outputs f(x) to compute complex decision boundaries  
ç¥ç»ç½‘ç»œä½¿ç”¨è¾“å…¥ x åˆ°è¾“å‡º f (x)çš„éçº¿æ€§æ˜ å°„æ¥è®¡ç®—å¤æ‚çš„å†³ç­–è¾¹ç•Œ

- reason of use deeper NNs:
  
  - The fact that deep NNs work better is an empirical observation   
    äº‹å®ä¸Šï¼Œæ·±å±‚ç¥ç»ç½‘ç»œå·¥ä½œå¾—æ›´å¥½æ˜¯ä¸€ä¸ªç»éªŒè§‚å¯Ÿ
  
  - Mathematically, deep NNs have the same representational power as a one-layer NN  
    ä»æ•°å­¦ä¸Šè®²ï¼Œæ·±å±‚ç¥ç»ç½‘ç»œå…·æœ‰ä¸å•å±‚ç¥ç»ç½‘ç»œç›¸åŒçš„è¡¨ç¤ºèƒ½åŠ›

##### Introduction to Neural Networks ç¥ç»ç½‘ç»œå¯¼è®º

Handwritten digit recognition (MNIST dataset)   
æ‰‹å†™æ•°å­—è¯†åˆ«(MNIST æ•°æ®é›†)

- The intensity of each pixel is considered an input element  
  æ¯ä¸ªåƒç´ çš„å¼ºåº¦è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªè¾“å…¥å…ƒç´ 

- Output is the class of the digit  
  è¾“å‡ºæ˜¯æ•°å­—çš„ç±»

å¯¹äºæ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªå›¾ç‰‡çŸ©é˜µï¼Œè¾“å‡ºä¸ºä»1åˆ°0çš„æ•°å­—çš„æ¦‚ç‡ï¼ˆEach dimension represents the confidence of a digit  æ¯ä¸ªç»´è¡¨ç¤ºä¸€ä¸ªæ•°å­—çš„ç½®ä¿¡åº¦ï¼‰

#### Elements of neural networks and activation functions ç¥ç»ç½‘ç»œå…ƒç´ å’Œæ¿€æ´»å‡½æ•°

##### Elements of Neural Networks ç¥ç»ç½‘ç»œè¦ç´ 

- NNs consist of hidden layers with neurons (i.e., computational units) ï°  
  ç¥ç»ç½‘ç»œç”±å¸¦æœ‰ç¥ç»å…ƒ(å³è®¡ç®—å•å…ƒ)çš„éšå±‚ç»„æˆ

- A single neuron maps a set of inputs into an output number, or $f:R^k \to R $  
  å•ä¸ªç¥ç»å…ƒå°†ä¸€ç»„è¾“å…¥æ˜ å°„åˆ°ä¸€ä¸ªè¾“å‡ºæ•°å­—ï¼Œå³ $f: R ^ k\to R $

- neuron ç¥ç»å…ƒ
  
  $$
  z = a_1 w_1 + a_2 w_2 + \cdots + a_K w_K + b
\\
a = \sigma(z)

  $$
  
  $a_i$ input è¾“å…¥, $w_k$ weights æƒé‡, $b$ bias åŸºç¡€å€¼, $\sigma (z)$ avtivation function æ¿€æ´»å‡½æ•°, $a$ outputè¾“å‡º.

- hidden layerï¼ˆå›¾ç‰‡ä¸­é—´çš„å±‚ï¼‰
  
  <img title="" src="./images/62ba9238-5f3e-4046-90dd-a55dd5cf485d.png" alt="62ba9238-5f3e-4046-90dd-a55dd5cf485d" style="zoom:33%;">
  
  $hidden \ layer \ h=\sigma (W_1x+b_1)$
  
  - å›¾ç‰‡ä¸­ï¼š
    4 + 2 = 6 neurons (not counting inputs)   
    [3 Ã— 4] + [4 Ã— 2] = 20 weights  
    4 + 2 = 6 biases   
    26 learnable parameters

- Deep NNs have many **hidden layers**   
  æ·±å±‚ç¥ç»ç½‘ç»œæœ‰è®¸å¤šéšè—å±‚
  
  - **Fully-connected (dense)** layers (a.k.a. **Multi-Layer Perceptron or MLP**)   
    å®Œå…¨è¿æ¥(å¯†é›†)å±‚(åˆç§°å¤šå±‚æ„ŸçŸ¥å™¨æˆ– MLP)
  
  - Each neuron is connected to all neurons in the succeeding layer  
    æ¯ä¸ªç¥ç»å…ƒè¿æ¥åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒ
  
  - <img src="./images/ed7c6e99-e3e1-4ffd-b3d2-ffc99af62b15.png" title="" alt="ed7c6e99-e3e1-4ffd-b3d2-ffc99af62b15" style="zoom:50%;">

###### Matrix Operation çŸ©é˜µæ“ä½œ

- Matrix operations are helpful when working with multidimensional inputs and outputs  
  çŸ©é˜µè¿ç®—åœ¨å¤„ç†å¤šç»´è¾“å…¥å’Œè¾“å‡ºæ—¶å¾ˆæœ‰å¸®åŠ©

- $$
  \sigma(Wx+b)=a
\\ \
\\
\sigma \left( \begin{bmatrix} 1 & -2 \\ -1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix} + \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right) = \begin{bmatrix} 0.98 \\ 0.12 \end{bmatrix}

  $$

- Multilayer NN, matrix calculations for the first layer  
  å¤šå±‚ç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€å±‚çš„çŸ©é˜µè®¡ç®—
  $\text{Input vector } x, \text{ weights matrix } W^1, \text{ bias vector } b^1, \text{ output vector } a^1$ 
  
  <img src="./images/f722ac4f-37b1-4c08-98ec-283b20bf57c7.png" title="" alt="f722ac4f-37b1-4c08-98ec-283b20bf57c7" style="zoom:50%;">

##### Activation Functions æ¿€æ´»å‡½æ•°

- **Non-linear activations** are needed to learn complex (non-linear) data representations  
  å­¦ä¹ å¤æ‚(éçº¿æ€§)æ•°æ®è¡¨ç¤ºéœ€è¦éçº¿æ€§æ¿€æ´»  
  Otherwise, NNs would be just a linear function (such as $W_1W_2ğ‘¥ = ğ‘Šğ‘¥$)   
  å¦åˆ™ï¼ŒNN å°†åªæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°(ä¾‹å¦‚ $W _ 1W _ 2x = Wx $)

- NNs with large number of layers (and neurons) can approximate more complex functions   
   å…·æœ‰å¤§é‡å±‚(å’Œç¥ç»å…ƒ)çš„ç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘æ›´å¤æ‚çš„å‡½æ•°
  
  - Figure: more neurons improve representation (but, may overfit)  
    å›¾: æ›´å¤šçš„ç¥ç»å…ƒæ”¹å–„è¡¨å¾(ä½†æ˜¯ï¼Œå¯èƒ½è¿‡åº¦)

###### Activation: Linear Function æ¿€æ´»: çº¿æ€§å‡½æ•°

- **Linear function** means that the output signal is proportional to the input signal to the neuron  
  çº¿æ€§å‡½æ•°è¡¨ç¤ºè¾“å‡ºä¿¡å·ä¸ç¥ç»å…ƒçš„è¾“å…¥ä¿¡å·æˆæ­£æ¯”
  
  $$
  f(x) = cx,\mathbb{R}^n \rightarrow \mathbb{R}^n
  $$

- <img title="" src="./images/3a7b52d8-11f6-4d42-b663-8e7485af30cc.png" alt="3a7b52d8-11f6-4d42-b663-8e7485af30cc" style="zoom:33%;" data-align="center">
  
  - If the value of the constant c is 1, it is also called **identity activation function**  
    å¦‚æœå¸¸æ•° c çš„å€¼ä¸º1ï¼Œå®ƒä¹Ÿè¢«ç§°ä¸ºæ’ç­‰å¼æ¿€æ´»å‡½æ•°
  
  - This activation type is used in **regression problems**  
    æ­¤æ¿€æ´»ç±»å‹ç”¨äºå›å½’é—®é¢˜

###### Activation: Sigmoid sigmoidå‡½æ•°

- **Sigmoid function** Ïƒ: takes a real-valued number and â€œsquashesâ€ it into the range between 0 and 1   
  Så½¢å‡½æ•° Ïƒ: å–ä¸€ä¸ªå®å€¼æ•°ï¼Œå¹¶å°†å…¶â€œå‹ç¼©â€åˆ°0åˆ°1ä¹‹é—´çš„èŒƒå›´å†…
  
  $$
  f(x) = \frac{1}{1+e^{-x}},\mathbb{R}^n \rightarrow [0, 1]
  $$
  
  - The output can be interpreted as the firing rate of a biological neuron  
    è¾“å‡ºå¯ä»¥è§£é‡Šä¸ºç”Ÿç‰©ç¥ç»å…ƒçš„æ”¾ç”µé€Ÿç‡
  
  - When the neuronâ€™s activation are 0 or 1, sigmoid neurons saturate 
    å½“ç¥ç»å…ƒæ¿€æ´»ä¸º0æˆ–1æ—¶ï¼Œsigmoidç¥ç»å…ƒé¥±å’Œ
    
    - Gradients at these regions are almost zero (almost no signal will flow)  
      è¿™äº›åŒºåŸŸçš„æ¢¯åº¦å‡ ä¹ä¸ºé›¶(å‡ ä¹æ²¡æœ‰ä¿¡å·ä¼šæµåŠ¨)
  
  - Sigmoid activations are less common in modern NNs  
    sigmoidæ¿€æ´»åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­ä¸å¸¸è§
  
  <img title="" src="./images/c4ea676b-b0bf-48d4-9657-8bfa453903a7.png" alt="c4ea676b-b0bf-48d4-9657-8bfa453903a7" style="zoom:50%;" data-align="center">

###### Activation: Tanh

- Tanh function: takes a real-valued number and â€œsquashesâ€ it into range between -1 and 1   
  Tanh å‡½æ•°: è·å–ä¸€ä¸ªå®å€¼æ•°å­—ï¼Œå¹¶å°†å…¶â€œå‹ç¼©â€åˆ° -1åˆ°1ä¹‹é—´
  
  $$
  \tanh(x) = \frac{2}{1 + e^{-2x}} - 1, \mathbb{R}^n \rightarrow [-1, 1]

  $$
  
  - Like sigmoid, tanh neurons saturate é¥±å’Œ
  
  - Unlike sigmoid, the output is zero-centered ä¸ sigmoid ä¸åŒï¼Œè¾“å‡ºæ˜¯ä»¥é›¶ä¸ºä¸­å¿ƒçš„
    
    - It is therefore preferred than sigmoid  æ¯”sigmoidæ›´å¥½
  
  - Tanh is a scaled sigmoid: $\tanh(x) = 2 \cdot \sigma(2x) - 1$ 
  
  <img title="" src="./images/06d83913-7c96-4a6f-baec-e26d70af6bd7.png" alt="06d83913-7c96-4a6f-baec-e26d70af6bd7" style="zoom:33%;" data-align="center">

###### Activation: ReLU

- ReLU (Rectified Linear Unit): takes a real-valued number and thresholds it at zero  
  ä¿®æ­£çº¿æ€§å•ä½(ReLU) : å–ä¸€ä¸ªå®å€¼æ•°ï¼Œé˜ˆå€¼ä¸ºé›¶
  
  $$
  f(x) = \max(0, x)
\\ \ \\
f(x) = \begin{cases} 
0 & \text{for } x < 0 \\ 
x & \text{for } x \geq 0 
\end{cases}
\\ \ \\
\mathbb{R}^n \rightarrow \mathbb{R}_{+}^n
  $$

- åº”ç”¨
  
  - Most modern deep NNs use ReLU activations   
    å¤§å¤šæ•°ç°ä»£æ·±å±‚ç¥ç»ç½‘ç»œä½¿ç”¨ ReLU æ¿€æ´»

- ä¼˜åŠ¿
  
  - ReLU is fast to compute (Compared to sigmoid, tanh ; Simply threshold a matrix at zero)
    ReLU è®¡ç®—é€Ÿåº¦å¾ˆå¿« (ä¸ sigmoid ç›¸æ¯”ï¼Œtanh; ç®€å•åœ°é˜ˆå€¼ä¸ºé›¶çš„çŸ©é˜µ)
  
  - Accelerates the convergence of gradient descent  (Due to linear, non-saturating form)
    åŠ é€Ÿæ¢¯åº¦ä¸‹é™æ³•çš„èåˆ (ç”±äºçº¿æ€§ï¼Œéé¥±å’Œå½¢å¼)
  
  - Prevents the gradient vanishing problem  
    é˜²æ­¢æ¸å˜æ¶ˆå¤±é—®é¢˜

Â Â Â Â Â Â Â Â Â Â Â Â <img title="" src="./images/180bc5dc-317d-436d-b0e8-f1b5a79acb40.png" alt="180bc5dc-317d-436d-b0e8-f1b5a79acb40" style="zoom:50%;" data-align="center">

###### Activation: Leaky ReLU

- æ™®é€šReLUçš„é—®é¢˜  
  
  - The problem of ReLU activations: they can â€œdieâ€  
    ReLU æ¿€æ´»çš„é—®é¢˜: å®ƒä»¬å¯èƒ½â€œæ­»äº¡â€  
  
  - ReLU could cause weights to update in a way that the gradients can become zero and the neuron will not activate again on any data  
    ReLU å¯èƒ½å¯¼è‡´æƒé‡æ›´æ–°çš„æ–¹å¼ï¼Œæ¢¯åº¦å¯ä»¥æˆä¸ºé›¶ï¼Œç¥ç»å…ƒä¸ä¼šå†æ¬¡æ¿€æ´»ä»»ä½•æ•°æ®

- **Leaky ReLU** activation function is a variant of ReLU 
  
  - Instead of the function being 0 when ğ‘¥<0, a leaky ReLU has a small negative slope (e.g., Î± = 0.01, or similar)  
    å½“ x < 0æ—¶ï¼Œå‡½æ•°ä¸æ˜¯0ï¼Œè€Œæ˜¯ä¸€ä¸ªå°çš„è´Ÿæ–œç‡(ä¾‹å¦‚ï¼ŒÎ± = 0.01ï¼Œæˆ–ç±»ä¼¼)
  
  $$
  f(x) = \begin{cases} 
ax & \text{for } x < 0 \\ 
x & \text{for } x \gg 0 
\end{cases}
  $$
  
  - ä¼˜åŠ¿ï¼ˆè§£å†³é—®é¢˜ï¼‰  
     resolves the dying ReLU problem 
  
  - åº”ç”¨   
    
    - Most current works still use ReLU  
      ç›®å‰å¤§éƒ¨åˆ†çš„ä½œå“ä»ç„¶ä½¿ç”¨ ReLU
    
    - With a proper setting of the learning rate, the problem of dying ReLU can be avoided    
      é€šè¿‡åˆç†è®¾ç½®å­¦ä¹ é€Ÿç‡ï¼Œå¯ä»¥é¿å… RLU æ­»äº¡çš„é—®é¢˜

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <img title="" src="./images/bd95c434-47bf-4ac1-9674-4e5e68be8d6f.png" alt="bd95c434-47bf-4ac1-9674-4e5e68be8d6f" style="zoom:67%;">

###### Activation: Softmax

- The softmax layer applies softmax activations to output a probability value in the range [0, 1]  
  Softmax å±‚åº”ç”¨ softmax æ¿€æ´»æ¥è¾“å‡ºèŒƒå›´[0,1]å†…çš„æ¦‚ç‡å€¼

- å³å°†æ‰€æœ‰çš„è¾“å…¥è§„èŒƒæˆä¸€ä¸ªä»0åˆ°1çš„æ¦‚ç‡å€¼ï¼Œä¸”æ¯ä¸€ä¸ªæ¦‚ç‡çš„å€¼åŠ èµ·æ¥å°±æ˜¯1

- The values z inputted to the softmax layer are referred to as **logits**   
  è¾“å…¥åˆ° softmax å±‚çš„å€¼ z ç§°ä¸º logits

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \quad \quad (\text{æ»¡è¶³} \sum_{i=1}^n \sigma(z_i) = 1)
\\ \ \\
\sigma(z_i) \text{ è¡¨ç¤ºç¬¬ } i \text{ ä¸ªç±»åˆ«çš„Softmaxè¾“å‡ºã€‚} \\ \ \\
z_i \text{ æ˜¯æ¯ä¸ªç±»åˆ«çš„è¾“å…¥å€¼ã€‚} \\
\sum_{j=1}^n e^{z_j} \text{ æ˜¯æ‰€æœ‰ç±»åˆ«è¾“å…¥å€¼çš„æŒ‡æ•°å’Œï¼Œç”¨äºå½’ä¸€åŒ–ã€‚}


$$

<img title="" src="./images/4daef102-6e01-4e1d-81f8-6946a5d998f1.png" alt="4daef102-6e01-4e1d-81f8-6946a5d998f1" style="zoom:33%;" data-align="center">

#### Training NNs ç¥ç»ç½‘ç»œè®­ç»ƒ

- The network **parameters ğœƒ** include the **weight matrices** and **bias vectors** from all layers  
  ç½‘ç»œ **å‚æ•° Î¸** åŒ…æ‹¬æ¥è‡ªæ‰€æœ‰å±‚çš„ **æƒçŸ©é˜µ** å’Œ **åå‘å‘é‡** 
  
  $$
  \theta = \{ W^1, b^1, W^2, b^2, \dots, W^L, b^L \}

  $$
  
  Often, the model parameters ğœƒ are referred to as weights  
  é€šå¸¸ï¼Œæ¨¡å‹å‚æ•° Î¸ è¢«ç§°ä¸ºæƒé‡

- Training a model **to learn a set of parameters ğœƒ that are optimal** (according to a criterion) is one of the greatest challenges in ML  
  è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å­¦ä¹ ä¸€ç»„æœ€ä¼˜çš„å‚æ•° Î¸ (æ ¹æ®ä¸€ä¸ªæ ‡å‡†)æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¤§çš„æŒ‘æˆ˜ä¹‹ä¸€
  
  

- **Data preprocessing** - helps **convergence** during training   
  æ•°æ®é¢„å¤„ç†-æœ‰åŠ©äºåœ¨è®­ç»ƒæœŸé—´æ”¶æ•›
  
  - **Mean subtraction**, to obtain zero-centered data   
    å¹³å‡å‡æ³•ï¼Œå¾—åˆ°é›¶ä¸­å¿ƒçš„æ•°æ® 
    
    - Subtract the mean for each individual data dimension (feature)  
      å‡å»æ¯ä¸ªç‹¬ç«‹æ•°æ®ç»´åº¦(ç‰¹æ€§)çš„å¹³å‡å€¼
  
  - **Normalization**  è§„èŒƒåŒ–
    
    - Divide each feature by its standard deviation  
      å°†æ¯ä¸ªç‰¹å¾æŒ‰å…¶æ ‡å‡†å·®åˆ’åˆ†
      
      - To obtain standard deviation of 1 for each data dimension (feature)  
        ä¸ºæ¯ä¸ªæ•°æ®ç»´åº¦(ç‰¹å¾)å–å¾—1çš„æ ‡å‡†å·®
    
    - Or, scale the data within the range [0,1] or [-1, 1]  
      æˆ–è€…ï¼Œåœ¨[0,1]æˆ–[-1,1]èŒƒå›´å†…ç¼©æ”¾æ•°æ®
  
  - <img title="" src="./images/77f7decc-38df-43c9-9083-9a20e177285a.png" alt="77f7decc-38df-43c9-9083-9a20e177285a" style="zoom:50%;">

- To train a NN, set the parameters ğœƒ such that for a training subset of images, the corresponding elements in the predicted output have maximum values  
  ä¸ºäº†è®­ç»ƒç¥ç»ç½‘ç»œï¼Œéœ€è¦è®¾ç½®å‚æ•° Î¸ï¼Œä½¿å¾—å¯¹äºå›¾åƒçš„è®­ç»ƒå­é›†ï¼Œé¢„æµ‹è¾“å‡ºä¸­çš„ç›¸åº”å…ƒç´ å…·æœ‰æœ€å¤§å€¼

- Define a **loss function/objective function/cost function** $\mathcal{L}(\theta)$ that calculates the difference (error) between the model prediction and the true label  
  å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•°/æˆæœ¬å‡½æ•° $\mathcal{L}(\theta)$ï¼Œç”¨äºè®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚(è¯¯å·®)

- Find the optimal parameters $\theta^*$ that minimize the total loss $\mathcal{L}(\theta)$   
  å¯»æ‰¾æœ€å°åŒ–æ€»æŸå¤± $\mathcal{L}(\theta)$ çš„æœ€ä½³å‚æ•° $\theta ^ * $ 
  
  > For a training set of ğ‘ images, calculate the total loss overall all images:  
  > å¯¹äº N å¹…å›¾åƒçš„è®­ç»ƒé›†ï¼Œè®¡ç®—æ‰€æœ‰å›¾åƒçš„æ€»æŸå¤±:
  > 
  > $$
  > \mathcal{L}(\theta) = \sum_{n=1}^N \mathcal{L}_n(\theta)
  > $$

##### Loss Functions  æŸå¤±å‡½æ•°

###### Classification tasks åˆ†ç±»ä»»åŠ¡

- Training examples  è®­ç»ƒæ ·æœ¬
  
  $$
  \text{Pairs of } N \text{ inputs } x_i \text{ and ground-truth class labels } y_i
\\ 
N \text{ ä¸ªè¾“å…¥ } x_i \text{ ä¸çœŸå®ç±»åˆ«æ ‡ç­¾ } y_i \text{ çš„é…å¯¹}
  $$

- Output Layer è¾“å‡ºå±‚
  Softmax Activations [maps to a probability distribution]  
  Softmax æ¿€æ´»[æ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ]
  
  $$
  P(y = j \mid \mathbf{x}) = \frac{e^{\mathbf{x}^\top \mathbf{w}_j}}{\sum_{k=1}^K e^{\mathbf{x}^\top \mathbf{w}_k}}

  $$

- Loss function æŸå¤±å‡½æ•°
  Cross-entropy  äº¤å‰ç†µ
  
  $$
  \mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K \left[ y_k^{(i)} \log \hat{y}_k^{(i)} + \left( 1 - y_k^{(i)} \right) \log \left( 1 - \hat{y}_k^{(i)} \right) \right]
\\
\text{Ground-truth class labels ï¼ˆå®é™…å€¼ï¼‰ } y_i \text{ and model predicted class labels ï¼ˆæ¨¡å‹é¢„æµ‹å€¼ï¼‰ } \hat{y}_i

  $$

###### Regression tasks  å›å½’ä»»åŠ¡

- Training examples  è®­ç»ƒæ ·æœ¬
  
  $$
  \text{Pairs of } N \text{ inputs } x_i \text{ and ground-truth output values } y_i
\\
N \text{ ä¸ªè¾“å…¥ } x_i \text{ ä¸çœŸå®è¾“å‡ºå€¼ } y_i \text{ çš„å¯¹}

  $$

- Output Layer è¾“å‡ºå±‚
  
  $$
  \text{Linear (Identity) or Sigmoid Activation}
\\
\text{çº¿æ€§æˆ–è€…sigmoidæ¿€æ´»å‡½æ•°}
  $$

- Loss function æŸå¤±å‡½æ•°
  
  - Mean Squared Error  å‡æ–¹è¯¯å·®
    
    $$
    \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \left( y^{(i)} - \hat{y}^{(i)} \right)^2
    $$
  
  - Mean Absolute Error  å¹³å‡ç»å¯¹è¯¯å·®
    
    $$
    \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \left| y^{(i)} - \hat{y}^{(i)} \right|
    $$

#### Training NNs (2)

- Optimizing the loss function $\mathcal{L}(\theta)$  ä¼˜åŒ–æŸå¤±å‡½æ•°
  
  - **gradient descent (GD)** algorithm  
    æ¢¯åº¦ä¸‹é™æ³•ç®—æ³•
    
    - GD applies iterative refinement of the network parameters $\theta$   
      GD å¯¹ç½‘ç»œå‚æ•°$\theta$è¿›è¡Œè¿­ä»£æ±‚ç²¾
    
    - GD uses the opposite direction of the **gradient** of the loss with respect to the NN parameters for updating $\theta$ ($\nabla \mathcal{L}(\theta) = \left[ \frac{\partial \mathcal{L}}{\partial \theta_i} \right]$) 
      GD ä½¿ç”¨ç›¸å¯¹äºç¥ç»ç½‘ç»œå‚æ•°çš„æŸå¤±æ¢¯åº¦çš„ç›¸åæ–¹å‘æ¥æ›´æ–° $\theta $ 
    
    - The gradient of the loss function $\nabla \mathcal{L}(\theta)$ gives the direction of fastest increase of the loss function $\mathcal{L}(\theta)$ when the parameters $\theta$ are changed  
      æŸå¤±å‡½æ•° $\nabla \mathcal { L }(\theta) $çš„æ¢¯åº¦ç»™å‡ºäº†å½“å‚æ•° $\theta $æ”¹å˜æ—¶æŸå¤±å‡½æ•° $\mathcal{L}(\theta ) $å¢é•¿æœ€å¿«çš„æ–¹å‘
      
      <img title="" src="./images/746b78ac-0896-4c21-9762-ae45ba077ed2.png" alt="746b78ac-0896-4c21-9762-ae45ba077ed2" style="zoom:33%;">

#### Gradient descent  æ¢¯åº¦ä¸‹é™

- Steps in the gradient descent algorithm:  
  æ¢¯åº¦ä¸‹é™æ³•ç®—æ³•ä¸­çš„æ­¥éª¤:
  
  <img title="" src="./images/Gradient Descent Process Flowchart (English).png" alt="loading-ag-61851" style="zoom:100%;">
  
  <img title="" src="./images/æ¢¯åº¦ä¸‹é™æµç¨‹å›¾ (Chinese).png" alt="loading-ag-61853" data-align="inline" style="zoom:100%;">

- å…³é”®ç®—æ³•ï¼š
  
  $$
  \theta^{\text{new}} = \theta^0 - \alpha \nabla \mathcal{L}(\theta^0)
  $$

- å›¾è§£ï¼š
  
  <img src="./images/87db027f-655c-464e-a11b-99a78aad1fdd.png" title="" alt="87db027f-655c-464e-a11b-99a78aad1fdd" style="zoom:50%;">

- Gradient Descent Algorithm  æ¢¯åº¦ä¸‹é™ç®—æ³•
  
  <img title="" src="./images/2a571465-3c6a-4ec0-9698-abb760eee3a4.png" alt="2a571465-3c6a-4ec0-9698-abb760eee3a4" style="zoom:33%;" data-align="inline">
  
  <img title="" src="./images/3a609cf7-02b5-4101-b11b-025b05489f9e.png" alt="3a609cf7-02b5-4101-b11b-025b05489f9e" style="zoom:33%;">
  
  - Gradient descent algorithm stops when a local minimum of the loss surface is reached  
    å½“æŸè€—é¢è¾¾åˆ°å±€éƒ¨æœ€å°æ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ³•ç®—æ³•åœæ­¢
    
    - GD does not guarantee reaching a global minimum   
      GD å¹¶ä¸èƒ½ä¿è¯è¾¾åˆ°å…¨çƒæœ€ä½æ°´å¹³
    
    - However, empirical evidence suggests that GD works well for NNs  
      ä½†æ˜¯ï¼Œç»éªŒè¯æ®ä½“ç°å‡ºæ¢¯åº¦ä¸‹é™å¯¹äºç¥ç»ç½‘ç»œå…·æœ‰è‰¯å¥½çš„æ•ˆæœ
    
    <img title="" src="./images/48fc59ea-b99a-4d74-8876-35b67588b16a.png" alt="48fc59ea-b99a-4d74-8876-35b67588b16a" style="zoom:67%;">
  
  - Random initialization in NNs results in different initial parameters $\theta^0$ every time the NN is trained  
    ç¥ç»ç½‘ç»œä¸­çš„éšæœºåˆå§‹åŒ–åœ¨æ¯æ¬¡è®­ç»ƒç¥ç»ç½‘ç»œæ—¶éƒ½ä¼šäº§ç”Ÿä¸åŒçš„åˆå§‹å‚æ•° $\theta ^ 0 $ 
    
    - Gradient descent may reach different minima at every run â–ª  
      æ¯æ¬¡è¿è¡Œæ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ³•å¯èƒ½è¾¾åˆ°ä¸åŒçš„æœ€å°å€¼ã€‚
    
    - Therefore, NN will produce different predicted outputs  
      å› æ­¤ï¼Œç¥ç»ç½‘ç»œå°†äº§ç”Ÿä¸åŒçš„é¢„æµ‹è¾“å‡º
  
  - currently we donâ€™t have algorithms that guarantee reaching a global minimum for an arbitrary loss function.  
    ç›®å‰æˆ‘ä»¬è¿˜æ²¡æœ‰ç®—æ³•å¯ä»¥ä¿è¯è¾¾åˆ°ä»»æ„æŸå¤±å‡½æ•°çš„å…¨å±€æœ€å°å€¼ã€‚

##### Backpropagation (â€œbackward propagationâ€) åå‘ä¼ æ’­

- Modern NNs employ the backpropagation method for calculating the gradients of the loss function $\nabla \mathcal{L}(\theta) =  \frac{\partial \mathcal{L}}{\partial \theta_i} $

- For training NNs, forward propagation (forward pass) refers to passing the inputs ğ‘¥ through the hidden layers to obtain the model outputs (predictions)   
  å¯¹äºè®­ç»ƒç¥ç»ç½‘ç»œï¼Œå‰å‘ä¼ æ’­(å‰å‘ä¼ é€’)æ˜¯æŒ‡å°†è¾“å…¥ x é€šè¿‡éšè—å±‚ä»¥è·å¾—æ¨¡å‹è¾“å‡º(é¢„æµ‹)
  
  - The loss $\mathcal{L}(y, \hat{y})$ function is then calculated  
    ç„¶åè®¡ç®—äºæŸå‡½æ•°
  
  - Backpropagation traverses the network in reverse order, from the outputs ğ‘¦ backward toward the inputs ğ‘¥ to calculate the gradients of the loss $\nabla \mathcal{L} (\theta)$   
    åå‘ä¼ æ’­ä»¥ç›¸åçš„é¡ºåºéå†ç½‘ç»œï¼Œä»è¾“å‡º y å‘ååˆ°è¾“å…¥ x è®¡ç®—æŸå¤±çš„æ¢¯åº¦
  
  - The chain rule is used for calculating the partial derivatives of the loss function with respect to the parameters ğœƒ in the different layers in the network  
    åˆ©ç”¨é“¾å¼è§„åˆ™è®¡ç®—ç½‘ç»œå„å±‚æŸå¤±å‡½æ•°å¯¹å‚æ•° Î¸ çš„åå¯¼æ•°

- Each update of the model parameters ğœƒ during training takes one forward and one backward pass (e.g., of a batch of inputs)  
  åœ¨è®­ç»ƒæœŸé—´å¯¹æ¨¡å‹å‚æ•° Î¸ çš„æ¯æ¬¡æ›´æ–°éƒ½éœ€è¦ä¸€æ¬¡å‘å‰å’Œä¸€æ¬¡å‘åä¼ é€’(ä¾‹å¦‚ï¼Œä¸€æ‰¹è¾“å…¥)

- Automatic calculation of the gradients (automatic differentiation) is available in all current deep learning libraries  
  ç›®å‰æ‰€æœ‰çš„æ·±åº¦å­¦ä¹ åº“éƒ½å¯ä»¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦(è‡ªåŠ¨å¾®åˆ†) 
  
  - It significantly simplifies the implementation of deep learning algorithms, since it obviates deriving the partial derivatives of the loss function by hand  
    è¯¥æ–¹æ³•é¿å…äº†æ‰‹å·¥æ±‚æŸå¤±å‡½æ•°çš„åå¯¼æ•°ï¼Œå¤§å¤§ç®€åŒ–äº†æ·±åº¦å­¦ä¹ ç®—æ³•çš„å®ç°

##### Mini-batch Gradient Descent è¿·ä½ æ¢¯åº¦ä¸‹é™æ³•

- It is wasteful to compute the loss over the entire training dataset to perform a single parameter update for large datasets  
  ä¸ºäº†å¯¹å¤§å‹æ•°æ®é›†æ‰§è¡Œå•ä¸ªå‚æ•°æ›´æ–°ï¼Œè®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šçš„æŸå¤±æ˜¯å¾ˆæµªè´¹çš„
  ï¼ˆGDé€šå¸¸è¢«mini-batch GDå–ä»£ï¼‰

- Mini-batch gradient descent  è¿·ä½ æ¢¯åº¦ä¸‹é™æ³•
  
  - Approach: 
    
    - Compute the loss $\mathcal{L} (\theta)$ on a mini-batch of images, update the parameters $\theta $, and repeat until all images are used   
      åœ¨ä¸€å°æ‰¹å›¾åƒä¸Šè®¡ç®—ä¸¢å¤± $\mathcal { L }(\theta) $ï¼Œæ›´æ–°å‚æ•° $\theta $ï¼Œç„¶åé‡å¤ï¼Œç›´åˆ°ä½¿ç”¨æ‰€æœ‰å›¾åƒ
    
    - At the next epoch, shuffle the training data, and repeat the above process  
      åœ¨ä¸‹ä¸€ä¸ªæ—¶æœŸï¼Œé‡ç»„è®­ç»ƒæ•°æ®ï¼Œå¹¶é‡å¤ä¸Šè¿°è¿‡ç¨‹
  
  - Mini-batch GD results in much faster training  
    å°æ‰¹é‡ GD å¯¼è‡´æ›´å¿«çš„è®­ç»ƒ
  
  - Typical mini-batch size: 32 to 256 images  
    å…¸å‹çš„å°æ‰¹é‡: 32è‡³256å¼ å›¾åƒ
  
  - It works because the gradient from a mini-batch is a good approximation of the gradient from the entire training set  
    å®ƒä¹‹æ‰€ä»¥æœ‰æ•ˆæ˜¯å› ä¸ºæ¥è‡ªä¸€ä¸ªå°æ‰¹é‡çš„æ¢¯åº¦æ˜¯æ¥è‡ªæ•´ä¸ªè®­ç»ƒé›†çš„æ¢¯åº¦çš„ä¸€ä¸ªå¾ˆå¥½çš„è¿‘ä¼¼å€¼

##### Stochastic Gradient Descent (SGD) éšæœºæ¢¯åº¦ä¸‹é™

- SGD uses mini-batches that consist of a single input example  
  SGD ä½¿ç”¨ç”±å•ä¸ªè¾“å…¥ç¤ºä¾‹ç»„æˆçš„è¿·ä½ æ‰¹å¤„ç†

- Although this method is very fast, it may cause significant fluctuations in the loss function  
  è™½ç„¶è¿™ç§æ–¹æ³•å¾ˆå¿«ï¼Œä½†å¯èƒ½ä¼šé€ æˆæŸå¤±å‡½æ•°çš„æ˜¾è‘—æ³¢åŠ¨
  
  - Therefore, it is less commonly used, and mini-batch GD is preferred  
    å› æ­¤ï¼Œå®ƒè¾ƒå°‘è¢«ä½¿ç”¨ï¼Œè€Œå°æ‰¹é‡çš„ GD æ˜¯é¦–é€‰

- In most DL libraries, SGD typically means a mini-batch GD (with an option to add  momentum)  
  åœ¨å¤§å¤šæ•°æ·±åº¦å­¦ä¹ åº“ä¸­ï¼ŒSGD é€šå¸¸æ„å‘³ç€ä¸€ä¸ªå°æ‰¹é‡çš„è¿·ä½ æ¢¯åº¦ä¸‹é™(å¯ä»¥é€‰æ‹©å¢åŠ åŠ¨åŠ›)

##### Problems with Gradient Descent  æ¢¯åº¦ä¸‹é™æ³•çš„é—®é¢˜

Besides the local minima problem, the GD algorithm can be very slow at plateaus, and it can get stuck at saddle points  
é™¤äº†å±€éƒ¨æå°é—®é¢˜å¤–ï¼ŒGD ç®—æ³•åœ¨é«˜åŸæ—¶é€Ÿåº¦å¾ˆæ…¢ï¼Œå¹¶ä¸”åœ¨éç‚¹å¤„ä¼šå¡ä½

<img title="" src="./images/74915f00-5f84-4dc5-bcc9-957b10ddb391.png" alt="74915f00-5f84-4dc5-bcc9-957b10ddb391" style="zoom:33%;">

##### Gradient Descent with Momentum  åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•

Gradient descent with momentum uses the momentum of the gradient for parameter optimization  
åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•åˆ©ç”¨æ¢¯åº¦çš„åŠ¨é‡è¿›è¡Œå‚æ•°ä¼˜åŒ–  

$$
Movement = Negative \ of \ Gradient + Momentum
$$

<img title="" src="./images/f067d710-fcef-45a1-b9fd-7d2f1a6f29f6.png" alt="f067d710-fcef-45a1-b9fd-7d2f1a6f29f6" style="zoom:33%;" data-align="center">

- Parameters update in GD with momentum at iteration  
  åŸºäºåŠ¨é‡è¿­ä»£çš„ GD å‚æ•°æ›´æ–°
  
  $$
  \theta^t = \theta^{t-1} - V^t
\\
V^t = \beta V^{t-1} + \alpha \nabla \mathcal{L}(\theta^{t-1})
\\
\text{i.e., } \theta^t = \theta^{t-1} - \alpha \nabla \mathcal{L}(\theta^{t-1}) - \beta V^{t-1}

  $$

- The parameter ğ›½ is referred to as a coefficient of momentum   
  å‚æ•° Î² è¢«ç§°ä¸ºåŠ¨é‡ç³»æ•°
  
  - A typical value of the parameter ğ›½ is 0.9  
    å‚æ•° Î² çš„å…¸å‹å€¼ä¸º0.9

- This method updates the parameters ğœƒ in the direction of the weighted average of the past gradients  
  è¿™ç§æ–¹æ³•æ²¿ç€è¿‡å»æ¢¯åº¦åŠ æƒå¹³å‡æ•°çš„æ–¹å‘æ›´æ–°å‚æ•° Î¸

##### Adaptive Moment Estimation (Adam) è‡ªé€‚åº”çŸ©ä¼°è®¡

Adam combines insights from the momentum optimizers that accumulate the values of past gradients, and it also introduces new terms based on the second moment of the gradient  
Adam ç»“åˆäº†ç§¯ç´¯è¿‡å»æ¢¯åº¦å€¼çš„åŠ¨é‡ä¼˜åŒ–å™¨çš„è§è§£ï¼Œå¹¶ä¸”è¿˜å¼•å…¥äº†åŸºäºæ¢¯åº¦ç¬¬äºŒé˜¶æ®µçš„æ–°æœ¯è¯­

- Similar to GD with momentum, Adam computes a weighted average of past gradients (first moment of the gradient)  
  ä¸åŠ¨é‡çš„ GD ç±»ä¼¼ï¼Œäºšå½“è®¡ç®—äº†è¿‡å»æ¢¯åº¦çš„åŠ æƒå¹³å‡æ•°(æ¢¯åº¦çš„ç¬¬ä¸€ä¸ªæ—¶åˆ»)

- Adam also computes a weighted average of past squared gradients (second moment of the gradient)  
  äºšå½“è¿˜è®¡ç®—äº†è¿‡å»å¹³æ–¹æ¢¯åº¦çš„åŠ æƒå¹³å‡æ•°(æ¢¯åº¦çš„ç¬¬äºŒä¸ªæ—¶åˆ»)

$$
\theta^t = \theta^{t-1} - \alpha \frac{\hat{V}^t}{\sqrt{\hat{U}^t + \epsilon}}
\\
\text{Where: } \hat{V}^t = \frac{V^t}{1 - \beta_1} \text{ and } \hat{U}^t = \frac{U^t}{1 - \beta_2}
\\ \ \\
\text{The proposed default values are ï¼ˆå»ºè®®çš„é»˜è®¤å€¼ä¸ºï¼‰ } \\ \beta_1 = 0.9, \beta_2 = 0.999, \text{ and } \epsilon = 10^{-8}


$$

- Other commonly used optimization methods include:   
  å…¶ä»–å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬:
  
  - Adagrad, Adadelta, RMSprop, Nadam, etc.
  
  - Most commonly used optimizers nowadays are Adam and SGD with momentum.   
    ç›®å‰æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ Adam å’Œå¸¦åŠ¨é‡çš„ SGD

##### Optimizer ä¼˜åŒ–å™¨

- How to update the weights based on the loss function ï° 
  å¦‚ä½•æ ¹æ®æŸå¤±å‡½æ•°æ›´æ–°æƒé‡

- Learning rate (+scheduling) ï° 
  å­¦ä¹ ç‡(+ è¿›åº¦)

- Stochastic gradient descent, momentum, and their variants  
  éšæœºæ¢¯åº¦ä¸‹é™ã€åŠ¨é‡åŠå…¶å˜ä½“
  
  - RMSProp is usually a good first choice   
    RMSProp é€šå¸¸æ˜¯ä¸é”™çš„é¦–é€‰

---

<img title="" src="./images/cfa3c9c6-a92c-4057-abfa-5a92a48e5892.png" alt="cfa3c9c6-a92c-4057-abfa-5a92a48e5892" style="zoom:50%;">

è¿™å¼ å›¾æ˜¾ç¤ºäº†ä¸åŒä¼˜åŒ–ç®—æ³•åœ¨ä¸€ä¸ªå…¸å‹çš„å‡¸å‡½æ•°ä¸­çš„ä¼˜åŒ–è·¯å¾„ã€‚å›¾ä¸­çš„äº”è§’æ˜Ÿé€šå¸¸è¡¨ç¤ºç›®æ ‡æœ€å°å€¼ç‚¹ï¼Œè€Œé»‘ç‚¹æ˜¯ä¼˜åŒ–çš„èµ·å§‹ç‚¹ã€‚å½©è‰²çš„æ›²çº¿å›¾ç¤ºäº†ä¸åŒç®—æ³•åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è½¨è¿¹ã€‚

å›¾ä¾‹ä¸­çš„æ ‡è¯†ç¬¦è¡¨ç¤ºæ¯ç§ä¼˜åŒ–ç®—æ³•ï¼š

1. **SGD (Stochastic Gradient Descent)**ï¼šçº¢è‰²çº¿æ¡ï¼Œæ˜¾ç¤ºäº†åŸºæœ¬çš„æ¢¯åº¦ä¸‹é™è·¯å¾„ã€‚
2. **Momentum**ï¼šç»¿è‰²çº¿æ¡ï¼ŒåŠ å…¥äº†åŠ¨é‡é¡¹ï¼Œä½¿è·¯å¾„å¹³æ»‘ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚
3. **NAG (Nesterov Accelerated Gradient)**ï¼šç´«è‰²çº¿æ¡ï¼Œæ”¹è¿›äº†åŠ¨é‡æ³•ï¼Œé€šè¿‡åœ¨é¢„æœŸæ–¹å‘ä¸Šè¿›è¡Œæ¢¯åº¦è®¡ç®—æ¥åŠ é€Ÿæ”¶æ•›ã€‚
4. **Adagrad**ï¼šè“è‰²çº¿æ¡ï¼Œè‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ï¼Œæ›´é€‚åˆå¤„ç†ç¨€ç–æ•°æ®ã€‚
5. **Adadelta**ï¼šé»„è‰²çº¿æ¡ï¼Œè§£å†³äº†Adagradå­¦ä¹ ç‡è¡°å‡è¿‡å¿«çš„é—®é¢˜ã€‚
6. **RMSprop**ï¼šé»‘è‰²çº¿æ¡ï¼Œé€šè¿‡æŒ‡æ•°åŠ æƒå¹³å‡æ¥è°ƒæ•´å­¦ä¹ ç‡ï¼Œé¿å…äº†è¿‡å¿«çš„å­¦ä¹ ç‡è¡°å‡ã€‚

è¿™å¼ å›¾å¯ä»¥å¸®åŠ©å¯¹æ¯”ä¸åŒä¼˜åŒ–ç®—æ³•çš„ç‰¹æ€§ï¼Œä¾‹å¦‚å®ƒä»¬çš„æ”¶æ•›é€Ÿåº¦å’Œè·¯å¾„å¹³æ»‘ç¨‹åº¦ã€‚è¿™åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­æœ‰åŠ©äºé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨ã€‚

---

##### Learning Rate  å­¦ä¹ ç‡

- The gradient tells us the direction in which the loss has the steepest rate of increase, but it does not tell us how far along the opposite direction we should step  
  æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬æŸå¤±å¢é•¿é€Ÿåº¦æœ€å¿«çš„æ–¹å‘ï¼Œä½†å®ƒå¹¶ä¸å‘Šè¯‰æˆ‘ä»¬åº”è¯¥æ²¿ç€ç›¸åçš„æ–¹å‘èµ°å¤šè¿œ

- Choosing the learning rate (also called the **step size**) is one of the most important hyper-parameter settings for NN training    
  é€‰æ‹©å­¦ä¹ ç‡(ä¹Ÿç§°ä¸ºæ­¥é•¿)æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­æœ€é‡è¦çš„è¶…å‚æ•°è®¾ç½®ä¹‹ä¸€

<img title="" src="./images/dd6b8d7e-a879-46e5-b392-2358a6883872.png" alt="dd6b8d7e-a879-46e5-b392-2358a6883872" style="zoom:50%;">

- Training loss for different learning rates 
  
  - High learning rate: the loss increases or plateaus too quickly  
    é«˜å­¦ä¹ ç‡: æŸå¤±å¢åŠ æˆ–åœæ»è¿‡å¿«
  
  - Low learning rate: the loss decreases too slowly (takes many epochs to reach a solution)  
    å­¦ä¹ ç‡ä½: æŸå¤±é™ä½å¾—å¤ªæ…¢(éœ€è¦å¾ˆå¤šæ—¶æœŸæ‰èƒ½è¾¾åˆ°è§£å†³æ–¹æ¡ˆ)

<img title="" src="./images/9de91125-453f-4062-87a8-da05ed688ed1.png" alt="9de91125-453f-4062-87a8-da05ed688ed1" style="zoom:33%;" data-align="center">

##### Vanishing Gradient Problem æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

- In some cases, during training, the gradients can become either very small (vanishing gradients) of very large (exploding gradients) 
  åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œæ¢¯åº¦å¯ä»¥å˜å¾—éå¸¸å°(æ¶ˆå¤±çš„æ¢¯åº¦)éå¸¸å¤§(çˆ†ç‚¸æ¢¯åº¦)
  
  - They result in very small or very large update of the parameters   
    å®ƒä»¬ä¼šå¯¼è‡´éå¸¸å°æˆ–éå¸¸å¤§çš„å‚æ•°æ›´æ–°
  
  - Solutions: change learning rate, ReLU activations, regularization, LSTM units in RNNs  
    è§£å†³æ–¹æ¡ˆ: æ”¹å˜å­¦ä¹ é€Ÿç‡ï¼ŒReLU æ¿€æ´»ï¼Œæ­£åˆ™åŒ–ï¼Œåœ¨ RNN ä¸­çš„ LSTM å•ä½

##### Generalization

- Underfitting æ¬ æ‹Ÿåˆ
  
  - The model is too â€œsimpleâ€ to represent all the relevant class characteristics   
    æ¨¡å‹è¿‡äºâ€œç®€å•â€ï¼Œä¸èƒ½ä»£è¡¨æ‰€æœ‰ç›¸å…³çš„ç±»åˆ«ç‰¹å¾
  
  - Produces high error on the training set and high error on the validation set  
    åœ¨è®­ç»ƒé›†ä¸Šäº§ç”Ÿé«˜è¯¯å·®ï¼Œåœ¨éªŒè¯é›†ä¸Šäº§ç”Ÿé«˜è¯¯å·®
  
  <img src="./images/e86eff30-1411-47f1-bee7-3ae6f040faf2.png" title="" alt="e86eff30-1411-47f1-bee7-3ae6f040faf2" style="zoom:33%;">

- Overfitting è¿‡æ‹Ÿåˆ
  
  - The model is too â€œcomplexâ€ and fits irrelevant characteristics (noise) in the data    
    è¯¥æ¨¡å‹è¿‡äºâ€œå¤æ‚â€ï¼Œé€‚åˆæ•°æ®ä¸­ä¸ç›¸å…³çš„ç‰¹å¾(å™ªå£°)
  - Produces low error on the training error and high error on the validation set  
    åœ¨è®­ç»ƒé”™è¯¯ä¸Šäº§ç”Ÿè¾ƒä½çš„é”™è¯¯ï¼Œåœ¨éªŒè¯é›†ä¸Šäº§ç”Ÿè¾ƒé«˜çš„é”™è¯¯
  
  <img title="" src="./images/58fd5e5b-8abd-4536-8311-ef0f93ed1121.png" alt="58fd5e5b-8abd-4536-8311-ef0f93ed1121" style="zoom:33%;">

#### Regularization methodsÂ æ­£åˆ™åŒ–æ–¹æ³•

##### Overfitting è¿‡æ‹Ÿåˆ

- Overfitting â€“ a model with high capacity fits the noise in the data instead of the underlying relationship  
  è¿‡åº¦æ‹Ÿåˆ-ä¸€ä¸ªé«˜å®¹é‡çš„æ¨¡å‹æ‹Ÿåˆæ•°æ®ä¸­çš„å™ªå£°ï¼Œè€Œä¸æ˜¯æ½œåœ¨çš„å…³ç³»
  
  <img src="./images/ad181e6d-1551-4324-ba3e-83c623012693.png" title="" alt="ad181e6d-1551-4324-ba3e-83c623012693" style="zoom:33%;">
  
  The model may fit the training data very  well, but fails to generalize to new  examples (test or validation data)  
  è¯¥æ¨¡å‹å¯èƒ½éå¸¸é€‚åˆè®­ç»ƒæ•°æ®ï¼Œä½†ä¸èƒ½æ¨å¹¿åˆ°æ–°çš„ä¾‹å­(æµ‹è¯•æˆ–éªŒè¯æ•°æ®)

##### Regularization: Early Stopping  æ­£è§„åŒ–: æå‰åœæ­¢

- During model training, use **a validation set**  
  åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´ï¼Œä½¿ç”¨éªŒè¯é›† 

- Stop when the validation accuracy (or loss) has not improved after n epochs   
  å½“éªŒè¯ç²¾åº¦(æˆ–æŸå¤±)åœ¨ n ä¸ªçºªå…ƒåæ²¡æœ‰æ”¹å–„æ—¶åœæ­¢
  
  - The parameter n is called **patience**  
    å‚æ•° n å«åšè€å¿ƒ

<img src="./images/eb518d07-9024-479a-a7e7-5d4286fb7580.png" title="" alt="eb518d07-9024-479a-a7e7-5d4286fb7580" style="zoom:33%;">

##### Regularization: Weight Decay æ­£åˆ™åŒ–: æƒé‡è¡°å‡

- $\mathcal{l}_2$ weight decay
   A regularization term that penalizes large weights is added to the loss function  
  åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æƒ©ç½šå¤§æƒé‡çš„æ­£åˆ™åŒ–é¡¹
  
  $$
  \mathcal{L}_{\text{reg}}(\theta) = \mathcal{L}(\theta) + \lambda \sum_k \theta_k^2
\\ \ \\ 
\text{Data loss} + \text{Regularization loss}
  $$
  
  - For every weight in the network, we add the regularization term to the loss value  
    å¯¹äºç½‘ç»œä¸­çš„æ¯ä¸ªæƒé‡ï¼Œæˆ‘ä»¬å°†æ­£åˆ™é¡¹åŠ åˆ°æŸå¤±å€¼ä¸­
    
    - During gradient descent parameter update, every weight is decayed linearly toward zero   
      åœ¨æ¢¯åº¦ä¸‹é™æ³•å‚æ•°æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæƒé‡éƒ½å‘ˆçº¿æ€§è¡°å‡è¶‹äºé›¶
  
  - The **weight decay coefficient ğœ†** determines how dominant the regularization is during the gradient computation  
    æƒè¡°å‡ç³»æ•° Î» å†³å®šäº†åœ¨æ¢¯åº¦è®¡ç®—è¿‡ç¨‹ä¸­æ­£åˆ™åŒ–çš„ä¼˜åŠ¿ç¨‹åº¦
    
    - Large weight decay coefficient â†’ penalty for weights with large values  
      å¤§æƒé‡è¡°å‡ç³»æ•°â†’å¤§å€¼æƒé‡çš„æƒ©ç½š

- $\mathcal{l}_1$ weight decay
  The regularization term is based on the $\mathcal{l}_1$ norm of the weights  
  æ­£åˆ™åŒ–é¡¹åŸºäºæƒé‡çš„ $\mathcal{ l } _ 1 $èŒƒæ•°
  
  $$
  \mathcal{L}_{\text{reg}}(\theta) = \mathcal{L}(\theta) + \lambda \sum_k |\theta_k|
  $$
  
  - $\mathcal{ l } _ 1 $ weight decay is less common with NN   
    è¿™ä¸€ç§æ–¹æ³•ä¸å¸¸è§
    
    - Often performs worse than $\mathcal{ l } _ 2 $ weight decay   
      é€šå¸¸æ¯”ä¸Šä¸€ç§æ–¹æ³•æ•ˆæœä¸å¥½
  
  - It is also possible to combine $\mathcal{ l } _ 1 $ and $\mathcal{ l } _ 2 $ regularization   
    ä¸¤ç§æ–¹æ³•å¯ä»¥æ··åˆä½¿ç”¨
    
    - Called **elastic net regularization**  
      ç§°ä¸ºå¼¹æ€§ç½‘æ­£åˆ™åŒ–
    
    - $$
      \mathcal{L}_{\text{reg}}(\theta) = \mathcal{L}(\theta) + \lambda_1 \sum_k |\theta_k| + \lambda_2 \sum_k \theta_k^2
      $$

##### Regularization: Dropout

- Randomly drop units (along with their connections) during training   
  åœ¨è®­ç»ƒæœŸé—´éšæœºæ”¾å¼ƒå•ä½(è¿åŒä»–ä»¬çš„è¿æ¥)

- Each unit is retained with a fixed **dropout rate p**, independent of other units   
  æ¯ä¸ªå•å…ƒä¿ç•™ä¸€ä¸ªå›ºå®šçš„dropoutæ¦‚ç‡ç‹¬ç«‹äºå…¶å®ƒå•å…ƒ

- The hyper-parameter p needs to be chosen (tuned)   
  éœ€è¦é€‰æ‹©(è°ƒæ•´)è¶…å‚æ•° p
  
  - Often, between 20% and 50% of the units are dropped  
    é€šå¸¸ï¼Œ20% è‡³50% çš„å•ä½è¢«ä¸¢å¼ƒ
  
  <img src="./images/86eba97e-c55c-45e2-a095-f9229b0ba627.png" title="" alt="86eba97e-c55c-45e2-a095-f9229b0ba627" style="zoom:33%;">

- Dropout is a kind of ensemble learning   
  dropoutæ˜¯ä¸€ç§é›†æˆå­¦ä¹ 
  
  - Using one mini-batch to train one network with a slightly different architecture  
    ä½¿ç”¨ä¸€ä¸ªå°æ‰¹é‡åŸ¹è®­ä¸€ä¸ªæ¶æ„ç•¥æœ‰ä¸åŒçš„ç½‘ç»œ
    
    <img src="./images/a9c6113b-b384-4e24-8369-8fc2ff745597.png" title="" alt="a9c6113b-b384-4e24-8369-8fc2ff745597" style="zoom:33%;">

#### NN architectures ç¥ç»ç½‘ç»œä½“ç³»ç»“æ„

##### Batch Normalization æ‰¹å½’ä¸€åŒ–

- Batch normalization layers act similar to the data preprocessing steps mentioned earlier   
  æ‰¹é‡æ ‡å‡†åŒ–å±‚çš„ä½œç”¨ç±»ä¼¼äºå‰é¢æåˆ°çš„æ•°æ®é¢„å¤„ç†æ­¥éª¤
  
  - They calculate the mean Î¼ and variance Ïƒ of a batch of input data, and normalize the data x to a zero mean and unit variance  
    ä»–ä»¬è®¡ç®—ä¸€æ‰¹è¾“å…¥æ•°æ®çš„å¹³å‡ Î¼ å’Œæ–¹å·® Ïƒï¼Œå¹¶å°†æ•°æ® x å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼å’Œå•ä½æ–¹å·®

- **BatchNorm layers** alleviate the problems of proper initialization of the parameters and hyper-parameters BatchNorm å±‚å‡è½»äº†æ­£ç¡®åˆå§‹åŒ–å‚æ•°å’Œè¶…å‚æ•°çš„é—®é¢˜ 
  
  - Result in faster convergence training, allow larger learning rates   
    ç»“æœæ›´å¿«çš„æ”¶æ•›è®­ç»ƒï¼Œå…è®¸æ›´å¤§çš„å­¦ä¹ ç‡
  
  - Reduce the internal covariate shift  
    å‡å°å†…éƒ¨åå˜é‡ç§»ä½

- BatchNorm layers are inserted immediately after convolutional layers or fullyconnected layers, and before activation layers  
  BatchNorm å±‚æ’å…¥åˆ°å·ç§¯å±‚æˆ–å®Œå…¨è¿æ¥å±‚ä¹‹åï¼Œä»¥åŠæ¿€æ´»å±‚ä¹‹å‰
  
  - They are very common with convolutional NN  
    å®ƒä»¬åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­éå¸¸å¸¸è§

##### Deep vs Shallow NetworksÂ  æ·±å±‚ç½‘ç»œä¸æµ…å±‚ç½‘ç»œ

- Deeper networks perform better than shallow networks   
  æ·±å±‚ç½‘ç»œæ¯”æµ…å±‚ç½‘ç»œæ€§èƒ½æ›´å¥½
  
  - But only up to some limit: after a certain number of layers, the performance of deeper networks plateaus  
    ä½†åªæ˜¯è¾¾åˆ°äº†ä¸€å®šçš„é™åˆ¶: ç»è¿‡ä¸€å®šæ•°é‡çš„å±‚æ¬¡åï¼Œç½‘ç»œçš„æ€§èƒ½è¿›ä¸€æ­¥åœæ»ä¸å‰

<img title="" src="./images/8a6d9035-dfc9-404b-b7f9-971436dfe298.png" alt="8a6d9035-dfc9-404b-b7f9-971436dfe298" style="zoom:33%;">

##### Convolutional Neural Networks (CNNs) å·ç§¯ç¥ç»ç½‘ç»œ

- Convolutional neural networks (CNNs) were primarily designed for image data  
  å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ä¸»è¦æ˜¯ä¸ºå›¾åƒæ•°æ®è€Œè®¾è®¡çš„

- CNNs use a convolutional operator for extracting data features  
  CNN ä½¿ç”¨å·ç§¯è¿ç®—ç¬¦æå–æ•°æ®ç‰¹å¾ 
  
  - Allows parameter sharing 
    å…è®¸å‚æ•°å…±äº«
  
  - Efficient to train   
    è®­ç»ƒæ•ˆç‡é«˜
  
  - Have less parameters than NNs with fully-connected layers   
    å‚æ•°å°‘äºå…·æœ‰å®Œå…¨è¿æ¥å±‚çš„ç¥ç»ç½‘ç»œ

- CNNs are robust to spatial translations of objects in images  
  ç¥ç»ç½‘ç»œå¯¹å›¾åƒä¸­ç›®æ ‡çš„ç©ºé—´è½¬æ¢å…·æœ‰é²æ£’æ€§

- A convolutional filter slides (i.e., convolves) across the image  
  ä¸€ä¸ªå·ç§¯æ»¤æ³¢å™¨å¹»ç¯ç‰‡(å³ï¼Œå·ç§¯)æ¨ªè·¨å›¾åƒ
  
  <img src="./images/dbdeb1cf-aa17-4ced-ba0a-25f7c3fa7189.png" title="" alt="dbdeb1cf-aa17-4ced-ba0a-25f7c3fa7189" style="zoom:33%;">
  
  - When the convolutional filters are scanned over the image, they capture useful features  
    å½“å·ç§¯æ»¤æ³¢å™¨åœ¨å›¾åƒä¸Šæ‰«ææ—¶ï¼Œå®ƒä»¬æ•è·æœ‰ç”¨çš„ç‰¹å¾

- In CNNs, hidden units in a layer are only connected to a small region of the layer before it (called local **receptive field**)   
  åœ¨ CNN ä¸­ï¼Œä¸€ä¸ªå±‚ä¸­çš„éšè—å•å…ƒåªè¿æ¥åˆ°è¯¥å±‚ä¹‹å‰çš„ä¸€ä¸ªå°åŒºåŸŸ(ç§°ä¸ºæœ¬åœ°æ¥æ”¶åœº)
  
  - The depth of each **feature map** corresponds to the number of convolutional filters used at each layer  
    æ¯ä¸ªç‰¹å¾å›¾çš„æ·±åº¦ä¸æ¯ä¸€å±‚ä½¿ç”¨çš„å·ç§¯æ»¤æ³¢å™¨æ•°ç›®ç›¸å¯¹åº”

- Pooling layer æ± åŒ–å±‚
  
  - Max pooling: reports the maximum output within a rectangular neighborhood   
    Max pool: æŠ¥å‘ŠçŸ©å½¢é‚»åŸŸå†…çš„æœ€å¤§è¾“å‡º
  
  - Average pooling: reports the average output of a rectangular neighborhood  
    Average pooling: æŠ¥å‘Šä¸€ä¸ªçŸ©å½¢é‚»å±…çš„å¹³å‡äº§å‡º
  
  - Pooling layers reduce the spatial size of the feature maps  
    æ± å±‚å‡å°‘äº†ç‰¹å¾æ˜ å°„çš„ç©ºé—´å¤§å° 
    
    - Reduce the number of parameters, prevent overfitting  
      å‡å°‘å‚æ•°æ•°ç›®ï¼Œé˜²æ­¢è¿‡åº¦é…åˆ
    
    ![de132f3b-3d44-418e-a7bd-b3e17242d7d4](./images/de132f3b-3d44-418e-a7bd-b3e17242d7d4.png)

- Feature extraction architecture  ç‰¹å¾æå–ä½“ç³»ç»“æ„
  
  - After 2 convolutional layers, a max-pooling layer reduces the size of the feature maps (typically by 2)   
    ç»è¿‡2å·ç§¯å±‚ï¼Œæœ€å¤§æ± å±‚å‡å°‘ç‰¹å¾æ˜ å°„çš„å¤§å°(é€šå¸¸ä¸º2)
  
  - A fully convolutional and a softmax layers are added last to perform classification  
    æœ€åæ·»åŠ ä¸€ä¸ªå®Œå…¨å·ç§¯å±‚å’Œä¸€ä¸ªè½¯æœ€å¤§å±‚æ¥æ‰§è¡Œåˆ†ç±»
  
  ![a6740ec1-24d7-4324-bd02-0614dbc529ef](./images/a6740ec1-24d7-4324-bd02-0614dbc529ef.png)

#### Residual CNNs (ResNets) æ®‹å·®ç¥ç»ç½‘ç»œ

- Introduce â€œidentityâ€ **skip connections**  
  å¼•å…¥â€œæ ‡è¯†â€è·³è¿‡è¿æ¥
  
  - Layer inputs are propagated and added to the layer output   
    å±‚è¾“å…¥è¢«ä¼ æ’­å¹¶æ·»åŠ åˆ°å±‚è¾“å‡º
  
  - Mitigate the problem of vanishing gradients during training  
    ç¼“è§£åŸ¹è®­æœŸé—´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜
  
  - Allow training very deep NN (with over 1,000 layers)  Â 
    å…è®¸è®­ç»ƒéå¸¸æ·±çš„ç¥ç»ç½‘ç»œ(è¶…è¿‡1000å±‚)

- Several ResNet variants exist: 18, 34, 50, 101, 152, and 200 layers   
  å­˜åœ¨å‡ ç§ ResNet å˜ä½“: 18ã€34ã€50ã€101ã€152å’Œ200å±‚

- Are used as base models of other state-of-the-art NNs   
  ç”¨ä½œå…¶ä»–æœ€å…ˆè¿›ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¨¡å‹
  
  - other similar models: ResNeXT, DenseNet  
    å…¶ä»–ç±»ä¼¼çš„æ¨¡å‹: ResNeXTï¼ŒDenseNet

- <img src="./images/28b4283a-f7f2-4182-81f9-926509bbc2d4.png" title="" alt="28b4283a-f7f2-4182-81f9-926509bbc2d4" style="zoom:33%;">

##### Recurrent Neural Networks (RNNs) å›å½’ç¥ç»ç½‘ç»œ

- Recurrent NNs are used for modeling **sequential data** and data with varying length of inputs and outputs  
  é€’å½’ç¥ç»ç½‘ç»œç”¨äºå¯¹ **é¡ºåºæ•°æ®** å’Œå…·æœ‰ä¸åŒè¾“å…¥å’Œè¾“å‡ºé•¿åº¦çš„æ•°æ®è¿›è¡Œå»ºæ¨¡
  
  - Videos, text, speech, DNA sequences, human skeletal data  
    è§†é¢‘ï¼Œæ–‡æœ¬ï¼Œè¯­éŸ³ DNA åºåˆ—ï¼Œäººä½“éª¨éª¼æ•°æ®

- RNNs introduce recurrent connections between the neurons   
  é€’å½’ç¥ç»ç½‘ç»œåœ¨ç¥ç»å…ƒä¹‹é—´å¼•å…¥å¤å‘æ€§è¿æ¥
  
  - This allows processing sequential data one element at a time by selectively passing information across a sequence   
    è¿™å…è®¸é€šè¿‡æœ‰é€‰æ‹©åœ°è·¨åºåˆ—ä¼ é€’ä¿¡æ¯æ¥ä¸€æ¬¡å¤„ç†ä¸€ä¸ªå…ƒç´ çš„é¡ºåºæ•°æ®
  
  - Memory of the previous inputs is stored in the modelâ€™s internal state and affect the model predictions   
    å…ˆå‰è¾“å…¥çš„å†…å­˜å­˜å‚¨åœ¨æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ä¸­ï¼Œå¹¶å½±å“æ¨¡å‹çš„é¢„æµ‹
  
  - Can capture correlations in sequential data   
    å¯ä»¥æ•è·åºåˆ—æ•°æ®ä¸­çš„ç›¸å…³æ€§

- RNNs use backpropagation-through-time for training   
  é€’å½’ç¥ç»ç½‘ç»œä½¿ç”¨æ—¶é—´åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒ

- RNNs are more sensitive to the vanishing gradient problem than CNNs  
  é€’å½’ç¥ç»ç½‘ç»œå¯¹æ¶ˆå¤±æ¢¯åº¦é—®é¢˜æ¯”å·ç§¯ç¥ç»ç½‘ç»œæ›´æ•æ„Ÿ

##### Long Short-Term Memory (LSTM) Networks é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ

- Long Short-Term Memory (LSTM) networks are a variant of RNNs  
  é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæ˜¯é€’å½’ç¥ç»ç½‘ç»œçš„ä¸€ç§å˜å½¢ 

- LSTM mitigates the vanishing/exploding gradient problem  
  LSTMå‡è½»äº†æ¶ˆå¤±/çˆ†ç‚¸æ¢¯åº¦
  
  - Solution: **a Memory Cell**, updated at each step in the sequence  
    è§£å†³æ–¹æ¡ˆ:ä¸€ä¸ªå­˜å‚¨å•å…ƒ,åœ¨æ¯ä¸€æ­¥æ›´æ–°

- Three gates control the flow of information to and from the Memory Cell   
  ä¸‰ä¸ªé—¨æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨
  
  - **Input Gate**: protects the current step from irrelevant inputs  
    è¾“å…¥é—¨ï¼šé€šè¿‡ä¸ç›¸å…³çš„è¾“å…¥ä¿æŠ¤ç›®å‰çš„æ­¥éª¤
  
  - **Output Gate**: prevents current step from passing irrelevant information to later steps  
    è¾“å‡ºé—¨: é˜²æ­¢å½“å‰æ­¥éª¤å°†ä¸ç›¸å…³çš„ä¿¡æ¯ä¼ é€’ç»™åé¢çš„æ­¥éª¤
  
  - **Forget Gate**: limits information passed from one cell to the next  
    å¿˜è®°é—¨: é™åˆ¶ä¿¡æ¯ä»ä¸€ä¸ªå•å…ƒä¼ é€’åˆ°ä¸‹ä¸€ä¸ªå•å…ƒ

- Most modern RNN models use either LSTM units or other more advanced types of recurrent units (e.g., GRU units)  
  å¤§å¤šæ•°ç°ä»£ RNN æ¨¡å‹ä½¿ç”¨ LSTM å•å…ƒæˆ–å…¶ä»–æ›´å…ˆè¿›ç±»å‹çš„å¾ªç¯å•å…ƒ(ä¾‹å¦‚ GRU å•å…ƒ)

- LSTM cell 
  
  - Input gate, output gate, forget gate, memory cell   
    è¾“å…¥é—¨ï¼Œè¾“å‡ºé—¨ï¼Œå¿˜è®°é—¨ï¼Œå­˜å‚¨å•å…ƒ
  
  - LSTM can learn long-term correlations within data sequences  
    LSTM å¯ä»¥å­¦ä¹ æ•°æ®åºåˆ—ä¸­çš„é•¿æœŸç›¸å…³æ€§
  
  - $$
    \begin{align*}
\mathbf{i}^{(k)} &= \sigma \left( \mathbf{W}_{oi} \mathbf{o}_m^{(k)} + \mathbf{W}_{hi} \mathbf{h}^{(k-1)} + \mathbf{b}_i \right) \\
\mathbf{f}^{(k)} &= \sigma \left( \mathbf{W}_{of} \mathbf{o}_m^{(k)} + \mathbf{W}_{hf} \mathbf{h}^{(k-1)} + \mathbf{b}_f \right) \\
\mathbf{q}^{(k)} &= \sigma \left( \mathbf{W}_{oq} \mathbf{o}_m^{(k)} + \mathbf{W}_{hq} \mathbf{h}^{(k-1)} + \mathbf{b}_q \right) \\
\mathbf{c}^{(k)} &= \mathbf{f}^{(k)} \mathbf{c}^{(k-1)} + \mathbf{i}^{(k)} \sigma \left( \mathbf{W}_{oc} \mathbf{o}_m^{(k)} + \mathbf{W}_{hc} \mathbf{h}^{(k-1)} + \mathbf{b}_c \right) \\
\mathbf{h}^{(k)} &= \mathbf{q}^{(k)} \tanh \left( \mathbf{c}^{(k)} \right)
\end{align*}

    $$
  
  <img title="" src="./images/0d8c443e-39fa-4c33-8f49-03fad10960d6.png" alt="0d8c443e-39fa-4c33-8f49-03fad10960d6" style="zoom:50%;">

##### Deep learning frameworks æ·±åº¦å­¦ä¹ æ¡†æ¶

- Kinds
  
  - Caffe
  
  - torch
  
  - Chainer
  
  - PyTorch
  
  - Caffe2
  
  - DeepLearning4J
  
  - TensorFlow
  
  - theano
  
  - dmlc-mxnet

- Keras is a high-level neural networks API  
  Kera æ˜¯ä¸€ä¸ªé«˜çº§ç¥ç»ç½‘ç»œ API
  
  - we will use TensorFlow as the compute backend  
    æˆ‘ä»¬å°†ä½¿ç”¨ TensorFlow ä½œä¸ºè®¡ç®—åç«¯

- PyTorch is
  
  - a GPU-based tensor library  
    ä¸€ä¸ªåŸºäºå›¾å½¢å¤„ç†å™¨çš„å¼ é‡åº“
  
  - an efficient library for dynamic neural networks  
    ä¸€ä¸ªé«˜æ•ˆçš„åŠ¨æ€ç¥ç»ç½‘ç»œåº“
  
  <img src="./images/57dd006d-cdd6-48f7-a608-b1940d59e933.png" title="" alt="57dd006d-cdd6-48f7-a608-b1940d59e933" style="zoom:50%;">

### Part 2: Reinforcement Learning ç¬¬äºŒéƒ¨åˆ†: å¼ºåŒ–å­¦ä¹ 

#### Introduction to Reinforcement Learning å¼ºåŒ–å­¦ä¹ å…¥é—¨

##### å¼ºåŒ–å­¦ä¹ èƒ½è§£å†³çš„é—®é¢˜ï¼ˆå’Œä¸èƒ½è§£å†³çš„é—®é¢˜ï¼‰

- Idea: An agent (an AI) will learn from the environment by **interacting with it** (through trial and error) and **receiving rewards** (negative or positive) as feedback for performing actions.   
  æƒ³æ³•: ä¸€ä¸ªæ™ºèƒ½ä½“(äººå·¥æ™ºèƒ½)å°†é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨(é€šè¿‡å°è¯•å’Œé”™è¯¯)æ¥å­¦ä¹ ï¼Œå¹¶æ¥å—å¥–åŠ±(è´Ÿé¢æˆ–æ­£é¢)ä½œä¸ºæ‰§è¡Œè¡ŒåŠ¨çš„åé¦ˆã€‚

- Goal: **Maximize the reward** by taking right actions 
  ç›®æ ‡: é€šè¿‡é‡‡å–æ­£ç¡®çš„è¡ŒåŠ¨ä½¿å¥–åŠ±æœ€å¤§åŒ–

<img src="./images/581331ea-63b7-4ef7-9532-966361498271.png" title="" alt="581331ea-63b7-4ef7-9532-966361498271" style="zoom:33%;">

##### Markov Decision Processes (MDPs)  é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

#### Markov Process and Markov ChainÂ  é©¬å°”å¯å¤«è¿‡ç¨‹ä¸é©¬å°”å¯å¤«é“¾

- A Markov process or Markov chain is a **stochastic model** describing a sequence of possible events where the probability of each event depends only on the state attained in the previous event.  
  é©¬å°”å¯å¤«è¿‡ç¨‹æˆ–é©¬å°”å¯å¤«é“¾æ˜¯æè¿°ä¸€ç³»åˆ—å¯èƒ½äº‹ä»¶çš„éšæœºæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªäº‹ä»¶çš„æ¦‚ç‡ä»…å–å†³äºå…ˆå‰äº‹ä»¶ä¸­æ‰€è¾¾åˆ°çš„çŠ¶æ€ã€‚

- Markov property: The state of the system at time $t+1$ depends only on the state of the system at time $t$  
  Markov æ€§è´¨: ç³»ç»Ÿåœ¨æ—¶é—´ $t + 1 $æ—¶çš„çŠ¶æ€ä»…å–å†³äºç³»ç»Ÿåœ¨æ—¶é—´ $t $æ—¶çš„çŠ¶æ€
  
  $$
  P\left[ X_{t+1} = x_{t+1} \mid X_1, X_t = x_1, x_t \right] = P\left[ X_{t+1} = x_{t+1} \mid X_t = x_t \right]
  $$

```mermaid
graph LR
    X1 --> X2
    X2 --> X3
    X3 --> X4
    X4 --> X5

```

- Stationary Assumption: State transition probabilities are **independent of time** (ğ‘¡)  
  å¹³ç¨³å‡è®¾: çŠ¶æ€è½¬ç§»æ¦‚ç‡ä¸æ—¶é—´(t)æ— å…³
  
  $$
  P \left[ X_{t+1} = b / X_t = a \right] = p_{ab}
  $$

---

é©¬å°”å¯å¤«é“¾çš„è®¡ç®—

é©¬å°”å¯å¤«é“¾çš„æ ¸å¿ƒæ˜¯è½¬æ¢æ¦‚ç‡çŸ©é˜µï¼ˆTransition Probability Matrixï¼‰ï¼Œé€šå¸¸è®°ä¸º \(P\)ã€‚è¿™ä¸ªçŸ©é˜µæè¿°äº†ç³»ç»Ÿåœ¨æ¯ä¸ªçŠ¶æ€ä¹‹é—´çš„è½¬æ¢æ¦‚ç‡ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªæœ‰ä¸‰ä¸ªçŠ¶æ€ $(S_1, S_2, S_3)$ çš„ç³»ç»Ÿï¼Œè½¬æ¢æ¦‚ç‡çŸ©é˜µ \(P\) å¯èƒ½æ˜¯è¿™æ ·çš„ï¼š

$$
P = \begin{bmatrix} P(S_1 \to S_1) & P(S_1 \to S_2) & P(S_1 \to S_3) \\ P(S_2 \to S_1) & P(S_2 \to S_2) & P(S_2 \to S_3) \\ P(S_3 \to S_1) & P(S_3 \to S_2) & P(S_3 \to S_3) \end{bmatrix}
$$

æ¯ä¸ªå…ƒç´  $(P(i \to j))$ è¡¨ç¤ºç³»ç»Ÿä»çŠ¶æ€$i$è½¬æ¢åˆ°çŠ¶æ€$j$çš„æ¦‚ç‡ã€‚çŸ©é˜µçš„æ¯ä¸€è¡Œçš„å’Œä¸º1ï¼Œå› ä¸ºæ¯è¡Œè¡¨ç¤ºä»æŸä¸ªçŠ¶æ€å¼€å§‹çš„æ‰€æœ‰å¯èƒ½è½¬æ¢çš„æ¦‚ç‡æ€»å’Œã€‚

<img title="" src="./images/2ff74e36-12bc-4d2a-8fd4-671c1c20746f.png" alt="2ff74e36-12bc-4d2a-8fd4-671c1c20746f" style="zoom:33%;">

EXP

- Given that a personâ€™s last cola purchase was Coke, there is a 90% chance that his next cola purchase will also be Coke. If a personâ€™s last cola purchase was Pepsi, there is an 80% chance that his next cola purchase will also be Pepsi.  
  é‰´äºä¸€ä¸ªäººä¸Šä¸€æ¬¡è´­ä¹°å¯ä¹æ˜¯å¯å£å¯ä¹ï¼Œé‚£ä¹ˆä»–ä¸‹ä¸€æ¬¡è´­ä¹°å¯ä¹ä¹Ÿæœ‰90% çš„å¯èƒ½æ˜¯å¯å£å¯ä¹ã€‚å¦‚æœä¸€ä¸ªäººä¸Šä¸€æ¬¡è´­ä¹°å¯ä¹æ˜¯ç™¾äº‹å¯ä¹ï¼Œé‚£ä¹ˆä»–ä¸‹ä¸€æ¬¡è´­ä¹°å¯ä¹çš„å¯èƒ½æ€§ä¹Ÿæœ‰80% æ˜¯ç™¾äº‹å¯ä¹ã€‚
  
  <img title="" src="./images/274cd91a-56ef-4dae-a31d-94ce84b0e688.png" alt="274cd91a-56ef-4dae-a31d-94ce84b0e688" style="zoom:33%;">

- Given that a person is currently a Pepsi purchaser, what is the probability that he will purchase Coke TWO purchases from now?  
  å‡è®¾ä¸€ä¸ªäººç›®å‰æ˜¯ç™¾äº‹å¯ä¹çš„è´­ä¹°è€…ï¼Œé‚£ä¹ˆä»–ä»ç°åœ¨å¼€å§‹è´­ä¹°å¯å£å¯ä¹çš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿ
  
  $$
  P\left[ \text{Pepsi} \rightarrow ? \rightarrow \text{Coke} \right] = P\left[ \text{Pepsi} \rightarrow \text{Coke} \rightarrow \text{Coke} \right] + P\left[ \text{Pepsi} \rightarrow \text{Pepsi} \rightarrow \text{Coke} \right] \\
= 0.2 \times 0.9 + 0.8 \times 0.2 = 0.34 \\ \ \\
P^2 = \begin{bmatrix} 0.9 & 0.1 \\ 0.2 & 0.8 \end{bmatrix} \begin{bmatrix} 0.9 & 0.1 \\ 0.2 & 0.8 \end{bmatrix} = \begin{bmatrix} 0.83 & 0.17 \\ 0.34 & 0.66 \end{bmatrix}
  $$

- Assume each person makes one cola purchase per week;  Suppose 60% of all people now drink Coke, and 40% drink Pepsi; What fraction of people will be drinking Coke three weeks from now?  
  å‡è®¾æ¯ä¸ªäººæ¯å‘¨è´­ä¹°ä¸€æ¯å¯ä¹; å‡è®¾60% çš„äººç°åœ¨å–å¯å£å¯ä¹ï¼Œ40% çš„äººå–ç™¾äº‹å¯ä¹; ä»ç°åœ¨å¼€å§‹çš„ä¸‰å‘¨å†…ï¼Œæœ‰å¤šå°‘äººä¼šå–å¯å£å¯ä¹ï¼Ÿ
  
  $$
  P = \begin{bmatrix} 0.9 & 0.1 \\ 0.2 & 0.8 \end{bmatrix} \quad P^3 = \begin{bmatrix} 0.781 & 0.219 \\ 0.438 & 0.562 \end{bmatrix} \\P \left[X_3 = \text{Coke}\right] = 0.6 \times 0.781 + 0.4 \times 0.438 = 0.6438
  $$

- * **åˆå§‹çŠ¶æ€åˆ†å¸ƒ initial distribution $Q_0$**ï¼šåˆå§‹çŠ¶æ€åˆ†å¸ƒ$Q_0$ç»™å‡ºäº†å½“å‰é€‰æ‹© Coke å’Œ Pepsi çš„æ¯”ä¾‹ã€‚è¿™é‡Œæ˜¯ï¼š
    
    $$
    Q_0 = \begin{bmatrix} 0.6 & 0.4 \end{bmatrix}
    $$
    
    è¡¨ç¤ºå½“å‰æœ‰ 60% çš„äººå– Cokeï¼Œ40% çš„äººå– Pepsiã€‚
  
  * **ä¸‰å‘¨åçš„åˆ†å¸ƒ $P^3$**ï¼šè¦è®¡ç®—ä¸‰å‘¨åçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—è½¬ç§»çŸ©é˜µçš„ä¸‰æ¬¡æ–¹$P^3$ã€‚æ ¹æ®é¢˜ç›®ç»™å‡ºçš„ä¿¡æ¯ï¼Œå·²çŸ¥ï¼š
    
    $$
    P^3 = \begin{bmatrix} 0.781 & 0.219 \\ 0.438 & 0.562 \end{bmatrix}
    $$
    
    è¿™ä¸ªçŸ©é˜µè¡¨ç¤ºåœ¨ç»è¿‡ä¸‰æ¬¡è½¬ç§»åï¼ˆå³ä¸‰å‘¨åï¼‰ï¼Œä» Coke è½¬ç§»åˆ° Coke çš„ç´¯è®¡æ¦‚ç‡ä¸º 0.781ï¼Œä» Coke è½¬ç§»åˆ° Pepsi çš„ç´¯è®¡æ¦‚ç‡ä¸º 0.219ï¼Œç­‰ç­‰ã€‚
  
  * **è®¡ç®—ä¸‰å‘¨åé€‰æ‹© Coke çš„æ¯”ä¾‹**ï¼šä½¿ç”¨åˆå§‹åˆ†å¸ƒ $Q_0$ ä¸ $P^3$ ç›¸ä¹˜å¾—åˆ°ä¸‰å‘¨åçš„åˆ†å¸ƒ $Q_3$ï¼šå…·ä½“è®¡ç®—ä¸ºï¼š
    
    $$
    Q_3 = \begin{bmatrix} 0.6 & 0.4 \end{bmatrix} \times \begin{bmatrix} 0.781 & 0.219 \\ 0.438 & 0.562 \end{bmatrix}
    $$
    
    åˆ†åˆ«è®¡ç®—æ¯ä¸€é¡¹ï¼š
    
    $$
    Q_3(\text{Coke}) = 0.6 \times 0.781 + 0.4 \times 0.438 = 0.6438 \\ Q_3(\text{Pepsi}) = 0.6 \times 0.219 + 0.4 \times 0.562 = 0.3562
    $$
    
    å› æ­¤ï¼Œä¸‰å‘¨åé€‰æ‹© Coke çš„æ¯”ä¾‹ä¸º 0.6438ï¼Œè€Œé€‰æ‹© Pepsi çš„æ¯”ä¾‹ä¸º 0.3562ã€‚
  - <img title="" src="./images/bb1eaf00-c58b-4db5-b88e-f8f0d30e055f.png" alt="bb1eaf00-c58b-4db5-b88e-f8f0d30e055f" style="zoom:33%;">

---

##### Markov decision process (MDP) definition  Â é©¬å¯å¤«å†³ç­–è¿‡ç¨‹å®šä¹‰

- Markov decision process (MDP): The mathematical description of reinforcement learning  
  é©¬å¯å¤«å†³ç­–è¿‡ç¨‹: å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æè¿°
  
  $$
  \langle S, A, R, P, \gamma \rangle
\\ \ \\
\textit{S} \text{ is set of possible \textcolor{red}{states}} \\
\text{ä¸€ç»„å¯èƒ½çŠ¶æ€çš„é›†åˆ}\\
\textit{A} \text{ is set of possible \textcolor{red}{actions}} \\
\text{ä¸€ç»„å¯èƒ½ä½œåŠ¨çš„é›†åˆ}\\
\textit{R} \text{ is distribution of \textcolor{red}{reward} given (state, action) pair} \\
\text{ç»™äºˆ(çŠ¶æ€ï¼Œè¡ŒåŠ¨)å¯¹çš„æŠ¥é…¬åˆ†é…}\\
\textit{P} \text{ is \textcolor{red}{transition probability} } \\
\text{è½¬ç§»æ¦‚ç‡}\\
\gamma \text{ is reward \textcolor{red}{discount factor} in } [0,1]\\
\text{[0,1]ä¸­çš„å¥–åŠ±æŠ˜æ‰£å› å­}\\

  $$
  
  $\gamma $ Lower value encourages shortterm rewards while higher value promises long-term reward  
  è¾ƒä½çš„å€¼é¼“åŠ±çŸ­æœŸå›æŠ¥ï¼Œè€Œè¾ƒé«˜çš„å€¼æ‰¿è¯ºé•¿æœŸå›æŠ¥

##### Deterministic vs. Stochastic  ç¡®å®šæ€§ä¸éšæœºæ€§

- **Deterministic**: The next state and the corresponding reward are determined **solely by the current state and the action chosen**.   
  ç¡®å®šæ€§: ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œç›¸åº”çš„å¥–åŠ±å®Œå…¨ç”±å½“å‰çŠ¶æ€å’Œé€‰æ‹©çš„è¡ŒåŠ¨å†³å®šã€‚

- Stochastic: The next state and the corresponding reward are **determined by a probability distribution**.  
  éšæœº: ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œç›¸åº”çš„å¥–åŠ±æ˜¯ç”±æ¦‚ç‡åˆ†å¸ƒå†³å®šçš„ã€‚

##### Markov decision process (MDP) calculate  é©¬å¯å¤«å†³ç­–è¿‡ç¨‹è®¡ç®—

- è®¡ç®—æ–¹æ³•
  
  - Environment initializes a state at time step t=0  
    ç¯å¢ƒåœ¨æ—¶é—´æ­¥éª¤ t = 0åˆå§‹åŒ–çŠ¶æ€
  
  - While not done:  
    å½“ä¸‹é¢çš„æ­¥éª¤æœªå®Œæˆ
    
    - Agent selects action at $a_t$   
      æ™ºèƒ½é€‰æ‹©ä¸€ä¸ªæ“ä½œ
    
    - Environment returns reward $r_t$   
      ç¯å¢ƒç»™å‡ºä¸€ä¸ªå¥–åŠ±
    
    - Environment gives next state $s_{t+1}$   
      ç¯å¢ƒç»™å‡ºä¸‹ä¸€çŠ¶æ€
    
    - Agent receives reward $r_t$ and next state $s_{t+1}$   
      æ™ºèƒ½æ¥å—å¥–åŠ±å’Œè¥¿åŒ»çŠ¶æ€

- A policy Ï€ is a function from S to A that specifies what action to take in each state:  
  ç­–ç•¥ Ï€ æ˜¯ä¸€ä¸ªä» S åˆ° A çš„å‡½æ•°ï¼ŒæŒ‡å®šåœ¨æ¯ä¸ªçŠ¶æ€ä¸‹é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨:
  
  $$
  \pi(a | s) = \mathbb{P}[A_t = a | S_t = s]
  $$

- Objective: find optimal policy Ï€* that maximizes accumulative discounted reward:  
  ç›®æ ‡: å¯»æ‰¾æœ€ä¼˜æ”¿ç­– Ï€ * ï¼Œæœ€å¤§åŒ–ç´¯è®¡è´´ç°å›æŠ¥:
  
  $$
  \sum_{t \geq 0} \gamma^t r_t
  $$

##### The optimal policy Ï€*  æœ€ä¼˜ç­–ç•¥ Ï€ *

- The aim is to **find optimal policy Ï€*** that maximizes the accumulative rewards.   
  ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ Ï€ * ï¼Œä½¿ç´¯ç§¯å›æŠ¥æœ€å¤§åŒ–

- How do we **handle the randomness**.   
  æˆ‘ä»¬å¦‚ä½•å¤„ç†éšæœºæ€§ã€‚

- Maximize the expected sum of rewards:  
  æœ€å¤§åŒ–é¢„æœŸå›æŠ¥æ€»é¢:
  
  $$
  \pi^* = \arg\max_{\pi} \mathbb{E} \left( \sum_{t \geq 0} \gamma^t r_t \mid \pi \right)
  $$
  
  <img title="" src="./images/a3f7ed37-13a7-4d85-9b17-d231a87b722c.png" alt="a3f7ed37-13a7-4d85-9b17-d231a87b722c" style="zoom:33%;">

##### Value function  ä»·å€¼å‡½æ•°

- The relationship between state value function and Q-value function:  
  çŠ¶æ€å€¼å‡½æ•°ä¸ Q å€¼å‡½æ•°çš„å…³ç³»:
  
  $$
  Q(s, a) = R + \gamma \sum_{s'} P V(s')\\
R \ Reward \\
\gamma \  Discount \ factor \\
P \ Transition \ probability \\
V(s') \ Value \ function \ of \ next \  state sâ€™
  $$

##### The Bellman equation è´å°”æ›¼æ–¹ç¨‹

- Bellman equation helps agent to iterate on value, thus progressively optimizing the policy:  
  è´å°”æ›¼æ–¹ç¨‹å¸®åŠ©ä»£ç†äººè¿­ä»£ä»·å€¼ï¼Œä»è€Œé€æ­¥ä¼˜åŒ–æ”¿ç­–:

$$
V(s) = \mathbb{E} \left[ R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s \right] \\
s \text{ is the state} \\
R \text{ is reward, } G_{t+1} \text{ is the accumulative reward from } S_{t+1} \\
\gamma \text{ is the reward discount factor in } [0,1] 


$$

- According to the Bellman Equation,**long-term reward in a given action** is equal to **the reward from the current action combined with the expected reward from the future actions** taken at the following time.  
  æ ¹æ®è´å°”æ›¼æ–¹ç¨‹ï¼Œä¸€ä¸ªç»™å®šè¡Œä¸ºçš„é•¿æœŸå›æŠ¥ç­‰äºå½“å‰è¡Œä¸ºçš„å›æŠ¥å’Œä¸‹ä¸€æ—¶é—´æœªæ¥è¡Œä¸ºçš„é¢„æœŸå›æŠ¥ã€‚

#### RL Techniques: From Q-learning to Actor-Critic RL æŠ€æœ¯ï¼šä» Q-learning åˆ° Actor-Critic

![e3f7d09a-17ab-4a4c-bd7b-5944b5eb7f75](./images/e3f7d09a-17ab-4a4c-bd7b-5944b5eb7f75.png)

##### Model-Free RL æ— æ¨¡å‹ RL

- learns strategies directly **without the need for an explicit model of the environment**.  
  ç›´æ¥å­¦ä¹ ç­–ç•¥è€Œä¸éœ€è¦ä¸€ä¸ªæ˜ç¡®çš„ç¯å¢ƒæ¨¡å‹ã€‚ 

- Agent interacts with the real environment and relies on real environment feedback and reward for learning, and as a result, it may take irreversible and disruptive actions.  
  æ™ºèƒ½ä½“ä¸çœŸå®ç¯å¢ƒç›¸äº’ä½œç”¨ï¼Œä¾èµ–çœŸå®ç¯å¢ƒçš„åé¦ˆå’Œå­¦ä¹ å¥–åŠ±ï¼Œå› æ­¤ï¼Œå®ƒå¯èƒ½é‡‡å–ä¸å¯é€†è½¬å’Œç ´åæ€§çš„è¡ŒåŠ¨ã€‚

##### Model-Based RL åŸºäºæ¨¡å‹çš„ RL

- attempts to **model the environment** and plan future actions.  
  è¯•å›¾æ¨¡æ‹Ÿç¯å¢ƒå’Œè®¡åˆ’æœªæ¥çš„è¡ŒåŠ¨ã€‚

- Agent constructs a simulated model first. The information an agent receives from the environment for a given state and action is transition probability and reward.  
  Agent é¦–å…ˆæ„å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ã€‚ä»£ç†ä»ç¯å¢ƒä¸­æ¥æ”¶åˆ°çš„å…³äºç»™å®šçŠ¶æ€å’Œè¡Œä¸ºçš„ä¿¡æ¯æ˜¯è½¬ç§»æ¦‚ç‡å’ŒæŠ¥é…¬ã€‚

##### Value-Based RL åŸºäºä»·å€¼çš„ RL

- selects actions by learning **value functions** and **efficient in finding optimal policies**.   
  é€šè¿‡å­¦ä¹ ä»·å€¼å‡½æ•°é€‰æ‹©è¡Œä¸ºï¼Œæœ‰æ•ˆåœ°æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚
  
  - The agent optimises the policy by selecting the action **that has the highest value function in a given state**.  
    ä»£ç†é€šè¿‡é€‰æ‹©åœ¨ä¸€èˆ¬çŠ¶æ€ä¸­å…·æœ‰æœ€é«˜ä»·å€¼å‡½æ•°çš„æ“ä½œæ¥ä¼˜åŒ–ç­–ç•¥ã€‚
  
  - **Advantage**: it can **find the optimal policy** efficiently and have **high sample efficiency**.   
    ä¼˜ç‚¹: èƒ½æœ‰æ•ˆåœ°æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œæ ·æœ¬æ•ˆç‡é«˜ã€‚
  
  - **Disadvantage**: it **cannot solve problem with continuous action space and sensitive to hyperparameters**.  
    ç¼ºç‚¹: ä¸èƒ½è§£å†³åŠ¨ä½œç©ºé—´è¿ç»­ä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚

##### Policy-Based RL åŸºäºç­–ç•¥çš„ RL

- selects actions directly by learning **policy functions** with **efficient convergence** and can **handle continuous action spaces**.  
  é€šè¿‡å­¦ä¹ å…·æœ‰é«˜æ•ˆæ”¶æ•›æ€§çš„ç­–ç•¥å‡½æ•°ç›´æ¥é€‰æ‹©åŠ¨ä½œï¼Œå¹¶èƒ½å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ã€‚
  
  - Advantage: it can **deal with continuous action spaces**, and it is **easier to converge** in real-world environments.   
    ä¼˜ç‚¹: å®ƒå¯ä»¥å¤„ç†è¿ç»­çš„åŠ¨ä½œç©ºé—´ï¼Œå¹¶ä¸”åœ¨ç°å®ç¯å¢ƒä¸­æ›´å®¹æ˜“æ”¶æ•›ã€‚
  
  - Disadvantage: it may require **more training data** due to the direct learning of policies, and often converge to a **local optimum**.  
    ç¼ºç‚¹: ç”±äºå¯¹ç­–ç•¥çš„ç›´æ¥å­¦ä¹ ï¼Œå®ƒå¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç»å¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚

##### On-policy RL

- learns from **the policy that is being currently followed during exploration**.   
  ä»ç›®å‰åœ¨å‹˜æ¢è¿‡ç¨‹ä¸­æ‰€éµå¾ªçš„æ”¿ç­–ä¸­å­¦ä¹ ã€‚

##### Off-policy RL

- learns from a **different policy** than the one that the agent is currently following.  
  ä»ä¸ä»£ç†å½“å‰éµå¾ªçš„ç­–ç•¥ä¸åŒçš„ç­–ç•¥ä¸­å­¦ä¹ ã€‚

##### Q-learning

![0f68f770-05fb-4659-bb3b-a3a8066d17c5](./images/0f68f770-05fb-4659-bb3b-a3a8066d17c5.png)

- Q-learning: Use a function approximator to estimate the state-action value function, namely the Q-function:  
  Q å­¦ä¹ : ä½¿ç”¨å‡½æ•°é€¼è¿‘å™¨æ¥ä¼°è®¡çŠ¶æ€ä½œç”¨å€¼å‡½æ•°ï¼Œå³ Q å‡½æ•°:
  $$
  Q(s, a \mid \omega) \approx Q^*(s, a) \\

\omega \text{ is the parameters of the function (weights)}

  $$

- To train this Q-Function, that given a state and action as input, output the Q-value, we initialize and update a Q-table  
  ä¸ºäº†è®­ç»ƒè¿™ä¸ªç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œä½œä¸ºè¾“å…¥çš„ Q å‡½æ•°ï¼Œè¾“å‡º Q å€¼ï¼Œæˆ‘ä»¬åˆå§‹åŒ–å¹¶æ›´æ–°ä¸€ä¸ª Q è¡¨![34a32813-5989-4c8d-be48-d122a252b2ce](./images/34a32813-5989-4c8d-be48-d122a252b2ce.png)

##### From Q-learning to Deep Q-network ä» Q å­¦ä¹ åˆ°æ·±åº¦ Q ç½‘ç»œ

- If the function approximator is a deep neural network (DNN)   
  å¦‚æœå‡½æ•°é€¼è¿‘å™¨æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)  
  Deep Q-learning/-network (DQN)  
  æ·±åº¦ Q å­¦ä¹ /-ç½‘ç»œ(DQN)
  
  <img title="" src="./images/94678e32-2451-4db0-8334-3a5431c56a42.png" alt="94678e32-2451-4db0-8334-3a5431c56a42" style="zoom:33%;">

- **Q-value is the expected accumulative reward** from taking action $a$ at state $s$  
  Q å€¼æ˜¯åœ¨çŠ¶æ€$s$ä¸‹é‡‡å–è¡ŒåŠ¨$a$çš„é¢„æœŸç´¯ç§¯æŠ¥é…¬

##### Deep Q-network (DQN)

- Using **deep neural network** to help scale up to making decisions in **extremely large domains**.   
  ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå¸®åŠ©åœ¨æå¤§çš„é¢†åŸŸä¸­åšå‡ºå†³ç­–ã€‚

- For example, when we have **many states** (e.g., every frame of the Atari game) **or even continuous state space**, we **cannot list all** the state-action pair in the Q-table.   
  ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬æœ‰è®¸å¤šçŠ¶æ€(ä¾‹å¦‚ï¼Œé›…è¾¾åˆ©æ¸¸æˆçš„æ¯ä¸€å¸§) ï¼Œç”šè‡³è¿ç»­çš„çŠ¶æ€ç©ºé—´ï¼Œæˆ‘ä»¬ä¸èƒ½åˆ—å‡ºæ‰€æœ‰çš„çŠ¶æ€-è¡ŒåŠ¨å¯¹åœ¨ Q è¡¨ã€‚

- Represent value function by **Q-network with weights Ï‰**  
  ç”¨æƒé‡ä¸º Ï‰ çš„ Q ç½‘ç»œè¡¨ç¤ºå€¼å‡½æ•°
  ![c0ce0430-68ae-4907-802b-ef4e5336438a](./images/c0ce0430-68ae-4907-802b-ef4e5336438a.png)

- Optimize loss function by stochastic gradient descent (SGD)  
  æŒ‰éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–æŸå¤±å‡½æ•°

##### Train the DQN   è®­ç»ƒDQN

###### Experience replay   ç»éªŒé‡æ’­

- To help remove correlations of training data, store dataset D (called a replay buffer) from prior experience.  
  ä¸ºäº†å¸®åŠ©æ¶ˆé™¤è®­ç»ƒæ•°æ®çš„ç›¸å…³æ€§ï¼Œä»ä»¥å‰çš„ç»éªŒä¸­å­˜å‚¨æ•°æ®é›† D (ç§°ä¸ºé‡æ’­ç¼“å†²åŒº)ã€‚
  
  <img src="./images/67c38004-6ac4-4419-b65c-6d7080593efb.png" title="" alt="67c38004-6ac4-4419-b65c-6d7080593efb" style="zoom:33%;">

- To perform experience replay, repeat the following:   
  è¿›è¡Œç»éªŒé‡æ’­æ—¶ï¼Œè¯·é‡å¤ä»¥ä¸‹æ­¥éª¤:
  
  - $(s, a, r, s') \sim \mathcal{D}$: **sample** an experience tuple from the dataset    
    ä»æ•°æ®é›†ä¸­æŠ½æ ·ä¸€ä¸ªç»éªŒå…ƒç»„
  
  - Compute the **target value** for the sampled state:  
    è®¡ç®—å–æ ·çŠ¶æ€çš„ç›®æ ‡å€¼:
    
    $$
    y = r + \gamma \max_{a'} \hat{Q}(s', a' \mid \omega)
    $$
  
  - Use SGD to **update** the network weights  
    ä½¿ç”¨ SGD æ›´æ–°ç½‘ç»œæƒé‡

###### Fixed Q-target ä¿®æ­£Q-ç›®æ ‡

- To help **improve stability**, **fix the target network weights** used in the target value calculation for multiple updates   
  ä¸ºäº†å¸®åŠ©æé«˜ç¨³å®šæ€§ï¼Œä¿®æ”¹ç”¨äºå¤šæ¬¡æ›´æ–°çš„ç›®æ ‡å€¼è®¡ç®—ä¸­çš„ç›®æ ‡ç½‘ç»œæƒé‡

- Use a different set of weights to compute target than is being updated   
  ä½¿ç”¨ä¸€ç»„ä¸åŒçš„æƒé‡æ¥è®¡ç®—ç›®æ ‡ï¼Œè€Œä¸æ˜¯è¿›è¡Œæ›´æ–°

- Let parameters $\omega^0$ be the set of **weights used in the target**, and  $\omega$ be the weights that are being updated  
  è®¾å‚æ•° $\omega^0$ æ˜¯ç›®æ ‡ä¸­ä½¿ç”¨çš„æƒé‡é›†ï¼Œ$\omega$ æ˜¯æ­£åœ¨æ›´æ–°çš„æƒé‡é›†

- Slight change to computation of target value:   
   ç›®æ ‡å€¼çš„è®¡ç®—ç•¥æœ‰å˜åŒ–:
  
  - $(s, a, r, s') \sim \mathcal{D}$: sample an experience tuple from the dataset   
    ä»æ•°æ®é›†ä¸­æŠ½æ ·ä¸€ä¸ªç»éªŒå…ƒç»„
  
  - Compute the **target value** for the sampled state:  
    è®¡ç®—å–æ ·çŠ¶æ€çš„ç›®æ ‡å€¼:
    
    $$
    y = r + \gamma \max_{a'} \hat{Q}(s', a' \mid \omega^o)

    $$
  
  - Use SGD to update the network weights  
    ä½¿ç”¨ SGD æ›´æ–°ç½‘ç»œæƒé‡
    
    

##### Policy gradient ç­–ç•¥æ¢¯åº¦

- The **Q-function can be very complicated**    
  Q å‡½æ•°å¯èƒ½éå¸¸å¤æ‚

- Hence, it is **impossible to learn exact value of every state-action pair**.   
  å› æ­¤ï¼Œä¸å¯èƒ½äº†è§£æ¯ä¸ªçŠ¶æ€æ“ä½œå¯¹çš„ç¡®åˆ‡ä»·å€¼ã€‚

- The **policy-based method**  
  åŸºäºç­–ç•¥çš„æ–¹æ³•
  
  - e.g., policy gradient, **can learn a policy directly**  
    ä¾‹å¦‚ï¼Œæ”¿ç­–æ¢¯åº¦ï¼Œå¯ä»¥ç›´æ¥å­¦ä¹ æ”¿ç­–
  
  - e.g., finding the best policy (i.e., **how to choose an action at a state**) from a collection of policies.  
    ä¾‹å¦‚ï¼Œä»ä¸€ç»„ç­–ç•¥ä¸­æ‰¾åˆ°æœ€ä½³ç­–ç•¥(å³ï¼Œå¦‚ä½•åœ¨ä¸€ä¸ªçŠ¶æ€é€‰æ‹©ä¸€ä¸ªæ“ä½œ)ã€‚

- steps 
  
  1. define a class of parametrized policies:  
     å®šä¹‰ä¸€ç±»å‚æ•°åŒ–ç­–ç•¥:
     
     $$
     \Pi = \left\{ \pi_{\theta}, \theta \in \mathbb{R}^m \right\}
     $$
  
  2. For each policy, define its value based on the discounted **accumulative reward**:  
     å¯¹äºæ¯ä»½ç­–ç•¥ï¼Œæ ¹æ®æŠ˜ç°çš„ç´¯è®¡æŠ¥é…¬ç¡®å®šå…¶ä»·å€¼:
     
     $$
     J(\theta) = \mathbb{E} \left[ \sum_{t \geq 0} \gamma^t r_t \mid \pi_{\theta} \right]
     $$
  
  3. We want to find the optimal policy:  
     æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°æœ€ä½³æ”¿ç­–:
     
     $$
     \theta^* = \arg\max_{\theta} J(\theta)
     $$
  
  4. **Do gradient ascent on policy parameters**  
     åœ¨ç­–ç•¥å‚æ•°ä¸Šè¿›è¡Œæ¢¯åº¦ä¸Šå‡

##### Actor-critic architecture   â€œè¡Œä¸ºè€…-è¯„ä»·è€…â€æ¨¡å‹

- The Actor-Critic model consists of both an **Actor network** and a **Critic network**.   
  **Actor-Criticæ¨¡å‹ç»“æ„**ï¼šè¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªã€ŒActorç½‘ç»œã€å’Œä¸€ä¸ªã€ŒCriticç½‘ç»œã€

- The Actor network learns the policy, deciding which action to take in a given state.   
  **Actorç½‘ç»œ**ï¼šè´Ÿè´£å­¦ä¹ ç­–ç•¥ï¼Œå†³å®šåœ¨ç»™å®šçŠ¶æ€ä¸‹è¦é‡‡å–çš„è¡ŒåŠ¨ã€‚

- The Critic network estimates the value function, evaluating the value of the action at the state.   
  **Criticç½‘ç»œ**ï¼šè´Ÿè´£è¯„ä¼°ä»·å€¼å‡½æ•°ï¼Œä¼°ç®—åœ¨è¯¥çŠ¶æ€ä¸‹æŸä¸€è¡ŒåŠ¨çš„ä»·å€¼ã€‚

- Motivation: This combination allows the agent to learn from both policy and value function aspects, making full use of the advantages of both methods.  
  **åŠ¨æœº**ï¼šè¿™ç§ç»“åˆä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤ŸåŒæ—¶ä»ã€Œç­–ç•¥ã€å’Œã€Œä»·å€¼å‡½æ•°ã€ä¸¤ä¸ªæ–¹é¢è¿›è¡Œå­¦ä¹ ï¼Œå……åˆ†åˆ©ç”¨è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿ã€‚

<img src="./images/977b30b1-d5cc-47ff-a931-706adc2b37fe.png" title="" alt="977b30b1-d5cc-47ff-a931-706adc2b37fe" style="zoom:50%;">

#### Applications of RL  å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨

- Self-Driving Cars 
  
  - Trajectory optimization, motion planning, and controller optimization.   
    è½¨è¿¹ä¼˜åŒ–ã€è¿åŠ¨è§„åˆ’å’Œæ§åˆ¶å™¨ä¼˜åŒ–ã€‚
  
  - Learning policies for parking, lane changing, and overtaking.   
    å­¦ä¹ æ³Šè½¦ã€è½¬çº¿åŠè¶…è½¦ç­–ç•¥ã€‚
  
  - AWS DeepRacer uses RL in autonomous racing cars.   
    AWS DeepRacer åœ¨è‡ªåŠ¨èµ›è½¦ä¸­ä½¿ç”¨ RLã€‚

- Industry Automation 
  
  - Robots performing tasks efficiently and safely.   
    æœºå™¨äººé«˜æ•ˆå®‰å…¨åœ°æ‰§è¡Œä»»åŠ¡ã€‚
  
  - Example: DeepMind cooling Google Data Centers using RL.  
    ç¤ºä¾‹: DeepMind ä½¿ç”¨ RL ä¸º Google æ•°æ®ä¸­å¿ƒé™æ¸©ã€‚

- Natural Language Processing (NLP) 
  
  - RL used in question answering, text summarization, and machine translation.   
    RL ç”¨äºé—®ç­”ã€æ–‡æœ¬æ‘˜è¦å’Œæœºå™¨ç¿»è¯‘ã€‚

- Finance and Trading 
  
  - RL agents making financial decisions (buy, sell, hold).   
    RL ä»£ç†äººä½œå‡ºè´¢åŠ¡å†³ç­–(ä¹°å…¥ã€å–å‡ºã€æŒæœ‰)ã€‚
  
  - IBM's reinforcement learning-based platform for financial trades.  
    IBM åŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡‘èäº¤æ˜“å¹³å°ã€‚

- Healthcare 
  
  - RL systems providing treatment policies for patients.   
    ä¸ºç—…äººæä¾›æ²»ç–—æ”¿ç­–çš„ RL ç³»ç»Ÿã€‚
  
  - RL in dynamic treatment regimes and medical diagnosis.   
    åŠ¨æ€æ²»ç–—åˆ¶åº¦å’ŒåŒ»ç–—è¯Šæ–­ä¸­çš„ RLã€‚

- Engineering 
  
  - Facebook's Horizon - RL platform optimizing large-scale systems.   
    Facebook çš„ Horizon-RL å¹³å°ä¼˜åŒ–å¤§è§„æ¨¡ç³»ç»Ÿã€‚

- News Recommendation 
  
  - RL tracking user preferences for personalized news recommendations.   
    RL è·Ÿè¸ªä¸ªæ€§åŒ–æ–°é—»æ¨èçš„ç”¨æˆ·åå¥½ã€‚
  
  - Factors considered: news features, reader behavior, and context.  
    è€ƒè™‘å› ç´ : æ–°é—»ç‰¹ç‚¹ã€è¯»è€…è¡Œä¸ºå’Œä¸Šä¸‹æ–‡ã€‚

- Gaming 
  
  - AlphaGo Zero - RL mastering the game of Go through self-play.   
    AlphaGo Zero-RL é€šè¿‡è‡ªç©æŒæ¡å›´æ£‹æ¸¸æˆã€‚

- Bidding and Marketing 
  
  - Multi-agent RL enables real-time bidding to balance the trade-off between the competition and cooperation among advertisers.   
    å¤šä»£ç† RL å®ç°äº†å®æ—¶ç«ä»·ï¼Œå¹³è¡¡äº†å¹¿å‘Šå•†ä¹‹é—´çš„ç«äº‰ä¸åˆä½œã€‚

- Robotics Manipulation 
  
  - Deep RL enabling robots to grasp various objects unseen during training.   
    æ·± RL ä½¿æœºå™¨äººèƒ½å¤ŸæŠ“ä½è®­ç»ƒä¸­çœ‹ä¸è§çš„å„ç§ç‰©ä½“ã€‚
  
  - Google AI's approach to robotics grasping using QT-Opt.  


